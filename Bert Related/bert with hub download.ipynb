{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert with hub download.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"A82KJjIDK8rJ","colab_type":"text"},"source":["## This only includes bert model,\n","## To use the model, requires to get tokenization for bert"]},{"cell_type":"code","metadata":{"id":"uKvHYUdTJLin","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"835e50ca-6375-4f1e-8700-88651853029d","executionInfo":{"status":"ok","timestamp":1573000906063,"user_tz":-540,"elapsed":7008,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["# download pre-trained model\n","!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip"],"execution_count":1,"outputs":[{"output_type":"stream","text":["--2019-11-06 00:41:40--  https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.15.128, 2a00:1450:400c:c0a::80\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.15.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 407727028 (389M) [application/zip]\n","Saving to: ‘uncased_L-12_H-768_A-12.zip’\n","\n","uncased_L-12_H-768_ 100%[===================>] 388.84M   112MB/s    in 3.5s    \n","\n","2019-11-06 00:41:44 (112 MB/s) - ‘uncased_L-12_H-768_A-12.zip’ saved [407727028/407727028]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rgN9pmWaKNvi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"outputId":"22874fcf-6e69-4d0d-f4aa-30368c2d3508","executionInfo":{"status":"ok","timestamp":1573000940832,"user_tz":-540,"elapsed":7697,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["!unzip /content/uncased_L-12_H-768_A-12.zip"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Archive:  /content/uncased_L-12_H-768_A-12.zip\n","   creating: uncased_L-12_H-768_A-12/\n","  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.meta  \n","  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001  \n","  inflating: uncased_L-12_H-768_A-12/vocab.txt  \n","  inflating: uncased_L-12_H-768_A-12/bert_model.ckpt.index  \n","  inflating: uncased_L-12_H-768_A-12/bert_config.json  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OxW2RN87LH63","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"9d095a61-9ee8-4afd-d180-00ff757fcd98","executionInfo":{"status":"ok","timestamp":1573001147546,"user_tz":-540,"elapsed":6394,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["!pip install bert-tensorflow"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |████▉                           | 10kB 22.8MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 6.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 6.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.3MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h2Tc96H4LEfX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":63},"outputId":"2d766378-9215-4c1f-9002-893a5b0edaa1","executionInfo":{"status":"ok","timestamp":1573001151162,"user_tz":-540,"elapsed":2874,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["import tensorflow as tf\n","import pandas as pd\n","import tensorflow_hub as hub\n","import os\n","import re\n","import numpy as np\n","from bert.tokenization import FullTokenizer\n","from tqdm import tqdm_notebook\n","from tensorflow.keras import backend as K"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"kkfPgTi3LR-v","colab_type":"text"},"source":["## Below is for getting data < IMDB LARGE >"]},{"cell_type":"code","metadata":{"id":"a_z2eMXRLM9f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"outputId":"448ada44-dc6e-4b96-fc6a-5582b1cf54b4","executionInfo":{"status":"ok","timestamp":1573001218315,"user_tz":-540,"elapsed":45491,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["# Load all files from a directory in a DataFrame.\n","def load_directory_data(directory):\n","  data = {}\n","  data[\"sentence\"] = []\n","  data[\"sentiment\"] = []\n","  for file_path in os.listdir(directory):\n","    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n","      data[\"sentence\"].append(f.read())\n","      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n","  return pd.DataFrame.from_dict(data)\n","\n","# Merge positive and negative examples, add a polarity column and shuffle.\n","def load_dataset(directory):\n","  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n","  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n","  pos_df[\"polarity\"] = 1\n","  neg_df[\"polarity\"] = 0\n","  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","  dataset = tf.keras.utils.get_file(\n","      fname=\"aclImdb.tar.gz\", \n","      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n","      extract=True)\n","\n","  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                       \"aclImdb\", \"train\"))\n","  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                      \"aclImdb\", \"test\"))\n","\n","  return train_df, test_df\n","\n","# Reduce logging output.\n","tf.logging.set_verbosity(tf.logging.ERROR)\n","\n","train_df, test_df = download_and_load_datasets()\n","train_df.head()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","84131840/84125825 [==============================] - 7s 0us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>sentiment</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Have to be honest and say that I haven't seen ...</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>More wide-eyed, hysterical 50s hyper-cheerfuln...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>They had me from the first show.&lt;br /&gt;&lt;br /&gt;We...</td>\n","      <td>9</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This movie bewilders me. It may be that I'm ju...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Excellent episode movie ala Pulp Fiction. 7 da...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence sentiment  polarity\n","0  Have to be honest and say that I haven't seen ...         8         1\n","1  More wide-eyed, hysterical 50s hyper-cheerfuln...         3         0\n","2  They had me from the first show.<br /><br />We...         9         1\n","3  This movie bewilders me. It may be that I'm ju...         3         0\n","4  Excellent episode movie ala Pulp Fiction. 7 da...        10         1"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"t_VQeL8bLX--","colab_type":"code","colab":{}},"source":["max_seq_length = 256\n","\n","# Create datasets (Only take up to max_seq_length words for memory)\n","train_text = train_df['sentence'].tolist()\n","train_text = [' '.join(t.split()[0:max_seq_length]) for t in train_text]\n","train_text = np.array(train_text, dtype=object)[:, np.newaxis]\n","train_label = train_df['polarity'].tolist()\n","\n","test_text = test_df['sentence'].tolist()\n","test_text = [' '.join(t.split()[0:max_seq_length]) for t in test_text]\n","test_text = np.array(test_text, dtype=object)[:, np.newaxis]\n","test_label = test_df['polarity'].tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TvFJq7YeLuP2","colab_type":"code","colab":{}},"source":["# Setting Bert Config Setting\n","\n","BERT_VOCAB = '/content/uncased_L-12_H-768_A-12/vocab.txt'\n","BERT_INIT_CHKPNT = '/content/uncased_L-12_H-768_A-12/bert_model.ckpt'\n","BERT_CONFIG = '/content/uncased_L-12_H-768_A-12/bert_config.json'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsSjY__8MAMc","colab_type":"code","colab":{}},"source":["import bert\n","from bert import run_classifier, run_classifier_with_tfhub, modeling, tokenization"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6IfkWtb7MI4n","colab_type":"code","colab":{}},"source":["tokenization.validate_case_matches_checkpoint(True, BERT_INIT_CHKPNT) # <- should it be required?"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nrRcAaOgN72w","colab_type":"code","colab":{}},"source":["tokenizer = tokenization.FullTokenizer(vocab_file = BERT_VOCAB, do_lower_case=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7bhM9PzXOEQP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":221},"outputId":"cbed61b0-8785-4767-ec8e-cfef381d8185","executionInfo":{"status":"ok","timestamp":1573001927282,"user_tz":-540,"elapsed":1039,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["tokenizer.tokenize(\"This here’s an example of using the BERT tokenizer\")"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," '’',\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"6qpU2iAMOLh3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3c7ceaa7-3180-4001-bb03-76ccecff24c3","executionInfo":{"status":"ok","timestamp":1573001964673,"user_tz":-540,"elapsed":2018,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["len(tokenizer.vocab)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["30522"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"dY5r2_NAOcH9","colab_type":"text"},"source":["# Convert data to bert to understand"]},{"cell_type":"code","metadata":{"id":"C--TVFXSO3ju","colab_type":"code","colab":{}},"source":["test_sentence = \"This here’s an example of using the BERT tokenizer\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yf1JDrRSOUUu","colab_type":"code","colab":{}},"source":["class InputExample(object):\n","  def __init__(self, guid, text_a, text_b=None, labels=None):\n","    self.guid = guid\n","    self.text_a = text_a\n","    self.text_b = text_b\n","    self.labels = labels"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtbGGLbbOvKF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6b84a4f1-4575-4ffa-8f17-825b699ed815","executionInfo":{"status":"ok","timestamp":1573002950469,"user_tz":-540,"elapsed":1034,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["test_input_example = bert.run_classifier.InputExample(guid=\"\", text_a=test_sentence, text_b=None, label=0)\n","\n","test_input_example"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bert.run_classifier.InputExample at 0x7f0b05eb12b0>"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"C071iTl0PDWm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d1246870-bfda-42cc-c552-8ddbf5761222","executionInfo":{"status":"ok","timestamp":1573003019992,"user_tz":-540,"elapsed":1030,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["bert.run_classifier.convert_examples_to_features(examples=[test_input_example], label_list=[0,1], max_seq_length=32, tokenizer=tokenizer)"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<bert.run_classifier.InputFeatures at 0x7f0b05ed0400>]"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"9CptoRwDSwGn","colab_type":"code","colab":{}},"source":["train_df['index'] = train_df.index"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mxow3SJES9M9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"a48cb38d-095b-409a-c858-46f21ef4ba6e","executionInfo":{"status":"ok","timestamp":1573003189027,"user_tz":-540,"elapsed":1173,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["train_df.head()"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>sentiment</th>\n","      <th>polarity</th>\n","      <th>index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Have to be honest and say that I haven't seen ...</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>More wide-eyed, hysterical 50s hyper-cheerfuln...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>They had me from the first show.&lt;br /&gt;&lt;br /&gt;We...</td>\n","      <td>9</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>This movie bewilders me. It may be that I'm ju...</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Excellent episode movie ala Pulp Fiction. 7 da...</td>\n","      <td>10</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence sentiment  polarity  index\n","0  Have to be honest and say that I haven't seen ...         8         1      0\n","1  More wide-eyed, hysterical 50s hyper-cheerfuln...         3         0      1\n","2  They had me from the first show.<br /><br />We...         9         1      2\n","3  This movie bewilders me. It may be that I'm ju...         3         0      3\n","4  Excellent episode movie ala Pulp Fiction. 7 da...        10         1      4"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"tBor6iexSZVY","colab_type":"code","colab":{}},"source":["# create exmple from df\n","\n","def create_example(df, labels_available=True):\n","  '''\n","  create_examples(), reads data-frame and loads input text and corresponding target labels into InputExample objects.\n","  '''\n","  examples = []\n","  for (i, row) in enumerate(df.values):\n","    guid = row[3]\n","    text_a = row[1]\n","    if labels_available:\n","      labels = row[2]\n","    else:\n","      labels = [0]\n","    examples.append(bert.run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=labels))\n","  return examples"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G1LCS0fPTTQM","colab_type":"code","colab":{}},"source":["train_data = create_example(train_df)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HE1e5L2hUAj1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":119},"outputId":"0654a92e-9539-4a1c-d28f-f2906663f76e","executionInfo":{"status":"ok","timestamp":1573003481673,"user_tz":-540,"elapsed":1063,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["print(len(train_data))\n","train_data[:5]"],"execution_count":69,"outputs":[{"output_type":"stream","text":["25000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<bert.run_classifier.InputExample at 0x7f0b05f89a90>,\n"," <bert.run_classifier.InputExample at 0x7f0b05f89ac8>,\n"," <bert.run_classifier.InputExample at 0x7f0b05f899e8>,\n"," <bert.run_classifier.InputExample at 0x7f0b05f89a20>,\n"," <bert.run_classifier.InputExample at 0x7f0b05f89b00>]"]},"metadata":{"tags":[]},"execution_count":69}]},{"cell_type":"code","metadata":{"id":"iuxQ-A92UVal","colab_type":"code","colab":{}},"source":["LABEL_LIST = [0, 1]\n","MAX_LEN=256"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e6rgoRkYUG6P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":282},"outputId":"e1232ba7-9c55-4cf3-8c88-ee0b43676fe3","executionInfo":{"status":"error","timestamp":1573003586326,"user_tz":-540,"elapsed":1060,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["bert.run_classifier.convert_examples_to_features(train_data[0], label_list=LABEL_LIST, max_seq_length=MAX_LEN, tokenizer=tokenizer)"],"execution_count":72,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-72-04d0b18c14e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABEL_LIST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/bert/run_classifier.py\u001b[0m in \u001b[0;36mconvert_examples_to_features\u001b[0;34m(examples, label_list, max_seq_length, tokenizer)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m   \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mex_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Writing example %d of %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mex_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'InputExample' object is not iterable"]}]}]}