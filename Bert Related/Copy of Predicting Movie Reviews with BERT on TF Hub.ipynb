{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Predicting Movie Reviews with BERT on TF Hub.ipynb","provenance":[{"file_id":"https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb","timestamp":1572608438398}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"j0a4mTk9o1Qg","colab_type":"code","colab":{}},"source":["# Copyright 2019 Google Inc.\n","\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dCpvgG0vwXAZ","colab_type":"text"},"source":["#Predicting Movie Review Sentiment with BERT on TF Hub"]},{"cell_type":"markdown","metadata":{"id":"xiYrZKaHwV81","colab_type":"text"},"source":["If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n","\n","Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n","\n","Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from [this colab notebook](https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb). Let's get started!"]},{"cell_type":"code","metadata":{"id":"hsZvic2YxnTz","colab_type":"code","outputId":"06b8e74f-6195-4f38-d0fd-4bf8e64d4bc2","executionInfo":{"status":"ok","timestamp":1572608455683,"user_tz":-540,"elapsed":2958,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":64}},"source":["from sklearn.model_selection import train_test_split\n","import pandas as pd\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","from datetime import datetime"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"cp5wfXDx5SPH","colab_type":"text"},"source":["In addition to the standard libraries we imported above, we'll need to install BERT's python package."]},{"cell_type":"code","metadata":{"id":"jviywGyWyKsA","colab_type":"code","outputId":"b4c5c8cd-edbe-4b78-b202-302fd5cc0c8a","executionInfo":{"status":"ok","timestamp":1572608462952,"user_tz":-540,"elapsed":5853,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["!pip install bert-tensorflow"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting bert-tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n","\r\u001b[K     |████▉                           | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n","Installing collected packages: bert-tensorflow\n","Successfully installed bert-tensorflow-1.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hhbGEfwgdEtw","colab_type":"code","outputId":"c4d15cf6-463d-4971-e676-c4f8abd17492","executionInfo":{"status":"ok","timestamp":1572608465130,"user_tz":-540,"elapsed":920,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KVB3eOcjxxm1","colab_type":"text"},"source":["Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n","\n","Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n","\n","Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."]},{"cell_type":"code","metadata":{"id":"US_EAnICvP7f","colab_type":"code","outputId":"dc4ff7c6-8749-4629-ca25-42862d6b7dd2","cellView":"form","executionInfo":{"status":"ok","timestamp":1572612731583,"user_tz":-540,"elapsed":995,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# Set the output directory for saving model file\n","# Optionally, set a GCP bucket location\n","\n","OUTPUT_DIR = 'OUTPUT_DIR_NAME'#@param {type:\"string\"}\n","#@markdown Whether or not to clear/delete the directory and create a new one\n","DO_DELETE = True #@param {type:\"boolean\"}\n","#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n","USE_BUCKET = False #@param {type:\"boolean\"}\n","BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n","\n","if USE_BUCKET:\n","  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n","  from google.colab import auth\n","  auth.authenticate_user()\n","\n","if DO_DELETE:\n","  try:\n","    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n","  except:\n","    # Doesn't matter if the directory didn't exist\n","    pass\n","tf.gfile.MakeDirs(OUTPUT_DIR)\n","print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["***** Model output directory: OUTPUT_DIR_NAME *****\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pmFYvkylMwXn","colab_type":"text"},"source":["#Data"]},{"cell_type":"markdown","metadata":{"id":"MC_w8SRqN0fr","colab_type":"text"},"source":["First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."]},{"cell_type":"code","metadata":{"id":"fom_ff20gyy6","colab_type":"code","colab":{}},"source":["from tensorflow import keras\n","import os\n","import re\n","\n","# Load all files from a directory in a DataFrame.\n","def load_directory_data(directory):\n","  data = {}\n","  data[\"sentence\"] = []\n","  data[\"sentiment\"] = []\n","  for file_path in os.listdir(directory):\n","    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n","      data[\"sentence\"].append(f.read())\n","      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n","  return pd.DataFrame.from_dict(data)\n","\n","# Merge positive and negative examples, add a polarity column and shuffle.\n","def load_dataset(directory):\n","  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n","  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n","  pos_df[\"polarity\"] = 1\n","  neg_df[\"polarity\"] = 0\n","  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n","\n","# Download and process the dataset files.\n","def download_and_load_datasets(force_download=False):\n","  dataset = tf.keras.utils.get_file(\n","      fname=\"aclImdb.tar.gz\", \n","      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n","      extract=True)\n","  \n","  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                       \"aclImdb\", \"train\"))\n","  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n","                                      \"aclImdb\", \"test\"))\n","  \n","  return train_df, test_df\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2abfwdn-g135","colab_type":"code","outputId":"20138b0f-1afd-496a-9608-208fa68dcdd9","executionInfo":{"status":"ok","timestamp":1572608531637,"user_tz":-540,"elapsed":46832,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["train, test = download_and_load_datasets()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","84131840/84125825 [==============================] - 7s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zzx4KhO7xSWF","colab_type":"code","outputId":"7922321c-2bb1-4b8f-e3bf-8c802be2a025","executionInfo":{"status":"ok","timestamp":1572608556292,"user_tz":-540,"elapsed":948,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["train"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>sentiment</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Firstly, I am not easily scared by... Anything...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>This show is unbelievable in that . . . what i...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I'd like to start off by saying that I am NOT ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>If you want to remember MJ, this is a good pla...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>One of the best movies out there. Yeah maybe t...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>24995</th>\n","      <td>I wasn't really going to comment, but then I f...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24996</th>\n","      <td>This is Jackie Chan's best film, and my person...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24997</th>\n","      <td>Sitting, Typing Nothing is the latest \"what i...</td>\n","      <td>7</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24998</th>\n","      <td>Why didn't this pick up a bag full of Oscars? ...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24999</th>\n","      <td>For the first three seasons, Sabrina was a gem...</td>\n","      <td>9</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>25000 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                sentence sentiment  polarity\n","0      Firstly, I am not easily scared by... Anything...         1         0\n","1      This show is unbelievable in that . . . what i...         1         0\n","2      I'd like to start off by saying that I am NOT ...         1         0\n","3      If you want to remember MJ, this is a good pla...        10         1\n","4      One of the best movies out there. Yeah maybe t...        10         1\n","...                                                  ...       ...       ...\n","24995  I wasn't really going to comment, but then I f...         3         0\n","24996  This is Jackie Chan's best film, and my person...        10         1\n","24997  Sitting, Typing\n"," Nothing is the latest \"what i...         7         1\n","24998  Why didn't this pick up a bag full of Oscars? ...        10         1\n","24999  For the first three seasons, Sabrina was a gem...         9         1\n","\n","[25000 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"XA8WHJgzhIZf","colab_type":"text"},"source":["To keep training fast, we'll take a sample of 5000 train and test examples, respectively."]},{"cell_type":"code","metadata":{"id":"lw_F488eixTV","colab_type":"code","colab":{}},"source":["train = train.sample(5000)\n","test = test.sample(5000)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONgeKGNWxnED","colab_type":"code","outputId":"fdd17084-a431-4038-a39c-9564bd0f5034","executionInfo":{"status":"ok","timestamp":1572608571554,"user_tz":-540,"elapsed":888,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["train"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>sentiment</th>\n","      <th>polarity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>23849</th>\n","      <td>The sexploitation movie era of the late sixtie...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>13439</th>\n","      <td>I loved this masterpiece and quite frankly I, ...</td>\n","      <td>8</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>19188</th>\n","      <td>The acting may be okay, the more u watch this ...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>18280</th>\n","      <td>I must pat myself on the back for watching thi...</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>12373</th>\n","      <td>This movie is pure guano. Mom always said if y...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>6934</th>\n","      <td>As it is often the case, the impressive and ex...</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>7889</th>\n","      <td>Within the first 17 minutes of director Bradfo...</td>\n","      <td>4</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>548</th>\n","      <td>As anyone old enough knows, South Africa long ...</td>\n","      <td>10</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>24774</th>\n","      <td>In this documentary we meet Roger, the rich ma...</td>\n","      <td>9</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>115</th>\n","      <td>Perhaps not the absolute greatest entry in the...</td>\n","      <td>7</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5000 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                sentence sentiment  polarity\n","23849  The sexploitation movie era of the late sixtie...        10         1\n","13439  I loved this masterpiece and quite frankly I, ...         8         1\n","19188  The acting may be okay, the more u watch this ...         1         0\n","18280  I must pat myself on the back for watching thi...         3         0\n","12373  This movie is pure guano. Mom always said if y...         1         0\n","...                                                  ...       ...       ...\n","6934   As it is often the case, the impressive and ex...         4         0\n","7889   Within the first 17 minutes of director Bradfo...         4         0\n","548    As anyone old enough knows, South Africa long ...        10         1\n","24774  In this documentary we meet Roger, the rich ma...         9         1\n","115    Perhaps not the absolute greatest entry in the...         7         1\n","\n","[5000 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"prRQM8pDi8xI","colab_type":"code","outputId":"34445cb8-2be0-4379-fdbc-7794091f6049","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train.columns"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['sentence', 'sentiment', 'polarity'], dtype='object')"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"sfRnHSz3iSXz","colab_type":"text"},"source":["For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"]},{"cell_type":"code","metadata":{"id":"IuMOGwFui4it","colab_type":"code","colab":{}},"source":["DATA_COLUMN = 'sentence'\n","LABEL_COLUMN = 'polarity'\n","# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n","label_list = [0, 1]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V399W0rqNJ-Z","colab_type":"text"},"source":["#Data Preprocessing\n","We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n","\n","- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n","- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n","- `label` is the label for our example, i.e. True, False"]},{"cell_type":"code","metadata":{"id":"p9gEt5SmM6i6","colab_type":"code","colab":{}},"source":["# Use the InputExample class from BERT's run_classifier code to create examples from the data\n","train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)\n","\n","test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n","                                                                   text_a = x[DATA_COLUMN], \n","                                                                   text_b = None, \n","                                                                   label = x[LABEL_COLUMN]), axis = 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"A1_4Ja335h_n","colab_type":"code","outputId":"ff4f999a-1999-49d2-cfe8-857a6b3086e3","executionInfo":{"status":"ok","timestamp":1572610667647,"user_tz":-540,"elapsed":901,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["train_InputExamples[23849]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bert.run_classifier.InputExample at 0x7f3fddd0b080>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"SCZWZtKxObjh","colab_type":"text"},"source":["Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n","\n","\n","1. Lowercase our text (if we're using a BERT lowercase model)\n","2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n","3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n","4. Map our words to indexes using a vocab file that BERT provides\n","5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n","6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n","\n","Happily, we don't have to worry about most of these details.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qMWiDtpyQSoU","colab_type":"text"},"source":["To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"]},{"cell_type":"code","metadata":{"id":"IhJSe0QHNG7U","colab_type":"code","outputId":"2abc3e06-57be-4f13-ac39-64d4e6838472","executionInfo":{"status":"ok","timestamp":1572610734792,"user_tz":-540,"elapsed":17886,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# This is a path to an uncased (all lowercase) version of BERT\n","BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","def create_tokenizer_from_hub_module():\n","  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n","  with tf.Graph().as_default():\n","    bert_module = hub.Module(BERT_MODEL_HUB)\n","    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n","    with tf.Session() as sess:\n","      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n","                                            tokenization_info[\"do_lower_case\"]])\n","      \n","  return bert.tokenization.FullTokenizer(\n","      vocab_file=vocab_file, do_lower_case=do_lower_case)\n","\n","tokenizer = create_tokenizer_from_hub_module()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"z4oFkhpZBDKm","colab_type":"text"},"source":["Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"]},{"cell_type":"code","metadata":{"id":"dsBo6RCtQmwx","colab_type":"code","outputId":"d516953a-438d-4c37-e6c8-775a1f5c28fe","executionInfo":{"status":"ok","timestamp":1572610737798,"user_tz":-540,"elapsed":914,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":233}},"source":["tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this',\n"," 'here',\n"," \"'\",\n"," 's',\n"," 'an',\n"," 'example',\n"," 'of',\n"," 'using',\n"," 'the',\n"," 'bert',\n"," 'token',\n"," '##izer']"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"0OEzfFIt6GIc","colab_type":"text"},"source":["Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."]},{"cell_type":"code","metadata":{"id":"LL5W8gEGRTAf","colab_type":"code","outputId":"ac080329-cdbd-49c2-8f4e-953c0ef534c9","executionInfo":{"status":"ok","timestamp":1572610818243,"user_tz":-540,"elapsed":40764,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# We'll set sequences to be at most 128 tokens long.\n","MAX_SEQ_LENGTH = 128\n","\n","# Convert our train and test features to InputFeatures that BERT understands.\n","train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the sex ##pl ##oit ##ation movie era of the late sixties and early seventies began with the allowance of gr ##at ##uit ##ous nu ##dity in mainstream films and ended with the legal ##ization of hardcore porn . it ' s peak years were between 1968 and 1972 . one of the most loved and talented actresses of the era was monica gay ##le , who had a small but fan ##atic cult of followers . she was actually able to act , unlike many who filled the lead roles of these flick ##s , and her subsequent credits proved it . and her seemingly deliberate fade into ob ##scu ##rity right when her career was taking off only height ##ens her my ##sti ##que . [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the sex ##pl ##oit ##ation movie era of the late sixties and early seventies began with the allowance of gr ##at ##uit ##ous nu ##dity in mainstream films and ended with the legal ##ization of hardcore porn . it ' s peak years were between 1968 and 1972 . one of the most loved and talented actresses of the era was monica gay ##le , who had a small but fan ##atic cult of followers . she was actually able to act , unlike many who filled the lead roles of these flick ##s , and her subsequent credits proved it . and her seemingly deliberate fade into ob ##scu ##rity right when her career was taking off only height ##ens her my ##sti ##que . [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3348 24759 28100 3370 3185 3690 1997 1996 2397 22651 1998 2220 26232 2211 2007 1996 21447 1997 24665 4017 14663 3560 16371 25469 1999 7731 3152 1998 3092 2007 1996 3423 3989 1997 13076 22555 1012 2009 1005 1055 4672 2086 2020 2090 3380 1998 3285 1012 2028 1997 1996 2087 3866 1998 10904 19910 1997 1996 3690 2001 9018 5637 2571 1010 2040 2018 1037 2235 2021 5470 12070 8754 1997 8771 1012 2016 2001 2941 2583 2000 2552 1010 4406 2116 2040 3561 1996 2599 4395 1997 2122 17312 2015 1010 1998 2014 4745 6495 4928 2009 1012 1998 2014 9428 15063 12985 2046 27885 28817 15780 2157 2043 2014 2476 2001 2635 2125 2069 4578 6132 2014 2026 16643 4226 1012 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3348 24759 28100 3370 3185 3690 1997 1996 2397 22651 1998 2220 26232 2211 2007 1996 21447 1997 24665 4017 14663 3560 16371 25469 1999 7731 3152 1998 3092 2007 1996 3423 3989 1997 13076 22555 1012 2009 1005 1055 4672 2086 2020 2090 3380 1998 3285 1012 2028 1997 1996 2087 3866 1998 10904 19910 1997 1996 3690 2001 9018 5637 2571 1010 2040 2018 1037 2235 2021 5470 12070 8754 1997 8771 1012 2016 2001 2941 2583 2000 2552 1010 4406 2116 2040 3561 1996 2599 4395 1997 2122 17312 2015 1010 1998 2014 4745 6495 4928 2009 1012 1998 2014 9428 15063 12985 2046 27885 28817 15780 2157 2043 2014 2476 2001 2635 2125 2069 4578 6132 2014 2026 16643 4226 1012 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i loved this masterpiece and quite frankly i , too found mary pop ##pins ( although i love julie andrews and dick van ##dy ##ke ) to be silly and sac ##rine - sweet . angela lan ##sbury plays her character to perfection and i don ' t know why people think of this film as distorted . it was magical and it was lots of fun to watch . every scene held a certain charm as you got to know the characters better . you truly see how this little thrown together family learn to bond with each other , despite their age and differences . i thought the characters were well developed , especially charles who was at \" the age of not believing [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i loved this masterpiece and quite frankly i , too found mary pop ##pins ( although i love julie andrews and dick van ##dy ##ke ) to be silly and sac ##rine - sweet . angela lan ##sbury plays her character to perfection and i don ' t know why people think of this film as distorted . it was magical and it was lots of fun to watch . every scene held a certain charm as you got to know the characters better . you truly see how this little thrown together family learn to bond with each other , despite their age and differences . i thought the characters were well developed , especially charles who was at \" the age of not believing [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 3866 2023 17743 1998 3243 19597 1045 1010 2205 2179 2984 3769 27915 1006 2348 1045 2293 7628 9261 1998 5980 3158 5149 3489 1007 2000 2022 10021 1998 17266 11467 1011 4086 1012 10413 17595 18065 3248 2014 2839 2000 15401 1998 1045 2123 1005 1056 2113 2339 2111 2228 1997 2023 2143 2004 19112 1012 2009 2001 8687 1998 2009 2001 7167 1997 4569 2000 3422 1012 2296 3496 2218 1037 3056 11084 2004 2017 2288 2000 2113 1996 3494 2488 1012 2017 5621 2156 2129 2023 2210 6908 2362 2155 4553 2000 5416 2007 2169 2060 1010 2750 2037 2287 1998 5966 1012 1045 2245 1996 3494 2020 2092 2764 1010 2926 2798 2040 2001 2012 1000 1996 2287 1997 2025 8929 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 3866 2023 17743 1998 3243 19597 1045 1010 2205 2179 2984 3769 27915 1006 2348 1045 2293 7628 9261 1998 5980 3158 5149 3489 1007 2000 2022 10021 1998 17266 11467 1011 4086 1012 10413 17595 18065 3248 2014 2839 2000 15401 1998 1045 2123 1005 1056 2113 2339 2111 2228 1997 2023 2143 2004 19112 1012 2009 2001 8687 1998 2009 2001 7167 1997 4569 2000 3422 1012 2296 3496 2218 1037 3056 11084 2004 2017 2288 2000 2113 1996 3494 2488 1012 2017 5621 2156 2129 2023 2210 6908 2362 2155 4553 2000 5416 2007 2169 2060 1010 2750 2037 2287 1998 5966 1012 1045 2245 1996 3494 2020 2092 2764 1010 2926 2798 2040 2001 2012 1000 1996 2287 1997 2025 8929 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the acting may be okay , the more u watch this movie , the more u wish you weren ' t , this movie is so horrible , that if i could get a hold of every copy , i would burn them all and not look back , this movie is terrible ! ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the acting may be okay , the more u watch this movie , the more u wish you weren ' t , this movie is so horrible , that if i could get a hold of every copy , i would burn them all and not look back , this movie is terrible ! ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3772 2089 2022 3100 1010 1996 2062 1057 3422 2023 3185 1010 1996 2062 1057 4299 2017 4694 1005 1056 1010 2023 3185 2003 2061 9202 1010 2008 2065 1045 2071 2131 1037 2907 1997 2296 6100 1010 1045 2052 6402 2068 2035 1998 2025 2298 2067 1010 2023 3185 2003 6659 999 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3772 2089 2022 3100 1010 1996 2062 1057 3422 2023 3185 1010 1996 2062 1057 4299 2017 4694 1005 1056 1010 2023 3185 2003 2061 9202 1010 2008 2065 1045 2071 2131 1037 2907 1997 2296 6100 1010 1045 2052 6402 2068 2035 1998 2025 2298 2067 1010 2023 3185 2003 6659 999 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i must pat myself on the back for watching this movie all the way through because it truly was painful . an incapable painter becomes a fully capable hit - man ? this movie was ri ##fe with absurd ##ities of which if i mentioned them all i would be giving away the movie . norm peterson aka george wen ##dt must really be in a ru ##t to have agreed to do this movie . the acting was de ##pl ##ora ##ble and the story was even worse . as a sane minded and rational individual , i could not understand where the writer came from nor where he was going with this movie . there was in ##ept ##ness on the part of every [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] i must pat myself on the back for watching this movie all the way through because it truly was painful . an incapable painter becomes a fully capable hit - man ? this movie was ri ##fe with absurd ##ities of which if i mentioned them all i would be giving away the movie . norm peterson aka george wen ##dt must really be in a ru ##t to have agreed to do this movie . the acting was de ##pl ##ora ##ble and the story was even worse . as a sane minded and rational individual , i could not understand where the writer came from nor where he was going with this movie . there was in ##ept ##ness on the part of every [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2442 6986 2870 2006 1996 2067 2005 3666 2023 3185 2035 1996 2126 2083 2138 2009 5621 2001 9145 1012 2019 19907 5276 4150 1037 3929 5214 2718 1011 2158 1029 2023 3185 2001 15544 7959 2007 18691 6447 1997 2029 2065 1045 3855 2068 2035 1045 2052 2022 3228 2185 1996 3185 1012 13373 12001 9875 2577 19181 11927 2442 2428 2022 1999 1037 21766 2102 2000 2031 3530 2000 2079 2023 3185 1012 1996 3772 2001 2139 24759 6525 3468 1998 1996 2466 2001 2130 4788 1012 2004 1037 22856 13128 1998 11581 3265 1010 1045 2071 2025 3305 2073 1996 3213 2234 2013 4496 2073 2002 2001 2183 2007 2023 3185 1012 2045 2001 1999 23606 2791 2006 1996 2112 1997 2296 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1045 2442 6986 2870 2006 1996 2067 2005 3666 2023 3185 2035 1996 2126 2083 2138 2009 5621 2001 9145 1012 2019 19907 5276 4150 1037 3929 5214 2718 1011 2158 1029 2023 3185 2001 15544 7959 2007 18691 6447 1997 2029 2065 1045 3855 2068 2035 1045 2052 2022 3228 2185 1996 3185 1012 13373 12001 9875 2577 19181 11927 2442 2428 2022 1999 1037 21766 2102 2000 2031 3530 2000 2079 2023 3185 1012 1996 3772 2001 2139 24759 6525 3468 1998 1996 2466 2001 2130 4788 1012 2004 1037 22856 13128 1998 11581 3265 1010 1045 2071 2025 3305 2073 1996 3213 2234 2013 4496 2073 2002 2001 2183 2007 2023 3185 1012 2045 2001 1999 23606 2791 2006 1996 2112 1997 2296 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this movie is pure gu ##ano . mom always said if you can ' t say anything nice . . . but even mom would say i had to do my part to warn others of this movie . < br / > < br / > i can guarantee this is the film that geoffrey rush wishes would just go away . i would hope that greg kin ##nea ##r fired his agent . . from a cannon for giving him the script . after this ben still ##er is probably praying for someone to pitch \" there ' s still something about mary . \" i have always been a fan of wes stud ##i ' s , thank whatever you hold holy that [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this movie is pure gu ##ano . mom always said if you can ' t say anything nice . . . but even mom would say i had to do my part to warn others of this movie . < br / > < br / > i can guarantee this is the film that geoffrey rush wishes would just go away . i would hope that greg kin ##nea ##r fired his agent . . from a cannon for giving him the script . after this ben still ##er is probably praying for someone to pitch \" there ' s still something about mary . \" i have always been a fan of wes stud ##i ' s , thank whatever you hold holy that [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 3185 2003 5760 19739 6761 1012 3566 2467 2056 2065 2017 2064 1005 1056 2360 2505 3835 1012 1012 1012 2021 2130 3566 2052 2360 1045 2018 2000 2079 2026 2112 2000 11582 2500 1997 2023 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2064 11302 2023 2003 1996 2143 2008 11023 5481 8996 2052 2074 2175 2185 1012 1045 2052 3246 2008 6754 12631 22084 2099 5045 2010 4005 1012 1012 2013 1037 8854 2005 3228 2032 1996 5896 1012 2044 2023 3841 2145 2121 2003 2763 14488 2005 2619 2000 6510 1000 2045 1005 1055 2145 2242 2055 2984 1012 1000 1045 2031 2467 2042 1037 5470 1997 14008 16054 2072 1005 1055 1010 4067 3649 2017 2907 4151 2008 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 3185 2003 5760 19739 6761 1012 3566 2467 2056 2065 2017 2064 1005 1056 2360 2505 3835 1012 1012 1012 2021 2130 3566 2052 2360 1045 2018 2000 2079 2026 2112 2000 11582 2500 1997 2023 3185 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2064 11302 2023 2003 1996 2143 2008 11023 5481 8996 2052 2074 2175 2185 1012 1045 2052 3246 2008 6754 12631 22084 2099 5045 2010 4005 1012 1012 2013 1037 8854 2005 3228 2032 1996 5896 1012 2044 2023 3841 2145 2121 2003 2763 14488 2005 2619 2000 6510 1000 2045 1005 1055 2145 2242 2055 2984 1012 1000 1045 2031 2467 2042 1037 5470 1997 14008 16054 2072 1005 1055 1010 4067 3649 2017 2907 4151 2008 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 5000\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] nemesis game is a mind - bending film filled with riddle ##s , death , mystery , and philosophy . in it ' s simplest sense the film is about seeking answers and what happens when you ' ve finally found them all . the search for answers leads sarah novak down a path that gets darker as it gets more compelling . the final answer seems more dangerous than it is worth , yet sarah is so close to understanding it all . what would you do if you were offered the ability to finally make sense of the chaos of life ? < br / > < br / > the movie was written and directed by jesse warn . while this was warn [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] nemesis game is a mind - bending film filled with riddle ##s , death , mystery , and philosophy . in it ' s simplest sense the film is about seeking answers and what happens when you ' ve finally found them all . the search for answers leads sarah novak down a path that gets darker as it gets more compelling . the final answer seems more dangerous than it is worth , yet sarah is so close to understanding it all . what would you do if you were offered the ability to finally make sense of the chaos of life ? < br / > < br / > the movie was written and directed by jesse warn . while this was warn [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 21363 2208 2003 1037 2568 1011 14457 2143 3561 2007 21834 2015 1010 2331 1010 6547 1010 1998 4695 1012 1999 2009 1005 1055 21304 3168 1996 2143 2003 2055 6224 6998 1998 2054 6433 2043 2017 1005 2310 2633 2179 2068 2035 1012 1996 3945 2005 6998 5260 4532 19580 2091 1037 4130 2008 4152 9904 2004 2009 4152 2062 17075 1012 1996 2345 3437 3849 2062 4795 2084 2009 2003 4276 1010 2664 4532 2003 2061 2485 2000 4824 2009 2035 1012 2054 2052 2017 2079 2065 2017 2020 3253 1996 3754 2000 2633 2191 3168 1997 1996 8488 1997 2166 1029 1026 7987 1013 1028 1026 7987 1013 1028 1996 3185 2001 2517 1998 2856 2011 7627 11582 1012 2096 2023 2001 11582 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 21363 2208 2003 1037 2568 1011 14457 2143 3561 2007 21834 2015 1010 2331 1010 6547 1010 1998 4695 1012 1999 2009 1005 1055 21304 3168 1996 2143 2003 2055 6224 6998 1998 2054 6433 2043 2017 1005 2310 2633 2179 2068 2035 1012 1996 3945 2005 6998 5260 4532 19580 2091 1037 4130 2008 4152 9904 2004 2009 4152 2062 17075 1012 1996 2345 3437 3849 2062 4795 2084 2009 2003 4276 1010 2664 4532 2003 2061 2485 2000 4824 2009 2035 1012 2054 2052 2017 2079 2065 2017 2020 3253 1996 3754 2000 2633 2191 3168 1997 1996 8488 1997 2166 1029 1026 7987 1013 1028 1026 7987 1013 1028 1996 3185 2001 2517 1998 2856 2011 7627 11582 1012 2096 2023 2001 11582 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] anna ko ##kki ##nos ' success with ' head on ' now begins to look like it depended totally on the script and alex dimitri ##ades great lead performance . the degree to which this latest , \" the book of revelation \" is both derivative , pre ##ten ##tious and utterly uno ##ri ##ginal ( except for tristan milan ##i ' s fine cinematography ) seems to bear this out . . < br / > < br / > ala ##s , there have already been quite a few aus ##sie movies dealing with such themes , some rev ##iled for ' sex ##ism ' ( and / or explicit sex scenes ) in the 1970s and 1980s and beyond and maybe they ' [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] anna ko ##kki ##nos ' success with ' head on ' now begins to look like it depended totally on the script and alex dimitri ##ades great lead performance . the degree to which this latest , \" the book of revelation \" is both derivative , pre ##ten ##tious and utterly uno ##ri ##ginal ( except for tristan milan ##i ' s fine cinematography ) seems to bear this out . . < br / > < br / > ala ##s , there have already been quite a few aus ##sie movies dealing with such themes , some rev ##iled for ' sex ##ism ' ( and / or explicit sex scenes ) in the 1970s and 1980s and beyond and maybe they ' [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 4698 12849 24103 15460 1005 3112 2007 1005 2132 2006 1005 2085 4269 2000 2298 2066 2009 17292 6135 2006 1996 5896 1998 4074 15953 18673 2307 2599 2836 1012 1996 3014 2000 2029 2023 6745 1010 1000 1996 2338 1997 11449 1000 2003 2119 13819 1010 3653 6528 20771 1998 12580 27776 3089 24965 1006 3272 2005 9822 6954 2072 1005 1055 2986 16434 1007 3849 2000 4562 2023 2041 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 21862 2015 1010 2045 2031 2525 2042 3243 1037 2261 17151 11741 5691 7149 2007 2107 6991 1010 2070 7065 18450 2005 1005 3348 2964 1005 1006 1998 1013 2030 13216 3348 5019 1007 1999 1996 3955 1998 3865 1998 3458 1998 2672 2027 1005 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 4698 12849 24103 15460 1005 3112 2007 1005 2132 2006 1005 2085 4269 2000 2298 2066 2009 17292 6135 2006 1996 5896 1998 4074 15953 18673 2307 2599 2836 1012 1996 3014 2000 2029 2023 6745 1010 1000 1996 2338 1997 11449 1000 2003 2119 13819 1010 3653 6528 20771 1998 12580 27776 3089 24965 1006 3272 2005 9822 6954 2072 1005 1055 2986 16434 1007 3849 2000 4562 2023 2041 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 21862 2015 1010 2045 2031 2525 2042 3243 1037 2261 17151 11741 5691 7149 2007 2107 6991 1010 2070 7065 18450 2005 1005 3348 2964 1005 1006 1998 1013 2030 13216 3348 5019 1007 1999 1996 3955 1998 3865 1998 3458 1998 2672 2027 1005 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this has got to be the worst piece of crap i have ever seen . randy qu ##aid funny as a supporting actor in the original , but not as the leading . too much eddie . the original is a classic . like the original , the supporting actor carried the show . ed as ##ner was very funny . he is the eddie of christmas va ##ction 2 ! how come s ##not ##s still looks the same after all those years and everyone else in the original looks 14 years older . after 15 minutes of watching this movie i don ' t believe i had even laughed yet . the island thing just didn ' t work . if it wasn ' [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] this has got to be the worst piece of crap i have ever seen . randy qu ##aid funny as a supporting actor in the original , but not as the leading . too much eddie . the original is a classic . like the original , the supporting actor carried the show . ed as ##ner was very funny . he is the eddie of christmas va ##ction 2 ! how come s ##not ##s still looks the same after all those years and everyone else in the original looks 14 years older . after 15 minutes of watching this movie i don ' t believe i had even laughed yet . the island thing just didn ' t work . if it wasn ' [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 2038 2288 2000 2022 1996 5409 3538 1997 10231 1045 2031 2412 2464 1012 9744 24209 14326 6057 2004 1037 4637 3364 1999 1996 2434 1010 2021 2025 2004 1996 2877 1012 2205 2172 5752 1012 1996 2434 2003 1037 4438 1012 2066 1996 2434 1010 1996 4637 3364 3344 1996 2265 1012 3968 2004 3678 2001 2200 6057 1012 2002 2003 1996 5752 1997 4234 12436 7542 1016 999 2129 2272 1055 17048 2015 2145 3504 1996 2168 2044 2035 2216 2086 1998 3071 2842 1999 1996 2434 3504 2403 2086 3080 1012 2044 2321 2781 1997 3666 2023 3185 1045 2123 1005 1056 2903 1045 2018 2130 4191 2664 1012 1996 2479 2518 2074 2134 1005 1056 2147 1012 2065 2009 2347 1005 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2023 2038 2288 2000 2022 1996 5409 3538 1997 10231 1045 2031 2412 2464 1012 9744 24209 14326 6057 2004 1037 4637 3364 1999 1996 2434 1010 2021 2025 2004 1996 2877 1012 2205 2172 5752 1012 1996 2434 2003 1037 4438 1012 2066 1996 2434 1010 1996 4637 3364 3344 1996 2265 1012 3968 2004 3678 2001 2200 6057 1012 2002 2003 1996 5752 1997 4234 12436 7542 1016 999 2129 2272 1055 17048 2015 2145 3504 1996 2168 2044 2035 2216 2086 1998 3071 2842 1999 1996 2434 3504 2403 2086 3080 1012 2044 2321 2781 1997 3666 2023 3185 1045 2123 1005 1056 2903 1045 2018 2130 4191 2664 1012 1996 2479 2518 2074 2134 1005 1056 2147 1012 2065 2009 2347 1005 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] in mississippi , the former blues man lazarus ( samuel l . jackson ) is in crisis , missing his wife that has just left him . he finds the town sl ##ut and ny ##mp ##hom ##ania ##c rae ( ch ##rit ##ina ric ##ci ) dumped on the road nearby his little farm , drugged , beaten and almost dead . lazarus brings her home , giving medicine and nursing and no ##uri ##shing her like a father , keeping her chained to control her heat . when her boyfriend ronnie ( justin timber ##lake ) is discharged from the army due to his anxiety issue , he mis ##under ##stand ##s the relationship of lazarus and rae , and tries to kill him [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] in mississippi , the former blues man lazarus ( samuel l . jackson ) is in crisis , missing his wife that has just left him . he finds the town sl ##ut and ny ##mp ##hom ##ania ##c rae ( ch ##rit ##ina ric ##ci ) dumped on the road nearby his little farm , drugged , beaten and almost dead . lazarus brings her home , giving medicine and nursing and no ##uri ##shing her like a father , keeping her chained to control her heat . when her boyfriend ronnie ( justin timber ##lake ) is discharged from the army due to his anxiety issue , he mis ##under ##stand ##s the relationship of lazarus and rae , and tries to kill him [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1999 5900 1010 1996 2280 5132 2158 23623 1006 5212 1048 1012 4027 1007 2003 1999 5325 1010 4394 2010 2564 2008 2038 2074 2187 2032 1012 2002 4858 1996 2237 22889 4904 1998 6396 8737 23393 13241 2278 14786 1006 10381 14778 3981 26220 6895 1007 14019 2006 1996 2346 3518 2010 2210 3888 1010 25483 1010 7854 1998 2471 2757 1012 23623 7545 2014 2188 1010 3228 4200 1998 8329 1998 2053 9496 12227 2014 2066 1037 2269 1010 4363 2014 22075 2000 2491 2014 3684 1012 2043 2014 6898 11688 1006 6796 7227 14530 1007 2003 14374 2013 1996 2390 2349 2000 2010 10089 3277 1010 2002 28616 20824 21515 2015 1996 3276 1997 23623 1998 14786 1010 1998 5363 2000 3102 2032 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1999 5900 1010 1996 2280 5132 2158 23623 1006 5212 1048 1012 4027 1007 2003 1999 5325 1010 4394 2010 2564 2008 2038 2074 2187 2032 1012 2002 4858 1996 2237 22889 4904 1998 6396 8737 23393 13241 2278 14786 1006 10381 14778 3981 26220 6895 1007 14019 2006 1996 2346 3518 2010 2210 3888 1010 25483 1010 7854 1998 2471 2757 1012 23623 7545 2014 2188 1010 3228 4200 1998 8329 1998 2053 9496 12227 2014 2066 1037 2269 1010 4363 2014 22075 2000 2491 2014 3684 1012 2043 2014 6898 11688 1006 6796 7227 14530 1007 2003 14374 2013 1996 2390 2349 2000 2010 10089 3277 1010 2002 28616 20824 21515 2015 1996 3276 1997 23623 1998 14786 1010 1998 5363 2000 3102 2032 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: None\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] for me this is a good series . i am kind of disappointed that ana lucia and libby died but more upset that mic ##hea ##l lied to everyone about who killed them . and if any one can answer this what was that guys name who was supposedly \" henry gale \" he was like the leader of the others ( or was that ms . clue ? ? ) anyway if you know his name cool . well trying to think of what can possibly happen next after they finally didn ' t press the button . does that mess up the whole dar ##ma project thing and i personally thought it was cool how they had that key thing underground that desmond used [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] for me this is a good series . i am kind of disappointed that ana lucia and libby died but more upset that mic ##hea ##l lied to everyone about who killed them . and if any one can answer this what was that guys name who was supposedly \" henry gale \" he was like the leader of the others ( or was that ms . clue ? ? ) anyway if you know his name cool . well trying to think of what can possibly happen next after they finally didn ' t press the button . does that mess up the whole dar ##ma project thing and i personally thought it was cool how they had that key thing underground that desmond used [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2005 2033 2023 2003 1037 2204 2186 1012 1045 2572 2785 1997 9364 2008 9617 12337 1998 19533 2351 2021 2062 6314 2008 23025 20192 2140 9828 2000 3071 2055 2040 2730 2068 1012 1998 2065 2151 2028 2064 3437 2023 2054 2001 2008 4364 2171 2040 2001 10743 1000 2888 14554 1000 2002 2001 2066 1996 3003 1997 1996 2500 1006 2030 2001 2008 5796 1012 9789 1029 1029 1007 4312 2065 2017 2113 2010 2171 4658 1012 2092 2667 2000 2228 1997 2054 2064 4298 4148 2279 2044 2027 2633 2134 1005 1056 2811 1996 6462 1012 2515 2008 6752 2039 1996 2878 18243 2863 2622 2518 1998 1045 7714 2245 2009 2001 4658 2129 2027 2018 2008 3145 2518 5230 2008 16192 2109 102\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2005 2033 2023 2003 1037 2204 2186 1012 1045 2572 2785 1997 9364 2008 9617 12337 1998 19533 2351 2021 2062 6314 2008 23025 20192 2140 9828 2000 3071 2055 2040 2730 2068 1012 1998 2065 2151 2028 2064 3437 2023 2054 2001 2008 4364 2171 2040 2001 10743 1000 2888 14554 1000 2002 2001 2066 1996 3003 1997 1996 2500 1006 2030 2001 2008 5796 1012 9789 1029 1029 1007 4312 2065 2017 2113 2010 2171 4658 1012 2092 2667 2000 2228 1997 2054 2064 4298 4148 2279 2044 2027 2633 2134 1005 1056 2811 1996 6462 1012 2515 2008 6752 2039 1996 2878 18243 2863 2622 2518 1998 1045 7714 2245 2009 2001 4658 2129 2027 2018 2008 3145 2518 5230 2008 16192 2109 102\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 1 (id = 1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"ccp5trMwRtmr","colab_type":"text"},"source":["#Creating a model\n","\n","Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."]},{"cell_type":"code","metadata":{"id":"6o2a5ZIvRcJq","colab_type":"code","colab":{}},"source":["# BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n","\n","def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n","                 num_labels):\n","  \"\"\"Creates a classification model.\"\"\"\n","\n","  bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n","  bert_inputs = dict(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids)\n","  bert_outputs = bert_module(inputs=bert_inputs,  signature=\"tokens\", as_dict=True)\n","\n","  # Use \"pooled_output\" for classification tasks on an entire sentence.\n","  # Use \"sequence_outputs\" for token-level output.\n","  output_layer = bert_outputs[\"pooled_output\"]\n","\n","  hidden_size = output_layer.shape[-1].value\n","\n","  # Create our own layer to tune for politeness data.\n","  output_weights = tf.get_variable(\"output_weights\", [num_labels, hidden_size], initializer=tf.truncated_normal_initializer(stddev=0.02))\n","  output_bias = tf.get_variable(\"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n","\n","  with tf.variable_scope(\"loss\"):\n","\n","    # Dropout helps prevent overfitting\n","    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n","\n","    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n","    logits = tf.nn.bias_add(logits, output_bias)\n","    log_probs = tf.nn.log_softmax(logits, axis=-1)\n","\n","    # Convert labels into one-hot encoding\n","    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n","\n","    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n","    # If we're predicting, we want predicted labels and the probabiltiies.\n","    if is_predicting:\n","      return (predicted_labels, log_probs)\n","\n","    # If we're train/eval, compute loss between predicted and actual label\n","    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n","    loss = tf.reduce_mean(per_example_loss)\n","    return (loss, predicted_labels, log_probs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qpE0ZIDOCQzE","colab_type":"text"},"source":["Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."]},{"cell_type":"code","metadata":{"id":"FnH-AnOQ9KKW","colab_type":"code","colab":{}},"source":["# model_fn_builder actually creates our model function\n","# using the passed parameters for num_labels, learning_rate, etc.\n","def model_fn_builder(num_labels, learning_rate, num_train_steps,\n","                     num_warmup_steps):\n","  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n","  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n","    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n","\n","    input_ids = features[\"input_ids\"]\n","    input_mask = features[\"input_mask\"]\n","    segment_ids = features[\"segment_ids\"]\n","    label_ids = features[\"label_ids\"]\n","\n","    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n","    \n","    # TRAIN and EVAL\n","    if not is_predicting:\n","\n","      (loss, predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      train_op = bert.optimization.create_optimizer(\n","          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n","\n","      # Calculate evaluation metrics. \n","      def metric_fn(label_ids, predicted_labels):\n","        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n","        f1_score = tf.contrib.metrics.f1_score(\n","            label_ids,\n","            predicted_labels)\n","        auc = tf.metrics.auc(\n","            label_ids,\n","            predicted_labels)\n","        recall = tf.metrics.recall(\n","            label_ids,\n","            predicted_labels)\n","        precision = tf.metrics.precision(\n","            label_ids,\n","            predicted_labels) \n","        true_pos = tf.metrics.true_positives(\n","            label_ids,\n","            predicted_labels)\n","        true_neg = tf.metrics.true_negatives(\n","            label_ids,\n","            predicted_labels)   \n","        false_pos = tf.metrics.false_positives(\n","            label_ids,\n","            predicted_labels)  \n","        false_neg = tf.metrics.false_negatives(\n","            label_ids,\n","            predicted_labels)\n","        return {\n","            \"eval_accuracy\": accuracy,\n","            \"f1_score\": f1_score,\n","            \"auc\": auc,\n","            \"precision\": precision,\n","            \"recall\": recall,\n","            \"true_positives\": true_pos,\n","            \"true_negatives\": true_neg,\n","            \"false_positives\": false_pos,\n","            \"false_negatives\": false_neg\n","        }\n","\n","      eval_metrics = metric_fn(label_ids, predicted_labels)\n","\n","      if mode == tf.estimator.ModeKeys.TRAIN:\n","        return tf.estimator.EstimatorSpec(mode=mode,\n","          loss=loss,\n","          train_op=train_op)\n","      else:\n","          return tf.estimator.EstimatorSpec(mode=mode,\n","            loss=loss,\n","            eval_metric_ops=eval_metrics)\n","    else:\n","      (predicted_labels, log_probs) = create_model(\n","        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n","\n","      predictions = {\n","          'probabilities': log_probs,\n","          'labels': predicted_labels\n","      }\n","      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n","\n","  # Return the actual model function in the closure\n","  return model_fn\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjwJ4bTeWXD8","colab_type":"code","colab":{}},"source":["# Compute train and warmup steps from batch size\n","# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n","BATCH_SIZE = 32\n","LEARNING_RATE = 2e-5\n","NUM_TRAIN_EPOCHS = 3.0\n","# Warmup is a period of time where hte learning rate \n","# is small and gradually increases--usually helps training.\n","WARMUP_PROPORTION = 0.1\n","# Model configs\n","SAVE_CHECKPOINTS_STEPS = 500\n","SAVE_SUMMARY_STEPS = 100"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"emHf9GhfWBZ_","colab_type":"code","colab":{}},"source":["# Compute # train and warmup steps from batch size\n","num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n","\n","num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEJldMr3WYZa","colab_type":"code","colab":{}},"source":["# Specify outpit directory and number of checkpoint steps to save\n","run_config = tf.estimator.RunConfig(\n","    model_dir=OUTPUT_DIR,\n","    save_summary_steps=SAVE_SUMMARY_STEPS,\n","    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q_WebpS1X97v","colab_type":"code","outputId":"b1dbef84-494e-4233-c30f-00f85bb4a803","executionInfo":{"status":"ok","timestamp":1572612752040,"user_tz":-540,"elapsed":936,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["model_fn = model_fn_builder(\n","  num_labels=len(label_list),\n","  learning_rate=LEARNING_RATE,\n","  num_train_steps=num_train_steps,\n","  num_warmup_steps=num_warmup_steps)\n","\n","estimator = tf.estimator.Estimator(\n","  model_fn=model_fn,\n","  config=run_config,\n","  params={\"batch_size\": BATCH_SIZE})\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3fd8fc39e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Using config: {'_model_dir': 'OUTPUT_DIR_NAME', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n","graph_options {\n","  rewrite_options {\n","    meta_optimizer_iterations: ONE\n","  }\n","}\n",", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3fd8fc39e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"NOO3RfG1DYLo","colab_type":"text"},"source":["Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."]},{"cell_type":"code","metadata":{"id":"1Pv2bAlOX_-K","colab_type":"code","colab":{}},"source":["# Create an input function for training. drop_remainder = True for using TPUs.\n","train_input_fn = bert.run_classifier.input_fn_builder(\n","    features=train_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=True,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t6Nukby2EB6-","colab_type":"text"},"source":["Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."]},{"cell_type":"code","metadata":{"id":"nucD4gluYJmK","colab_type":"code","outputId":"40746e3c-d8b5-4a6c-b77c-3e95a6dc5ee0","executionInfo":{"status":"ok","timestamp":1572613597546,"user_tz":-540,"elapsed":821416,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print(f'Beginning Training!')\n","current_time = datetime.now()\n","estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n","print(\"Training took time \", datetime.now() - current_time)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Beginning Training!\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-18-aafbaf8d385f>:23: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-18-aafbaf8d385f>:23: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stdout"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Deprecated in favor of operator or tf.math.divide.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Create CheckpointSaverHook.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Create CheckpointSaverHook.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 0 into OUTPUT_DIR_NAME/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 0 into OUTPUT_DIR_NAME/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6879097, step = 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.6879097, step = 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.564738\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.564738\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.41604197, step = 100 (177.075 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.41604197, step = 100 (177.075 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.625161\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.625161\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.10498832, step = 200 (159.959 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.10498832, step = 200 (159.959 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.625965\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.625965\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.14403012, step = 300 (159.762 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.14403012, step = 300 (159.762 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.625882\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:global_step/sec: 0.625882\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.0039805495, step = 400 (159.771 sec)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:loss = 0.0039805495, step = 400 (159.771 sec)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 468 into OUTPUT_DIR_NAME/model.ckpt.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving checkpoints for 468 into OUTPUT_DIR_NAME/model.ckpt.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: 0.0031827232.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Loss for final step: 0.0031827232.\n"],"name":"stderr"},{"output_type":"stream","text":["Training took time  0:13:40.486579\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CmbLTVniARy3","colab_type":"text"},"source":["Now let's use our test data to see how well our model did:"]},{"cell_type":"code","metadata":{"id":"JIhejfpyJ8Bx","colab_type":"code","colab":{}},"source":["test_input_fn = run_classifier.input_fn_builder(\n","    features=test_features,\n","    seq_length=MAX_SEQ_LENGTH,\n","    is_training=False,\n","    drop_remainder=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PPVEXhNjYXC-","colab_type":"code","outputId":"9e88d10f-f918-42ee-b45b-5d03f07aeab2","executionInfo":{"status":"ok","timestamp":1572615619067,"user_tz":-540,"elapsed":108993,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":667}},"source":["estimator.evaluate(input_fn=test_input_fn, steps=None)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-11-01T13:38:43Z\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Starting evaluation at 2019-11-01T13:38:43Z\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-11-01-13:40:14\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished evaluation at 2019-11-01-13:40:14\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 468: auc = 0.8673169, eval_accuracy = 0.8672, f1_score = 0.86677366, false_negatives = 306.0, false_positives = 358.0, global_step = 468, loss = 0.5206094, precision = 0.85782367, recall = 0.8759124, true_negatives = 2176.0, true_positives = 2160.0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving dict for global step 468: auc = 0.8673169, eval_accuracy = 0.8672, f1_score = 0.86677366, false_negatives = 306.0, false_positives = 358.0, global_step = 468, loss = 0.5206094, precision = 0.85782367, recall = 0.8759124, true_negatives = 2176.0, true_positives = 2160.0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saving 'checkpoint_path' summary for global step 468: OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["{'auc': 0.8673169,\n"," 'eval_accuracy': 0.8672,\n"," 'f1_score': 0.86677366,\n"," 'false_negatives': 306.0,\n"," 'false_positives': 358.0,\n"," 'global_step': 468,\n"," 'loss': 0.5206094,\n"," 'precision': 0.85782367,\n"," 'recall': 0.8759124,\n"," 'true_negatives': 2176.0,\n"," 'true_positives': 2160.0}"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"ueKsULteiz1B","colab_type":"text"},"source":["Now let's write code to make predictions on new sentences:"]},{"cell_type":"code","metadata":{"id":"OsrbTD2EJTVl","colab_type":"code","colab":{}},"source":["def getPrediction(in_sentences):\n","  labels = [\"Negative\", \"Positive\"]\n","  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n","  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n","  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n","  predictions = estimator.predict(predict_input_fn)\n","  return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-thbodgih_VJ","colab_type":"code","colab":{}},"source":["pred_sentences = [\n","  \"That movie was absolutely awful\",\n","  \"The acting was a bit lacking\",\n","  \"The film was creative and surprising\",\n","  \"Absolutely fantastic!\"\n","]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QrZmvZySKQTm","colab_type":"code","outputId":"e175fa2c-397a-47ee-f692-5f7f77308387","executionInfo":{"status":"ok","timestamp":1572616096532,"user_tz":-540,"elapsed":6271,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["predictions = getPrediction(pred_sentences)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 4\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Writing example 0 of 4\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] that movie was absolutely awful [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 2008 3185 2001 7078 9643 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the acting was a bit lacking [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 3772 2001 1037 2978 11158 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] the film was creative and surprising [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 1996 2143 2001 5541 1998 11341 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Example ***\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:guid: \n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:tokens: [CLS] absolutely fantastic ! [SEP]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_ids: 101 7078 10392 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:input_mask: 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:label: 0 (id = 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done calling model_fn.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Graph was finalized.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from OUTPUT_DIR_NAME/model.ckpt-468\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Running local_init_op.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Done running local_init_op.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"MXkRiEBUqN3n","colab_type":"text"},"source":["Voila! We have a sentiment classifier!"]},{"cell_type":"code","metadata":{"id":"ERkTE8-7oQLZ","colab_type":"code","outputId":"2c75c307-2934-4539-a8cf-6f64c7b5f6fd","executionInfo":{"status":"ok","timestamp":1572616099328,"user_tz":-540,"elapsed":908,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":233}},"source":["predictions"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('That movie was absolutely awful',\n","  array([-2.8284090e-03, -5.8694515e+00], dtype=float32),\n","  'Negative'),\n"," ('The acting was a bit lacking',\n","  array([-0.1764962, -1.8214064], dtype=float32),\n","  'Negative'),\n"," ('The film was creative and surprising',\n","  array([-6.2206898e+00, -1.9898869e-03], dtype=float32),\n","  'Positive'),\n"," ('Absolutely fantastic!',\n","  array([-4.7243433 , -0.00891621], dtype=float32),\n","  'Positive')]"]},"metadata":{"tags":[]},"execution_count":36}]}]}