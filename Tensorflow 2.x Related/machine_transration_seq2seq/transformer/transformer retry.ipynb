{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"transformer retry.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNR/x2sXDCC64VoygSBwF5p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"5af50612ceff48e9a4117a9b5c5dfaf7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_676e11f25f4d4f38b0705df4ee63aff2","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9f96f72c5daf4e57be73345bd777d745","IPY_MODEL_4dbc8051ea9242c184eb6bf2c4c1ab63"]}},"676e11f25f4d4f38b0705df4ee63aff2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f96f72c5daf4e57be73345bd777d745":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8d09502c8f5341639855b4358b059656","_dom_classes":[],"description":"Dl Completed...: 100%","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6f8c7d37a4cd440f85620e8413de22c1"}},"4dbc8051ea9242c184eb6bf2c4c1ab63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6a75aaded650452ea625fa49c3d92165","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:07&lt;00:00,  7.96s/ url]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ae1a28f0f55d4f4eac7339a14eacc46b"}},"8d09502c8f5341639855b4358b059656":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6f8c7d37a4cd440f85620e8413de22c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6a75aaded650452ea625fa49c3d92165":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ae1a28f0f55d4f4eac7339a14eacc46b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b8d12525a7b64898a85f8be9602cb7a0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7e342a73fe89474c97d4d6c38158bc99","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9015a3b33e6649e3808db0462f55b6c4","IPY_MODEL_8aaf34c0bdff4d5493826568c9dfcf64"]}},"7e342a73fe89474c97d4d6c38158bc99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9015a3b33e6649e3808db0462f55b6c4":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_2df8d87bc5654846a9953f8fa3fe4f93","_dom_classes":[],"description":"Dl Size...: 100%","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_536fd9893ffd496293b059417878cdf4"}},"8aaf34c0bdff4d5493826568c9dfcf64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_588b73bb229643f8ad2bcaf12d5b9665","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 124/124 [00:07&lt;00:00, 15.65 MiB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_85b86cc2c5de4a56b2cd66d679624942"}},"2df8d87bc5654846a9953f8fa3fe4f93":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"536fd9893ffd496293b059417878cdf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"588b73bb229643f8ad2bcaf12d5b9665":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"85b86cc2c5de4a56b2cd66d679624942":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"27a7594fbe24434a85f1903ac353ab55":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_576a12d3050140068dd4bc22f1fe875b","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9c39d38a6df44601b43b85aca77b56ca","IPY_MODEL_cc16d2fb0ca24063a192dd1eec9d769c"]}},"576a12d3050140068dd4bc22f1fe875b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9c39d38a6df44601b43b85aca77b56ca":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c77835e2acf24cab8b289bf1bea31ee1","_dom_classes":[],"description":"Extraction completed...: 100%","_model_name":"IntProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_280fc297734442c2ab05bfa6905693ae"}},"cc16d2fb0ca24063a192dd1eec9d769c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5f7a372f965b42a6a8fd51f420890946","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:07&lt;00:00,  7.85s/ file]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b16025a6aaf64dbc824bbd922103e1f3"}},"c77835e2acf24cab8b289bf1bea31ee1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"280fc297734442c2ab05bfa6905693ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5f7a372f965b42a6a8fd51f420890946":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"b16025a6aaf64dbc824bbd922103e1f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ba34ad0fa0bd441da43acaec0677b399":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b2abfea978c6431284d75d36acdd7a7f","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_82489824de00455c9e9e84ceb8d55ba2","IPY_MODEL_c5e7b56e2bad408fa8c680a915bdebef"]}},"b2abfea978c6431284d75d36acdd7a7f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"82489824de00455c9e9e84ceb8d55ba2":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_797a3bf7571a41408c4cf9cea7a0c633","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d4e9b143c98d42a09a132ff4db0cf365"}},"c5e7b56e2bad408fa8c680a915bdebef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_78c703f3473d4356a7d48264d6355d20","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 51785/0 [00:15&lt;00:00, 3228.14 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_35b6ebbaf1ce46eaa240a4951c671f68"}},"797a3bf7571a41408c4cf9cea7a0c633":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"d4e9b143c98d42a09a132ff4db0cf365":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"78c703f3473d4356a7d48264d6355d20":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"35b6ebbaf1ce46eaa240a4951c671f68":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30415eb727db4a45a4c90a94afe5816d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_30be08e88224486fbf2299778ce95948","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_076d659b3ee143069c9c0187565889b6","IPY_MODEL_c4f5e83ff13a4a1ba4915235bed80537"]}},"30be08e88224486fbf2299778ce95948":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"076d659b3ee143069c9c0187565889b6":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b44f7074de3a4cfbac81f94c980fd002","_dom_classes":[],"description":" 86%","_model_name":"IntProgressModel","bar_style":"danger","max":51785,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":44693,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_65b5f79057ed43e6906f495d4021171c"}},"c4f5e83ff13a4a1ba4915235bed80537":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_123e3709b9c04ba489627a182ceb2b4e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44693/51785 [00:00&lt;00:00, 17374.66 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f1e808c3adcc454bad520cca3bcb7288"}},"b44f7074de3a4cfbac81f94c980fd002":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"65b5f79057ed43e6906f495d4021171c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"123e3709b9c04ba489627a182ceb2b4e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f1e808c3adcc454bad520cca3bcb7288":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5abd7b858e0b493d9ffc1d912199efbc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d442eea705394021bdb9e85109a6cc42","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d1ee502de3564c58a1f1506a5b05d9dd","IPY_MODEL_c76a868ab8e04b3dbf5cd0048bb519a3"]}},"d442eea705394021bdb9e85109a6cc42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d1ee502de3564c58a1f1506a5b05d9dd":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_049f6f9ce6a9437089f599d7c9b9ea57","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_876218ca2da242909017cadd2673d5cf"}},"c76a868ab8e04b3dbf5cd0048bb519a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_18ef43e33c6d459ca872f27b6820652e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1193/0 [00:00&lt;00:00, 2501.37 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c5bc23357e6642dbaee50e487bd63abc"}},"049f6f9ce6a9437089f599d7c9b9ea57":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"876218ca2da242909017cadd2673d5cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"18ef43e33c6d459ca872f27b6820652e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c5bc23357e6642dbaee50e487bd63abc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"34208ad24bdd47b7b0f3186f9add3402":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_fc062c8a220842c7bc2fd0615c475ceb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ce468ee8d7f249eca93085a8b0856b7d","IPY_MODEL_83ec5c83419e449d92df9dbf22e54953"]}},"fc062c8a220842c7bc2fd0615c475ceb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ce468ee8d7f249eca93085a8b0856b7d":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1d5e21e257534b10b16c8736beccd14a","_dom_classes":[],"description":"  0%","_model_name":"IntProgressModel","bar_style":"danger","max":1193,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4e5e6ae554284cab98d6041c2a90e54c"}},"83ec5c83419e449d92df9dbf22e54953":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9c014b478fb64f31a9e249dbaad977d7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/1193 [00:00&lt;?, ? examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1a3900992ff04919b16b7d142f25159c"}},"1d5e21e257534b10b16c8736beccd14a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4e5e6ae554284cab98d6041c2a90e54c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9c014b478fb64f31a9e249dbaad977d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"1a3900992ff04919b16b7d142f25159c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4bebf28dc9144383b32028ae9f376377":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3f7e18fea31b4f59b61f164b624e2464","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_94e49ddfe1ce4501a9513f1e227e7bda","IPY_MODEL_682c02eb0fa84d3897f46bc0584c1505"]}},"3f7e18fea31b4f59b61f164b624e2464":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"94e49ddfe1ce4501a9513f1e227e7bda":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d5f031429c9d4690a870a4edf1e7641c","_dom_classes":[],"description":"","_model_name":"IntProgressModel","bar_style":"info","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_4d315ebd59f24c9a96912a6b7cb5ced8"}},"682c02eb0fa84d3897f46bc0584c1505":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c7e2c0db683742bda2f5e6795be0f8be","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1803/0 [00:00&lt;00:00, 2520.47 examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_09165add0bcd4d1994da58c2730acc7a"}},"d5f031429c9d4690a870a4edf1e7641c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"4d315ebd59f24c9a96912a6b7cb5ced8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c7e2c0db683742bda2f5e6795be0f8be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"09165add0bcd4d1994da58c2730acc7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"548c6e0d75e9443e939b5b52e9ddd910":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a86b9229841543ccb69b19dc6b054603","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8985c468b7f541ea803e342dd94adbea","IPY_MODEL_36ef99ea40014907a63a7a518d92575c"]}},"a86b9229841543ccb69b19dc6b054603":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8985c468b7f541ea803e342dd94adbea":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d4aa504bea0c45ce983d790b7e6516b6","_dom_classes":[],"description":"  0%","_model_name":"IntProgressModel","bar_style":"danger","max":1803,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1d4d7f964920417bb00af10e22b062d9"}},"36ef99ea40014907a63a7a518d92575c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_659eac8bf8a44f26b41432185d3d424c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/1803 [00:05&lt;?, ? examples/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8efbdec68aaf403e80a991dec423a015"}},"d4aa504bea0c45ce983d790b7e6516b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1d4d7f964920417bb00af10e22b062d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"659eac8bf8a44f26b41432185d3d424c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8efbdec68aaf403e80a991dec423a015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"755PNdlJ98ad","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8Kf3SDM2VH3u","colab_type":"text"},"source":["Load Data\n","\n","The purpose of the task is machine translatioin with transfomer (Non using any rnn or cnn)"]},{"cell_type":"code","metadata":{"id":"_CN1zeMmVRi_","colab_type":"code","outputId":"9acbe14b-fde2-49ac-e18d-21a7273f3cb6","executionInfo":{"status":"ok","timestamp":1585835677170,"user_tz":-540,"elapsed":34907,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":416,"referenced_widgets":["5af50612ceff48e9a4117a9b5c5dfaf7","676e11f25f4d4f38b0705df4ee63aff2","9f96f72c5daf4e57be73345bd777d745","4dbc8051ea9242c184eb6bf2c4c1ab63","8d09502c8f5341639855b4358b059656","6f8c7d37a4cd440f85620e8413de22c1","6a75aaded650452ea625fa49c3d92165","ae1a28f0f55d4f4eac7339a14eacc46b","b8d12525a7b64898a85f8be9602cb7a0","7e342a73fe89474c97d4d6c38158bc99","9015a3b33e6649e3808db0462f55b6c4","8aaf34c0bdff4d5493826568c9dfcf64","2df8d87bc5654846a9953f8fa3fe4f93","536fd9893ffd496293b059417878cdf4","588b73bb229643f8ad2bcaf12d5b9665","85b86cc2c5de4a56b2cd66d679624942","27a7594fbe24434a85f1903ac353ab55","576a12d3050140068dd4bc22f1fe875b","9c39d38a6df44601b43b85aca77b56ca","cc16d2fb0ca24063a192dd1eec9d769c","c77835e2acf24cab8b289bf1bea31ee1","280fc297734442c2ab05bfa6905693ae","5f7a372f965b42a6a8fd51f420890946","b16025a6aaf64dbc824bbd922103e1f3","ba34ad0fa0bd441da43acaec0677b399","b2abfea978c6431284d75d36acdd7a7f","82489824de00455c9e9e84ceb8d55ba2","c5e7b56e2bad408fa8c680a915bdebef","797a3bf7571a41408c4cf9cea7a0c633","d4e9b143c98d42a09a132ff4db0cf365","78c703f3473d4356a7d48264d6355d20","35b6ebbaf1ce46eaa240a4951c671f68","30415eb727db4a45a4c90a94afe5816d","30be08e88224486fbf2299778ce95948","076d659b3ee143069c9c0187565889b6","c4f5e83ff13a4a1ba4915235bed80537","b44f7074de3a4cfbac81f94c980fd002","65b5f79057ed43e6906f495d4021171c","123e3709b9c04ba489627a182ceb2b4e","f1e808c3adcc454bad520cca3bcb7288","5abd7b858e0b493d9ffc1d912199efbc","d442eea705394021bdb9e85109a6cc42","d1ee502de3564c58a1f1506a5b05d9dd","c76a868ab8e04b3dbf5cd0048bb519a3","049f6f9ce6a9437089f599d7c9b9ea57","876218ca2da242909017cadd2673d5cf","18ef43e33c6d459ca872f27b6820652e","c5bc23357e6642dbaee50e487bd63abc","34208ad24bdd47b7b0f3186f9add3402","fc062c8a220842c7bc2fd0615c475ceb","ce468ee8d7f249eca93085a8b0856b7d","83ec5c83419e449d92df9dbf22e54953","1d5e21e257534b10b16c8736beccd14a","4e5e6ae554284cab98d6041c2a90e54c","9c014b478fb64f31a9e249dbaad977d7","1a3900992ff04919b16b7d142f25159c","4bebf28dc9144383b32028ae9f376377","3f7e18fea31b4f59b61f164b624e2464","94e49ddfe1ce4501a9513f1e227e7bda","682c02eb0fa84d3897f46bc0584c1505","d5f031429c9d4690a870a4edf1e7641c","4d315ebd59f24c9a96912a6b7cb5ced8","c7e2c0db683742bda2f5e6795be0f8be","09165add0bcd4d1994da58c2730acc7a","548c6e0d75e9443e939b5b52e9ddd910","a86b9229841543ccb69b19dc6b054603","8985c468b7f541ea803e342dd94adbea","36ef99ea40014907a63a7a518d92575c","d4aa504bea0c45ce983d790b7e6516b6","1d4d7f964920417bb00af10e22b062d9","659eac8bf8a44f26b41432185d3d424c","8efbdec68aaf403e80a991dec423a015"]}},"source":["examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True, as_supervised=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[1mDownloading and preparing dataset ted_hrlr_translate/pt_to_en/1.0.0 (download: 124.94 MiB, generated: Unknown size, total: 124.94 MiB) to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0...\u001b[0m\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5af50612ceff48e9a4117a9b5c5dfaf7","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Completed...', max=1, style=ProgressStyl…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b8d12525a7b64898a85f8be9602cb7a0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', description='Dl Size...', max=1, style=ProgressStyle(des…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"27a7594fbe24434a85f1903ac353ab55","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', description='Extraction completed...', max=1, style=Prog…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba34ad0fa0bd441da43acaec0677b399","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteR33F39/ted_hrlr_translate-train.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30415eb727db4a45a4c90a94afe5816d","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=51785), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5abd7b858e0b493d9ffc1d912199efbc","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteR33F39/ted_hrlr_translate-validation.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"34208ad24bdd47b7b0f3186f9add3402","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=1193), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4bebf28dc9144383b32028ae9f376377","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\rShuffling and writing examples to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0.incompleteR33F39/ted_hrlr_translate-test.tfrecord\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"548c6e0d75e9443e939b5b52e9ddd910","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, max=1803), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\u001b[1mDataset ted_hrlr_translate downloaded and prepared to /root/tensorflow_datasets/ted_hrlr_translate/pt_to_en/1.0.0. Subsequent calls will reuse this data.\u001b[0m\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hjflxvHMVYov","colab_type":"code","colab":{}},"source":["train_example, val_example = examples['train'], examples['validation']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kJjdVeBbVlQL","colab_type":"code","outputId":"14c7032a-a7b8-4168-ca37-ade3d2efc7f8","executionInfo":{"status":"ok","timestamp":1585835677375,"user_tz":-540,"elapsed":35101,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":241}},"source":["# Checking loaded data\n","\n","for n, i in enumerate(train_example.take(1)):\n","  print(i)\n","print('\\n')\n","  # i <- (prtuguese, english)\n","\n","for n, (pt, en) in enumerate(train_example.take(1)):\n","  print(pt)\n","  print()\n","  print(en)\n","\n","  print('\\nNumpy Format Below\\n')\n","  print(pt.numpy())\n","  print()\n","  print(en.numpy())"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(<tf.Tensor: shape=(), dtype=string, numpy=b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .'>, <tf.Tensor: shape=(), dtype=string, numpy=b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'>)\n","\n","\n","tf.Tensor(b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .', shape=(), dtype=string)\n","\n","tf.Tensor(b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .', shape=(), dtype=string)\n","\n","Numpy Format Below\n","\n","b'e quando melhoramos a procura , tiramos a \\xc3\\xbanica vantagem da impress\\xc3\\xa3o , que \\xc3\\xa9 a serendipidade .'\n","\n","b'and when you improve searchability , you actually take away the one advantage of print , which is serendipity .'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GgqpmWptV-Y5","colab_type":"code","colab":{}},"source":["# Create Tokenizer with tfds word peice\n"," \n"," tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus((en.numpy() for pt, en in train_example), \n","                                                                        target_vocab_size=2**13)\n"," \n"," tokenizer_pt = tfds.features.text.SubwordTextEncoder.build_from_corpus((pt.numpy() for pt, en in train_example), \n","                                                                        target_vocab_size=2**13)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"49LKM2k1YgEA","colab_type":"code","outputId":"ed042092-e8de-4e8b-fdf1-2e589b0aa60c","executionInfo":{"status":"ok","timestamp":1585835841948,"user_tz":-540,"elapsed":199663,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["# Testing tokenizer encode and decode\n","\n","sample_string = 'becarefull the covid 19'\n","\n","tokenized_string = tokenizer_en.encode(sample_string)\n","print('Tokenized string : {}'.format(tokenized_string))\n","\n","original_string = tokenizer_en.decode(tokenized_string)\n","print('Reverted string : {}'.format(original_string))\n","\n","\n","# Check how the each word converted\n","for word in tokenized_string:\n","  print('{} -> {}'.format(word, tokenizer_en.decode([word])))"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Tokenized string : [370, 3509, 246, 3, 759, 1259, 49, 1291]\n","Reverted string : becarefull the covid 19\n","370 -> be\n","3509 -> careful\n","246 -> l \n","3 -> the \n","759 -> co\n","1259 -> vi\n","49 -> d \n","1291 -> 19\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0kwXPktUY-v8","colab_type":"code","colab":{}},"source":["# Set the Hyperparam for data pipeline\n","BUFFER_SIZE = 20000\n","BATCH_SIZE = 64"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mu6b1uiLZdS8","colab_type":"code","colab":{}},"source":["# Add SOS & EOS\n","# SOS token = tokenizer.vocab_size      <- number\n","# EOS token = tokenizer.vocab_size + 1  <- number\n","def encode(lang1, lang2):\n","  '''\n","  lang1 = pt\n","  lang2 = en\n","  '''\n","  lang1 = [tokenizer_pt.vocab_size] + tokenizer_pt.encode(lang1.numpy()) + [tokenizer_pt.vocab_size + 1]\n","  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(lang2.numpy()) + [tokenizer_en.vocab_size + 1]\n","  return lang1, lang2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pccrx4BvaGiS","colab_type":"code","colab":{}},"source":["# To use the tf data format, convert python function to tf function\n","\n","def tf_encode(pt, en):\n","  result_pt, result_en = tf.py_function(encode, [pt, en], [tf.int64, tf.int64])\n","  result_pt.set_shape([None])\n","  result_en.set_shape([None])\n","  return result_pt, result_en"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E3857WaVadkx","colab_type":"code","colab":{}},"source":["# To reduce the training time, reduce its length below 40\n","\n","MAX_LENGTH = 40\n","\n","def filter_max_length(x, y, max_length=MAX_LENGTH):\n","  return tf.logical_and(tf.size(x) <= max_length, \n","                        tf.size(y) <= max_length)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKRZx2LHajwP","colab_type":"code","colab":{}},"source":["# Preprocessed\n","\n","train_preprocessed = train_example.map(tf_encode).filter(filter_max_length).cache().shuffle(BUFFER_SIZE)\n","\n","val_preprocessed = val_example.map(tf_encode).filter(filter_max_length)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xy581heFbZUi","colab_type":"code","colab":{}},"source":["# Pad the string for each element has the same length\n","\n","train_dataset = train_preprocessed.padded_batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n","val_dataset = val_preprocessed.padded_batch(BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2JsNQ4mbncV","colab_type":"code","outputId":"2f976f59-02e5-4b1c-ac4e-8edcd8445676","executionInfo":{"status":"ok","timestamp":1585835842550,"user_tz":-540,"elapsed":200245,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["# Check the data\n","\n","pt_batch, en_batch = next(iter(val_dataset))\n","\n","pt_batch, en_batch"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(64, 38), dtype=int64, numpy=\n"," array([[8214,  342, 3032, ...,    0,    0,    0],\n","        [8214,   95,  198, ...,    0,    0,    0],\n","        [8214, 4479, 7990, ...,    0,    0,    0],\n","        ...,\n","        [8214,  584,   12, ...,    0,    0,    0],\n","        [8214,   59, 1548, ...,    0,    0,    0],\n","        [8214,  118,   34, ...,    0,    0,    0]])>,\n"," <tf.Tensor: shape=(64, 40), dtype=int64, numpy=\n"," array([[8087,   98,   25, ...,    0,    0,    0],\n","        [8087,   12,   20, ...,    0,    0,    0],\n","        [8087,   12, 5453, ...,    0,    0,    0],\n","        ...,\n","        [8087,   18, 2059, ...,    0,    0,    0],\n","        [8087,   16, 1436, ...,    0,    0,    0],\n","        [8087,   15,   57, ...,    0,    0,    0]])>)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"58o_YFLFby8S","colab_type":"text"},"source":["## Important\n","\n","### Positional Encoding\n","\n","The transfomer does not use any rnn or cnn, so it requires to add a positional information to the data\n","\n","\n","- The paper used \"sin & cos encoding\"\n","$${k} = {2i}$$\n","$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n","----------\n","$${k} = {2i + 1}$$\n","$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$\n","\n","by following the fomula,\n","\n","- pos refers length of the sequence\n","- d_model refers embedding dimention\n","- ${2i}$ refers time-step or index of the sequence"]},{"cell_type":"code","metadata":{"id":"DdpA4po5b2BA","colab_type":"code","colab":{}},"source":["def get_angles_test(pos, i, d_model):\n","  # angle_rate = 1/1000^(2i/d_model)\n","  print('Pos: \\n',pos)\n","  print('I: \\n', i)\n","  print('I//2 \\n', i//2)\n","  print('D_Model: \\n',d_model)\n","  angle_rate = 1 / np.power(10000, (2 * (i // 2) / np.float32(d_model)))\n","  print('Angle Rate: \\n', angle_rate)\n","  \n","  return pos * angle_rate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XAsZFxwZj7Rb","colab_type":"code","colab":{}},"source":["def get_angles(pos, i, d_model):\n","  # angle_rate = 1/1000^(2i/d_model)\n","  angle_rate = 1 / np.power(10000, (2 * (i // 2) / np.float32(d_model)))\n","\n","  \n","  return pos * angle_rate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XZ-NThYFddaZ","colab_type":"code","colab":{}},"source":["def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis], # (seq_len, 1)\n","                          np.arange(d_model)[np.newaxis, :],  # (1, emb_dim)\n","                          d_model)                            # (emb_dim)\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","\n","  pos_encoding = angle_rads[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.float32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"c1bafd4f-64ca-49d5-a925-f8c6a6be38df","executionInfo":{"status":"ok","timestamp":1585835842552,"user_tz":-540,"elapsed":200233,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"id":"QdLAuT0of-4d","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["get_angles_test(10, 4, 4)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Pos: \n"," 10\n","I: \n"," 4\n","I//2 \n"," 2\n","D_Model: \n"," 4\n","Angle Rate: \n"," 0.0001\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.001"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"7YXq9La2gH1E","colab_type":"code","outputId":"b07c48e6-2b99-4b0a-e8d8-47e679d90e57","executionInfo":{"status":"ok","timestamp":1585837820523,"user_tz":-540,"elapsed":578,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["print('pos result shape', get_angles_test(np.arange(10)[:, np.newaxis], \n","                                          np.arange(10)[np.newaxis, :], \n","                                          10).shape)\n","\n","get_angles_test(np.arange(10)[:, np.newaxis], \n","           np.arange(10)[np.newaxis, :], \n","           10)\n","\n","\n","# get_angles(np.arange(10)[:, np.newaxis], \n","#            np.arange(4)[np.newaxis, :], \n","#            4)[:, 0::2] "],"execution_count":60,"outputs":[{"output_type":"stream","text":["Pos: \n"," [[0]\n"," [1]\n"," [2]\n"," [3]\n"," [4]\n"," [5]\n"," [6]\n"," [7]\n"," [8]\n"," [9]]\n","I: \n"," [[0 1 2 3 4 5 6 7 8 9]]\n","I//2 \n"," [[0 0 1 1 2 2 3 3 4 4]]\n","D_Model: \n"," 10\n","Angle Rate: \n"," [[1.         1.         0.15848932 0.15848932 0.02511886 0.02511886\n","  0.00398107 0.00398107 0.00063096 0.00063096]]\n","pos result shape (10, 10)\n","Pos: \n"," [[0]\n"," [1]\n"," [2]\n"," [3]\n"," [4]\n"," [5]\n"," [6]\n"," [7]\n"," [8]\n"," [9]]\n","I: \n"," [[0 1 2 3 4 5 6 7 8 9]]\n","I//2 \n"," [[0 0 1 1 2 2 3 3 4 4]]\n","D_Model: \n"," 10\n","Angle Rate: \n"," [[1.         1.         0.15848932 0.15848932 0.02511886 0.02511886\n","  0.00398107 0.00398107 0.00063096 0.00063096]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n","        0.        , 0.        , 0.        , 0.        , 0.        ],\n","       [1.        , 1.        , 0.15848932, 0.15848932, 0.02511886,\n","        0.02511886, 0.00398107, 0.00398107, 0.00063096, 0.00063096],\n","       [2.        , 2.        , 0.31697864, 0.31697864, 0.05023773,\n","        0.05023773, 0.00796214, 0.00796214, 0.00126191, 0.00126191],\n","       [3.        , 3.        , 0.47546796, 0.47546796, 0.07535659,\n","        0.07535659, 0.01194322, 0.01194322, 0.00189287, 0.00189287],\n","       [4.        , 4.        , 0.63395728, 0.63395728, 0.10047546,\n","        0.10047546, 0.01592429, 0.01592429, 0.00252383, 0.00252383],\n","       [5.        , 5.        , 0.7924466 , 0.7924466 , 0.12559432,\n","        0.12559432, 0.01990536, 0.01990536, 0.00315479, 0.00315479],\n","       [6.        , 6.        , 0.95093592, 0.95093592, 0.15071319,\n","        0.15071319, 0.02388643, 0.02388643, 0.00378574, 0.00378574],\n","       [7.        , 7.        , 1.10942523, 1.10942523, 0.17583205,\n","        0.17583205, 0.0278675 , 0.0278675 , 0.0044167 , 0.0044167 ],\n","       [8.        , 8.        , 1.26791455, 1.26791455, 0.20095091,\n","        0.20095091, 0.03184857, 0.03184857, 0.00504766, 0.00504766],\n","       [9.        , 9.        , 1.42640387, 1.42640387, 0.22606978,\n","        0.22606978, 0.03582965, 0.03582965, 0.00567862, 0.00567862]])"]},"metadata":{"tags":[]},"execution_count":60}]},{"cell_type":"code","metadata":{"id":"XMuVnXTwTY84","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"55e9ace3-5b29-4948-e00d-d8b3c688b477","executionInfo":{"status":"ok","timestamp":1585837890541,"user_tz":-540,"elapsed":544,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["tmp_pos = positional_encoding(50, 128)\n","\n","tmp_pos.shape"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([1, 50, 128])"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"rBY66l2Mj4HW","colab_type":"code","colab":{}},"source":["# Masking\n","# - Mask all the pad tokens in the batch of sequence\n","\n","def create_padding_mask(seq):\n","  seq = tf.math.equal(seq, 0)\n","  seq = tf.cast(seq, tf.float32) # seq = (b, seq_len)\n","  return seq[:, tf.newaxis, tf.newaxis, :] # return (b, 1, 1, seq_len)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGioGFzoxzAi","colab_type":"code","outputId":"0da45b02-839b-4bef-ba8a-6667cf34794e","executionInfo":{"status":"ok","timestamp":1585835842553,"user_tz":-540,"elapsed":200215,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["x = tf.constant([[7,6,0,0,0],\n","                 [1,2,3,0,0],\n","                 [0,0,0,4,5]])\n","\n","create_padding_mask(x)"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n","array([[[[0., 0., 1., 1., 1.]]],\n","\n","\n","       [[[0., 0., 0., 1., 1.]]],\n","\n","\n","       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"cOOpuoozx5bA","colab_type":"code","colab":{}},"source":["# Create Look-ahead-mask \n","# It is for the futre tokens in a sequence\n","# the maks indicates which entries should not be used\n","# When predict the 3rd words, only use the 1st and 2nd words\n","\n","# For example, \n","# [1,2,3,4,5,6]\n","# when predicting 4, use only 1,2,3,  Not 4, 5, 6\n","# So the mask should be [1,1,1,0,0,0]\n","# To set the mask has a role for define where \"NOT\" to use, the mask seq should be [0,0,0,1,1,1]\n","\n","\n","def create_look_ahead_mask(size):\n","  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","  return mask"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqrO4F5hyu_i","colab_type":"code","outputId":"5eb5d430-2c0e-42a3-9efe-29b456780744","executionInfo":{"status":"ok","timestamp":1585835842553,"user_tz":-540,"elapsed":200205,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["x = tf.random.uniform((1,3))\n","\n","temp = create_look_ahead_mask(x.shape[1])\n","temp"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n","array([[0., 1., 1.],\n","       [0., 0., 1.],\n","       [0., 0., 0.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"lI31ybtOzPKj","colab_type":"text"},"source":["## Model Architectures\n","\n","### 1) Scaled - Dot product Attention\n","\n","\n","https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms\n","\n","https://medium.com/@bgg/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d\n","\n","Requires to Check "]},{"cell_type":"code","metadata":{"id":"9aZsE9dry0fK","colab_type":"code","colab":{}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  # add mask to the attention weight\n","  \n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","  output = tf.matmul(attention_weights, v)\n","  return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EWiTGMPX6HYn","colab_type":"code","outputId":"38400504-a10b-4849-c0cb-074785d7d17f","executionInfo":{"status":"ok","timestamp":1585840475182,"user_tz":-540,"elapsed":1476,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["-1e9"],"execution_count":116,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-1000000000.0"]},"metadata":{"tags":[]},"execution_count":116}]},{"cell_type":"code","metadata":{"id":"qjejTbpnd3_6","colab_type":"code","colab":{}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","  \"\"\"Calculate the attention weights.\n","  q, k, v must have matching leading dimensions.\n","  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","  The mask has different shapes depending on its type(padding or look ahead) \n","  but it must be broadcastable for addition.\n","  \n","  Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","    \n","  Returns:\n","    output, attention_weights\n","  \"\"\"\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","  \n","  # scale matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","  return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iWjBFgIyTdSy","colab_type":"text"},"source":["## Scaled Dot Attention\n","\n","if the query is align with which value,\n","by following the attention weight, the values are avaraged."]},{"cell_type":"code","metadata":{"id":"hN5bofLJ6d6I","colab_type":"code","colab":{}},"source":["def scaled_dot_product_attention_temp(q, k, v, mask):\n","  print('q', q)\n","  print('k', k)\n","  print('v', v)\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)\n","  print('Matmul QK', matmul_qk)\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  print('D_k', dk)\n","\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","  print('QK / sqrt(d_k)', scaled_attention_logits)\n","\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  # add mask to the attention weight\n","    print('Masked Logits', scaled_attention_logits)\n","\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","  print('Attention Weights', attention_weights)\n","\n","  output = tf.matmul(attention_weights, v)\n","  return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RHFMC5Tq61wm","colab_type":"code","colab":{}},"source":["def print_out(q, k, v):\n","  temp_out, temp_attn = scaled_dot_product_attention_temp(q, k, v, None)\n","  print ('Attention weights are:')\n","  print (temp_attn)\n","  print ('Output is:')\n","  print (temp_out)\n","\n","def print_out_mask(q, k, v):\n","  temp_out, temp_attn = scaled_dot_product_attention_temp(q, k, v, True)\n","  print ('Attention weights are:')\n","  print (temp_attn)\n","  print ('Output is:')\n","  print (temp_out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpLW36g_64rC","colab_type":"code","outputId":"e4e9914f-fe72-4f48-c122-aef49fd5fc13","executionInfo":{"status":"ok","timestamp":1585840624418,"user_tz":-540,"elapsed":1717,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["np.set_printoptions(suppress=True)\n","\n","temp_k = tf.constant([[10,0,0],\n","                      [0,10,0],\n","                      [0,0,10],\n","                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n","\n","temp_v = tf.constant([[   1,0],\n","                      [  10,0],\n","                      [ 100,5],\n","                      [1000,6]], dtype=tf.float32)  # (4, 2)\n","\n","# This `query` aligns with the second `key`,\n","# so the second `value` is returned.\n","temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n","\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":161,"outputs":[{"output_type":"stream","text":["q tf.Tensor([[ 0. 10.  0.]], shape=(1, 3), dtype=float32)\n","k tf.Tensor(\n","[[10.  0.  0.]\n"," [ 0. 10.  0.]\n"," [ 0.  0. 10.]\n"," [ 0.  0. 10.]], shape=(4, 3), dtype=float32)\n","v tf.Tensor(\n","[[   1.    0.]\n"," [  10.    0.]\n"," [ 100.    5.]\n"," [1000.    6.]], shape=(4, 2), dtype=float32)\n","Matmul QK tf.Tensor([[  0. 100.   0.   0.]], shape=(1, 4), dtype=float32)\n","D_k tf.Tensor(3.0, shape=(), dtype=float32)\n","QK / sqrt(d_k) tf.Tensor([[ 0.       57.735027  0.        0.      ]], shape=(1, 4), dtype=float32)\n","Attention Weights tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n","Attention weights are:\n","tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_O5wNowhOr5b","colab_type":"code","outputId":"fd51fed8-379a-472c-b45e-ea8d6789d4ce","executionInfo":{"status":"ok","timestamp":1585840624418,"user_tz":-540,"elapsed":1701,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["tf.transpose(temp_k)"],"execution_count":162,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(3, 4), dtype=float32, numpy=\n","array([[10.,  0.,  0.,  0.],\n","       [ 0., 10.,  0.,  0.],\n","       [ 0.,  0., 10., 10.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":162}]},{"cell_type":"code","metadata":{"id":"4UGAONvrOzsX","colab_type":"code","outputId":"59059788-4d44-4314-bebd-ecbb583ab0bd","executionInfo":{"status":"ok","timestamp":1585840624419,"user_tz":-540,"elapsed":1692,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["tf.matmul(temp_q, tf.transpose(temp_k))"],"execution_count":163,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 4), dtype=float32, numpy=array([[  0., 100.,   0.,   0.]], dtype=float32)>"]},"metadata":{"tags":[]},"execution_count":163}]},{"cell_type":"code","metadata":{"id":"7Zj1k7Ll7OCR","colab_type":"code","outputId":"47a2c6d6-2637-402d-e418-4b6092578bf5","executionInfo":{"status":"ok","timestamp":1585840624419,"user_tz":-540,"elapsed":1679,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["print_out_mask(temp_q, temp_k, temp_v)"],"execution_count":164,"outputs":[{"output_type":"stream","text":["q tf.Tensor([[ 0. 10.  0.]], shape=(1, 3), dtype=float32)\n","k tf.Tensor(\n","[[10.  0.  0.]\n"," [ 0. 10.  0.]\n"," [ 0.  0. 10.]\n"," [ 0.  0. 10.]], shape=(4, 3), dtype=float32)\n","v tf.Tensor(\n","[[   1.    0.]\n"," [  10.    0.]\n"," [ 100.    5.]\n"," [1000.    6.]], shape=(4, 2), dtype=float32)\n","Matmul QK tf.Tensor([[  0. 100.   0.   0.]], shape=(1, 4), dtype=float32)\n","D_k tf.Tensor(3.0, shape=(), dtype=float32)\n","QK / sqrt(d_k) tf.Tensor([[ 0.       57.735027  0.        0.      ]], shape=(1, 4), dtype=float32)\n","Masked Logits tf.Tensor([[-1.0000000e+09 -9.9999994e+08 -1.0000000e+09 -1.0000000e+09]], shape=(1, 4), dtype=float32)\n","Attention Weights tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n","Attention weights are:\n","tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EIN68BQf8EQk","colab_type":"code","outputId":"05ec8fc0-c188-41e8-fc52-0f7c56d3fb63","executionInfo":{"status":"ok","timestamp":1585840624419,"user_tz":-540,"elapsed":1668,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":340}},"source":["temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":165,"outputs":[{"output_type":"stream","text":["q tf.Tensor([[10. 10.  0.]], shape=(1, 3), dtype=float32)\n","k tf.Tensor(\n","[[10.  0.  0.]\n"," [ 0. 10.  0.]\n"," [ 0.  0. 10.]\n"," [ 0.  0. 10.]], shape=(4, 3), dtype=float32)\n","v tf.Tensor(\n","[[   1.    0.]\n"," [  10.    0.]\n"," [ 100.    5.]\n"," [1000.    6.]], shape=(4, 2), dtype=float32)\n","Matmul QK tf.Tensor([[100. 100.   0.   0.]], shape=(1, 4), dtype=float32)\n","D_k tf.Tensor(3.0, shape=(), dtype=float32)\n","QK / sqrt(d_k) tf.Tensor([[57.735027 57.735027  0.        0.      ]], shape=(1, 4), dtype=float32)\n","Attention Weights tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n","Attention weights are:\n","tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9sEuUfkF8Egx","colab_type":"code","outputId":"c342a2b1-3b6a-4f48-a4ce-ca33e98a5aea","executionInfo":{"status":"ok","timestamp":1585840624420,"user_tz":-540,"elapsed":1658,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["print_out_mask(temp_q, temp_k, temp_v)"],"execution_count":166,"outputs":[{"output_type":"stream","text":["q tf.Tensor([[10. 10.  0.]], shape=(1, 3), dtype=float32)\n","k tf.Tensor(\n","[[10.  0.  0.]\n"," [ 0. 10.  0.]\n"," [ 0.  0. 10.]\n"," [ 0.  0. 10.]], shape=(4, 3), dtype=float32)\n","v tf.Tensor(\n","[[   1.    0.]\n"," [  10.    0.]\n"," [ 100.    5.]\n"," [1000.    6.]], shape=(4, 2), dtype=float32)\n","Matmul QK tf.Tensor([[100. 100.   0.   0.]], shape=(1, 4), dtype=float32)\n","D_k tf.Tensor(3.0, shape=(), dtype=float32)\n","QK / sqrt(d_k) tf.Tensor([[57.735027 57.735027  0.        0.      ]], shape=(1, 4), dtype=float32)\n","Masked Logits tf.Tensor([[-9.9999994e+08 -9.9999994e+08 -1.0000000e+09 -1.0000000e+09]], shape=(1, 4), dtype=float32)\n","Attention Weights tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n","Attention weights are:\n","tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"74T593IP8Oj-","colab_type":"code","outputId":"fe934d7c-8ff7-4f32-d584-4f1ee9632511","executionInfo":{"status":"ok","timestamp":1585840624420,"user_tz":-540,"elapsed":1647,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":646}},"source":["temp_q = tf.constant([[0, 0, 10], \n","                      [0, 10, 0], \n","                      [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n","                      \n","print_out(temp_q, temp_k, temp_v)"],"execution_count":167,"outputs":[{"output_type":"stream","text":["q tf.Tensor(\n","[[ 0.  0. 10.]\n"," [ 0. 10.  0.]\n"," [10. 10.  0.]], shape=(3, 3), dtype=float32)\n","k tf.Tensor(\n","[[10.  0.  0.]\n"," [ 0. 10.  0.]\n"," [ 0.  0. 10.]\n"," [ 0.  0. 10.]], shape=(4, 3), dtype=float32)\n","v tf.Tensor(\n","[[   1.    0.]\n"," [  10.    0.]\n"," [ 100.    5.]\n"," [1000.    6.]], shape=(4, 2), dtype=float32)\n","Matmul QK tf.Tensor(\n","[[  0.   0. 100. 100.]\n"," [  0. 100.   0.   0.]\n"," [100. 100.   0.   0.]], shape=(3, 4), dtype=float32)\n","D_k tf.Tensor(3.0, shape=(), dtype=float32)\n","QK / sqrt(d_k) tf.Tensor(\n","[[ 0.        0.       57.735027 57.735027]\n"," [ 0.       57.735027  0.        0.      ]\n"," [57.735027 57.735027  0.        0.      ]], shape=(3, 4), dtype=float32)\n","Attention Weights tf.Tensor(\n","[[0.  0.  0.5 0.5]\n"," [0.  1.  0.  0. ]\n"," [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n","Attention weights are:\n","tf.Tensor(\n","[[0.  0.  0.5 0.5]\n"," [0.  1.  0.  0. ]\n"," [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n","Output is:\n","tf.Tensor(\n","[[550.    5.5]\n"," [ 10.    0. ]\n"," [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZqRcQnND8QLV","colab_type":"code","outputId":"0837c97a-1394-4016-df54-636098a058db","executionInfo":{"status":"ok","timestamp":1585840624420,"user_tz":-540,"elapsed":1635,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":714}},"source":["print_out_mask(temp_q, temp_k, temp_v)"],"execution_count":168,"outputs":[{"output_type":"stream","text":["q tf.Tensor(\n","[[ 0.  0. 10.]\n"," [ 0. 10.  0.]\n"," [10. 10.  0.]], shape=(3, 3), dtype=float32)\n","k tf.Tensor(\n","[[10.  0.  0.]\n"," [ 0. 10.  0.]\n"," [ 0.  0. 10.]\n"," [ 0.  0. 10.]], shape=(4, 3), dtype=float32)\n","v tf.Tensor(\n","[[   1.    0.]\n"," [  10.    0.]\n"," [ 100.    5.]\n"," [1000.    6.]], shape=(4, 2), dtype=float32)\n","Matmul QK tf.Tensor(\n","[[  0.   0. 100. 100.]\n"," [  0. 100.   0.   0.]\n"," [100. 100.   0.   0.]], shape=(3, 4), dtype=float32)\n","D_k tf.Tensor(3.0, shape=(), dtype=float32)\n","QK / sqrt(d_k) tf.Tensor(\n","[[ 0.        0.       57.735027 57.735027]\n"," [ 0.       57.735027  0.        0.      ]\n"," [57.735027 57.735027  0.        0.      ]], shape=(3, 4), dtype=float32)\n","Masked Logits tf.Tensor(\n","[[-1.0000000e+09 -1.0000000e+09 -9.9999994e+08 -9.9999994e+08]\n"," [-1.0000000e+09 -9.9999994e+08 -1.0000000e+09 -1.0000000e+09]\n"," [-9.9999994e+08 -9.9999994e+08 -1.0000000e+09 -1.0000000e+09]], shape=(3, 4), dtype=float32)\n","Attention Weights tf.Tensor(\n","[[0.  0.  0.5 0.5]\n"," [0.  1.  0.  0. ]\n"," [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n","Attention weights are:\n","tf.Tensor(\n","[[0.  0.  0.5 0.5]\n"," [0.  1.  0.  0. ]\n"," [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n","Output is:\n","tf.Tensor(\n","[[550.    5.5]\n"," [ 10.    0. ]\n"," [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4v9EhwAh_oau","colab_type":"code","outputId":"b7f9afef-5b7d-4a05-d1bc-bb4cacb1ead3","executionInfo":{"status":"ok","timestamp":1585840624421,"user_tz":-540,"elapsed":1625,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":646}},"source":["temp_q = tf.constant([[0, 5, 10], \n","                      [4, 10, 7], \n","                      [10, 10, 9]], dtype=tf.float32)  # (3, 3)\n","                      \n","print_out(temp_q, temp_k, temp_v)"],"execution_count":169,"outputs":[{"output_type":"stream","text":["q tf.Tensor(\n","[[ 0.  5. 10.]\n"," [ 4. 10.  7.]\n"," [10. 10.  9.]], shape=(3, 3), dtype=float32)\n","k tf.Tensor(\n","[[10.  0.  0.]\n"," [ 0. 10.  0.]\n"," [ 0.  0. 10.]\n"," [ 0.  0. 10.]], shape=(4, 3), dtype=float32)\n","v tf.Tensor(\n","[[   1.    0.]\n"," [  10.    0.]\n"," [ 100.    5.]\n"," [1000.    6.]], shape=(4, 2), dtype=float32)\n","Matmul QK tf.Tensor(\n","[[  0.  50. 100. 100.]\n"," [ 40. 100.  70.  70.]\n"," [100. 100.  90.  90.]], shape=(3, 4), dtype=float32)\n","D_k tf.Tensor(3.0, shape=(), dtype=float32)\n","QK / sqrt(d_k) tf.Tensor(\n","[[ 0.       28.867514 57.735027 57.735027]\n"," [23.094011 57.735027 40.41452  40.41452 ]\n"," [57.735027 57.735027 51.961525 51.961525]], shape=(3, 4), dtype=float32)\n","Attention Weights tf.Tensor(\n","[[0.         0.         0.5        0.5       ]\n"," [0.         0.9999999  0.00000003 0.00000003]\n"," [0.4984504  0.4984504  0.00154961 0.00154961]], shape=(3, 4), dtype=float32)\n","Attention weights are:\n","tf.Tensor(\n","[[0.         0.         0.5        0.5       ]\n"," [0.         0.9999999  0.00000003 0.00000003]\n"," [0.4984504  0.4984504  0.00154961 0.00154961]], shape=(3, 4), dtype=float32)\n","Output is:\n","tf.Tensor(\n","[[550.           5.5       ]\n"," [ 10.000032     0.00000033]\n"," [  7.1875224    0.01704568]], shape=(3, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Do4B3LF1LIjm","colab_type":"text"},"source":["## Multi-Head Attention"]},{"cell_type":"code","metadata":{"id":"Gn0zZK9nAa76","colab_type":"code","colab":{}},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    # Multi linear -> Multi Attention -> concat -> linear\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","    \n","    self.depth = d_model // self.num_heads  # split the emb_dim by heads\n","\n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","\n","    self.dense = tf.keras.layers.Dense(d_model)\n","  \n","  def split_heads(self, x, batch_size):\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","\n","    q = self.wq(q)\n","    k = self.wk(k)\n","    v = self.wv(v)\n","\n","    q = self.split_heads(q, batch_size)\n","    k = self.split_heads(k, batch_size)\n","    v = self.split_heads(v, batch_size)\n","\n","    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask) # b, heads, seq_length, depth\n","\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # b, seq_length, heads, depth\n","\n","    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # concat to (b, seq_length, d_model)\n","\n","    output = self.dense(concat_attention)\n","    return output, attention_weights "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i2utcl3uNCd0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6b694142-02b1-4f34-bac3-c8b9b6eacec6","executionInfo":{"status":"ok","timestamp":1585840624421,"user_tz":-540,"elapsed":1605,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n","\n","y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n","\n","out, attn = temp_mha(y, k=y, q=y, mask=None)\n","\n","out.shape, attn.shape"],"execution_count":171,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"]},"metadata":{"tags":[]},"execution_count":171}]},{"cell_type":"code","metadata":{"id":"d61N_FaGNIgy","colab_type":"code","colab":{}},"source":["class MultiHeadAttentionTest(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    # Multi linear -> Multi Attention -> concat -> linear\n","    super(MultiHeadAttentionTest, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","\n","    assert d_model % self.num_heads == 0\n","    \n","    self.depth = d_model // self.num_heads  # split the emb_dim by heads\n","\n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","\n","    self.dense = tf.keras.layers.Dense(d_model)\n","  \n","  def split_heads(self, x, batch_size):\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    print('\\nReshape to (B, seq_length, heads, depth)\\n', x.shape)\n","    x = tf.transpose(x, perm=[0, 2, 1, 3])\n","    print('\\nTranspos to (B, heads, seq_lengths, depth)\\n', x.shape)\n","    return x \n","\n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","\n","    q = self.wq(q)\n","    k = self.wk(k)\n","    v = self.wv(v)\n","\n","    q = self.split_heads(q, batch_size)\n","    k = self.split_heads(k, batch_size)\n","    v = self.split_heads(v, batch_size)\n","\n","    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask) # b, heads, seq_length, depth\n","    print('Attention Alpha Shape ', scaled_attention.shape)\n","    # print(attention_weights)\n","\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # b, seq_length, heads, depth\n","\n","    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # concat to (b, seq_length, d_model)\n","\n","    output = self.dense(concat_attention)\n","    return output, attention_weights "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yJSEpny2NlVC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"187f2e45-b0a5-429a-c730-132852c278c1","executionInfo":{"status":"ok","timestamp":1585840624818,"user_tz":-540,"elapsed":1982,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["temp_mha = MultiHeadAttentionTest(d_model=512, num_heads=2)\n","\n","y = tf.random.uniform((1, 60, 4))  # (batch_size, encoder_sequence, d_model)\n","\n","out, attn = temp_mha(y, k=y, q=y, mask=None)\n","\n","print('---')\n","out.shape, attn.shape"],"execution_count":173,"outputs":[{"output_type":"stream","text":["\n","Reshape to (B, seq_length, heads, depth)\n"," (1, 60, 2, 256)\n","\n","Transpos to (B, heads, seq_lengths, depth)\n"," (1, 2, 60, 256)\n","\n","Reshape to (B, seq_length, heads, depth)\n"," (1, 60, 2, 256)\n","\n","Transpos to (B, heads, seq_lengths, depth)\n"," (1, 2, 60, 256)\n","\n","Reshape to (B, seq_length, heads, depth)\n"," (1, 60, 2, 256)\n","\n","Transpos to (B, heads, seq_lengths, depth)\n"," (1, 2, 60, 256)\n","Attention Alpha Shape  (1, 2, 60, 256)\n","---\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 60, 512]), TensorShape([1, 2, 60, 60]))"]},"metadata":{"tags":[]},"execution_count":173}]},{"cell_type":"code","metadata":{"id":"K9baYFx4OvlF","colab_type":"code","colab":{}},"source":["def point_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([tf.keras.layers.Dense(dff, activation='relu'),    # b, seq_length, dff\n","                              tf.keras.layers.Dense(d_model)])                  # b, seq_length, d_model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TI3NqntjO-kg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d3b546c3-9938-4824-a102-e427498a3fde","executionInfo":{"status":"ok","timestamp":1585840624818,"user_tz":-540,"elapsed":1955,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_ffn = point_wise_feed_forward_network(512, 2048)\n","\n","sample_ffn(tf.random.uniform((64, 50, 512))).shape"],"execution_count":175,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 50, 512])"]},"metadata":{"tags":[]},"execution_count":175}]},{"cell_type":"markdown","metadata":{"id":"39-lEoFJQGsy","colab_type":"text"},"source":["## Encoder Layer"]},{"cell_type":"code","metadata":{"id":"AcqwjFRGPCNl","colab_type":"code","colab":{}},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","    \n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, training, mask):\n","    attn_output, _ = self.mha(x, x, x, mask)\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = self.layernorm1(x + attn_output)\n","\n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    out2 = self.layernorm2(out1 + ffn_output)\n","    return out2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l8EJk585Q-ag","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6059adc0-407b-46c1-8a4b-7cb5589e9e61","executionInfo":{"status":"ok","timestamp":1585840624819,"user_tz":-540,"elapsed":1891,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_encoder_layer = EncoderLayer(512, 8, 2048)\n","\n","sample_encoder_layer_output = sample_encoder_layer(tf.random.uniform((64, 43, 512)), False, None)\n","\n","sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"],"execution_count":177,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 43, 512])"]},"metadata":{"tags":[]},"execution_count":177}]},{"cell_type":"code","metadata":{"id":"oEVGdHYhRJGm","colab_type":"code","colab":{}},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(d_model, num_heads)\n","    self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n","    attn1 = self.dropout1(attn1, training=training)\n","    out1 = self.layernorm1(attn1 + x)\n","\n","    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask) # v, k, q\n","    attn2 = self.dropout2(attn2, training=training)\n","    out2 = self.layernorm2(attn2 + out1)\n","\n","    ffn_output = self.ffn(out2)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","    out3 = self.layernorm3(ffn_output + out2)\n","\n","    return out3, attn_weights_block1, attn_weights_block2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSxXCOBASVOc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"2b462927-ef08-4c8b-fa17-f237ed73e6bf","executionInfo":{"status":"ok","timestamp":1585840821667,"user_tz":-540,"elapsed":1006,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_decoder_layer = DecoderLayer(512, 8, 2048)\n","\n","sample_decoder_layer_output, _, _ = sample_decoder_layer(\n","    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n","    False, None, None)\n","\n","sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"],"execution_count":244,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 50, 512])"]},"metadata":{"tags":[]},"execution_count":244}]},{"cell_type":"markdown","metadata":{"id":"j0vY9y1JSX9d","colab_type":"text"},"source":["# Building Encoder"]},{"cell_type":"code","metadata":{"id":"XXTabLdiSWQd","colab_type":"code","colab":{}},"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate = 0.1):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n","\n","    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","\n","  def call(self, x, training, mask):\n","    seq_len = tf.shape(x)[1]\n","\n","    x = self.embedding(x)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UkVZEhOxWHEK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8c074cfd-24c4-47bf-f754-53bc97bc07a8","executionInfo":{"status":"ok","timestamp":1585840822221,"user_tz":-540,"elapsed":1537,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n","                         dff=2048, input_vocab_size=8500,\n","                         maximum_position_encoding=10000)\n","\n","temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n","\n","sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n","\n","print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"],"execution_count":246,"outputs":[{"output_type":"stream","text":["(64, 62, 512)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t8sEvWPrTniE","colab_type":"code","colab":{}},"source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","\n","    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","\n","    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","  \n","  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","\n","    x = self.embedding(x)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training = training)\n","\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n","\n","      attention_weights['decoder_layer{}_block1'.format(i + 1)] = block1\n","      attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n","\n","    # x shape == b, seq_len, d_model\n","\n","    return x, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yDK_upEhVVMn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"236e0a24-2c2b-4b92-8625-2b76b2ec43a6","executionInfo":{"status":"ok","timestamp":1585840822442,"user_tz":-540,"elapsed":1739,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n","                         dff=2048, target_vocab_size=8000,\n","                         maximum_position_encoding=5000)\n","\n","temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n","\n","output, attn = sample_decoder(temp_input, \n","                              enc_output=sample_encoder_output, \n","                              training=False,\n","                              look_ahead_mask=None, \n","                              padding_mask=None)\n","\n","output.shape, attn['decoder_layer2_block2'].shape"],"execution_count":248,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"]},"metadata":{"tags":[]},"execution_count":248}]},{"cell_type":"markdown","metadata":{"id":"vAJmYzEVWRif","colab_type":"text"},"source":["# Build Full Model"]},{"cell_type":"code","metadata":{"id":"jjntvf7uVWRF","colab_type":"code","colab":{}},"source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n","    super(Transformer, self).__init__()\n","\n","    self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n","    self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n","\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","\n","  \n","  def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n","    enc_output = self.encoder(inp, training, enc_padding_mask)\n","    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","\n","    final_output = self.final_layer(dec_output)\n","\n","    return final_output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"la86uimCXIZi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e87609c2-107d-4dc9-81d0-c870da65c6c4","executionInfo":{"status":"ok","timestamp":1585840823247,"user_tz":-540,"elapsed":2526,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_transformer = Transformer(\n","                                num_layers=2, d_model=512, num_heads=8, dff=2048, \n","                                input_vocab_size=8500, target_vocab_size=8000, \n","                                pe_input=10000, pe_target=6000)\n","\n","temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n","temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n","\n","\n","fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n","                               enc_padding_mask=None, \n","                               look_ahead_mask=None,\n","                               dec_padding_mask=None)\n","\n","\n","fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"],"execution_count":250,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 36, 8000])"]},"metadata":{"tags":[]},"execution_count":250}]},{"cell_type":"markdown","metadata":{"id":"lecWWnAhb00l","colab_type":"text"},"source":["## Training Status"]},{"cell_type":"code","metadata":{"id":"j-K5_FTFXi7y","colab_type":"code","colab":{}},"source":["# Set Hyperparams\n","\n","num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = tokenizer_pt.vocab_size + 2\n","target_vocab_size = tokenizer_en.vocab_size + 2\n","dropout_rate = 0.1\n","\n","EPOCHS = 20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQs5XUwLXrWN","colab_type":"code","colab":{}},"source":["# Set Learning Rate Scheudler\n","\n","class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps = 4000):\n","    super(CustomSchedule, self).__init__()\n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","    self.warmup_steps = warmup_steps\n","\n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)   "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"49H8Uf16YWTS","colab_type":"code","colab":{}},"source":["learning_rate = CustomSchedule(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, \n","                                     beta_1=0.9, \n","                                     beta_2=0.98, \n","                                     epsilon=1e-9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPCgUDhqZwFH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d76aec39-c1d6-49fc-c039-262d9550b116","executionInfo":{"status":"ok","timestamp":1585840823249,"user_tz":-540,"elapsed":2494,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["learning_rate"],"execution_count":254,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<__main__.CustomSchedule at 0x7f9a2a005ac8>"]},"metadata":{"tags":[]},"execution_count":254}]},{"cell_type":"code","metadata":{"id":"0oZj1tXJYXRn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":296},"outputId":"c2e57c5f-863e-499e-b50b-13694827c676","executionInfo":{"status":"ok","timestamp":1585840823526,"user_tz":-540,"elapsed":2759,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["temp_learning_rate_schedule = CustomSchedule(d_model)\n","\n","plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n","plt.ylabel(\"Learning Rate\")\n","plt.xlabel(\"Train Step\")"],"execution_count":255,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'Train Step')"]},"metadata":{"tags":[]},"execution_count":255},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZUAAAEGCAYAAACtqQjWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"qbPf_9mBYbhy","colab_type":"code","colab":{}},"source":["loss_obejct = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8EQrVi4XYhEH","colab_type":"code","colab":{}},"source":["def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_obejct(real, pred)\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_  *= mask\n","  \n","  return tf.reduce_sum(loss_) / tf.reduce_sum(mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YeEn2u0eY0Wg","colab_type":"code","colab":{}},"source":["train_loss = tf.keras.metrics.Mean(name='Train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='Train_Accuracy')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"doRtXFpkZAPp","colab_type":"code","colab":{}},"source":["transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, input_vocab_size, target_vocab_size, dropout_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fe7RSCGVZMJS","colab_type":"code","colab":{}},"source":["def create_mask(inp, tar):\n","  enc_padding_mask = create_padding_mask(inp)\n","\n","  dec_padding_mask = create_padding_mask(tar)\n","\n","  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","  dec_target_padding_mask = create_padding_mask(tar)\n","  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","  return enc_padding_mask, combined_mask, dec_padding_mask\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"j7qRvmSqZmHw","colab_type":"code","colab":{}},"source":["check_point = './checkpoins/train'\n","\n","ckpt = tf.train.Checkpoint(transfomer = transfomer, optimizer = optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, check_point, max_to_keep=5)\n","\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.resotre(ckpt_manager.latest_checkpoint)\n","  print('Latest Checkpoint Restored!')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mR-HJyVkaDrQ","colab_type":"code","colab":{}},"source":["train_step_signater = [tf.TensorSpec(shape = (None, None), dtype=tf.int64),\n","                       tf.TensorSpec(shape = (None, None), dtype=tf.int64)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ck_xOff8aRTF","colab_type":"code","colab":{}},"source":["@tf.function\n","def train_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]\n","\n","  enc_padding_mask, combined_mask, dec_padding_mask = create_mask(inp, tar_inp)\n","\n","  with tf.GradientTape() as tape:\n","    predictions, _  = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n","\n","    loss = loss_function(tar_real, predictions)\n","\n","  gradients = tape.gradients(loss, transformer.trainable_variables)\n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","\n","  train_loss(loss)\n","  train_accuracy(tar_real, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"biUIx3sibJxz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":921},"outputId":"7e45fbe8-fa59-4639-bfca-8cb33a632730","executionInfo":{"status":"error","timestamp":1585840842749,"user_tz":-540,"elapsed":21912,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["for epoch in range(EPOCHS):\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","\n","  for (batch, (inp, tar)) in enumerate(train_dataset):\n","    train_step(inp, tar)\n","\n","    if batch % 50 == 0:\n","      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n","\n","    if (epoch+1) % 5 == 0:\n","      ckpt_save_path = ckpt_manager.save()\n","      print('Saving ckpt for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n","\n","  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, train_loss.result(), train_accuracy.result()))  "],"execution_count":264,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-264-4e9e502d88fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    <ipython-input-241-486b6e1bc244>:9 train_step  *\n        predictions, _  = transformer(inp, tar_inp, True, enc_padding_mask, combined_mask, dec_padding_mask)\n    <ipython-input-89-07b59760a26e>:13 call  *\n        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n    <ipython-input-79-87e11f14ffd1>:25 call  *\n        x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n    <ipython-input-243-4da1df58ee9f>:27 call  *\n        attn2, attn_weights_block2 = self.mha2(\n    <ipython-input-36-607ce145a73b>:33 call  *\n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask) # b, heads, seq_length, depth\n    <ipython-input-158-d4bc96e7f9bb>:27 scaled_dot_product_attention  *\n        scaled_attention_logits += (mask * -1e9)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:984 binary_op_wrapper\n        return func(x, y, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:1276 _add_dispatch\n        return gen_math_ops.add_v2(x, y, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py:483 add_v2\n        \"AddV2\", x=x, y=y, name=name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:744 _apply_op_helper\n        attrs=attr_protos, op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py:595 _create_op_internal\n        compute_device)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:3327 _create_op_internal\n        op_def=op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1817 __init__\n        control_input_ops, op_def)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py:1657 _create_c_op\n        raise ValueError(str(e))\n\n    ValueError: Dimensions must be equal, but are 40 and 39 for '{{node transformer_17/decoder_29/decoder_layer_76/multi_head_attention_226/add}} = AddV2[T=DT_FLOAT](transformer_17/decoder_29/decoder_layer_76/multi_head_attention_226/truediv, transformer_17/decoder_29/decoder_layer_76/multi_head_attention_226/mul)' with input shapes: [64,8,39,40], [64,1,1,39].\n"]}]},{"cell_type":"code","metadata":{"id":"QKLwm_Wdbnzf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hvpesqdWfend","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CO_q-e49fxqk","colab_type":"text"},"source":["# Checking"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LazzUq3bJ5SH","colab":{}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","  \"\"\"Calculate the attention weights.\n","  q, k, v must have matching leading dimensions.\n","  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","  The mask has different shapes depending on its type(padding or look ahead) \n","  but it must be broadcastable for addition.\n","  \n","  Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","    \n","  Returns:\n","    output, attention_weights\n","  \"\"\"\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","  \n","  # scale matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","  return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FiqETnhCkoXh"},"source":["As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n","\n","The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"n90YjClyInFy","colab":{}},"source":["def print_out(q, k, v):\n","  temp_out, temp_attn = scaled_dot_product_attention(\n","      q, k, v, None)\n","  print ('Attention weights are:')\n","  print (temp_attn)\n","  print ('Output is:')\n","  print (temp_out)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yAzUAf2DPlNt","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"fccf22b1-4328-44d2-e030-0cb7d4806448","executionInfo":{"status":"ok","timestamp":1585841046188,"user_tz":-540,"elapsed":981,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["np.set_printoptions(suppress=True)\n","\n","temp_k = tf.constant([[10,0,0],\n","                      [0,10,0],\n","                      [0,0,10],\n","                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n","\n","temp_v = tf.constant([[   1,0],\n","                      [  10,0],\n","                      [ 100,5],\n","                      [1000,6]], dtype=tf.float32)  # (4, 2)\n","\n","# This `query` aligns with the second `key`,\n","# so the second `value` is returned.\n","temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":267,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zg6k-fGhgXra","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"f324e15f-de9f-49f0-e562-61237d0b1f89","executionInfo":{"status":"ok","timestamp":1585841046188,"user_tz":-540,"elapsed":979,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["# This query aligns with a repeated key (third and fourth), \n","# so all associated values get averaged.\n","temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":268,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UAq3YOzUgXhb","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"338fbe54-1e9f-4121-bfcb-01b0c0e814ee","executionInfo":{"status":"ok","timestamp":1585841046188,"user_tz":-540,"elapsed":977,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["# This query aligns equally with the first and second key, \n","# so their values get averaged.\n","temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":269,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n","Output is:\n","tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aOz-4_XIhaTP"},"source":["Pass all the queries together."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6dlU8Tm-hYrF","colab":{"base_uri":"https://localhost:8080/","height":187},"outputId":"cd60410a-a220-481f-c88a-bfb5108eab9f","executionInfo":{"status":"ok","timestamp":1585841046189,"user_tz":-540,"elapsed":976,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n","print_out(temp_q, temp_k, temp_v)"],"execution_count":270,"outputs":[{"output_type":"stream","text":["Attention weights are:\n","tf.Tensor(\n","[[0.  0.  0.5 0.5]\n"," [0.  1.  0.  0. ]\n"," [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n","Output is:\n","tf.Tensor(\n","[[550.    5.5]\n"," [ 10.    0. ]\n"," [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kmzGPEy64qmA"},"source":["## Multi-head attention"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fz5BMC8Kaoqo"},"source":["<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n","\n","\n","Multi-head attention consists of four parts:\n","*    Linear layers and split into heads.\n","*    Scaled dot-product attention.\n","*    Concatenation of heads.\n","*    Final linear layer."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JPmbr6F1C-v_"},"source":["Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n","\n","The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n","\n","Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"BSV3PPKsYecw","colab":{}},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","    \n","    assert d_model % self.num_heads == 0\n","    \n","    self.depth = d_model // self.num_heads\n","    \n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","    \n","    self.dense = tf.keras.layers.Dense(d_model)\n","        \n","  def split_heads(self, x, batch_size):\n","    \"\"\"Split the last dimension into (num_heads, depth).\n","    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","    \"\"\"\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","    \n","    q = self.wq(q)  # (batch_size, seq_len, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len, d_model)\n","    \n","    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","    \n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    scaled_attention, attention_weights = scaled_dot_product_attention(\n","        q, k, v, mask)\n","    \n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","    concat_attention = tf.reshape(scaled_attention, \n","                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","        \n","    return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0D8FJue5lDyZ"},"source":["Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Hu94p-_-2_BX","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a44d5b56-3f3b-42c2-f079-14ea51b1afe1","executionInfo":{"status":"ok","timestamp":1585841046534,"user_tz":-540,"elapsed":1317,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n","y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n","out, attn = temp_mha(y, k=y, q=y, mask=None)\n","out.shape, attn.shape"],"execution_count":272,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"]},"metadata":{"tags":[]},"execution_count":272}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RdDqGayx67vv"},"source":["## Point wise feed forward network"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gBqzJXGfHK3X"},"source":["Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ET7xLt0yCT6Z","colab":{}},"source":["def point_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mytb1lPyOHLB","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9a2ae97e-dbec-43ba-dcba-d1f1615af5ff","executionInfo":{"status":"ok","timestamp":1585841046534,"user_tz":-540,"elapsed":1313,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_ffn = point_wise_feed_forward_network(512, 2048)\n","sample_ffn(tf.random.uniform((64, 50, 512))).shape"],"execution_count":274,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 50, 512])"]},"metadata":{"tags":[]},"execution_count":274}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7e7hKcxn6-zd"},"source":["## Encoder and decoder"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"yScbC0MUH8dS"},"source":["<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MfYJG-Kvgwy2"},"source":["The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n","\n","* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n","* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QFv-FNYUmvpn"},"source":["### Encoder layer\n","\n","Each encoder layer consists of sublayers:\n","\n","1.   Multi-head attention (with padding mask) \n","2.    Point wise feed forward networks. \n","\n","Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n","\n","The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ncyS-Ms3i2x_","colab":{}},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","  def call(self, x, training, mask):\n","\n","    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","    attn_output = self.dropout1(attn_output, training=training)\n","    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","    \n","    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","    ffn_output = self.dropout2(ffn_output, training=training)\n","    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","    \n","    return out2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AzZRXdO0mI48","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"d5ca0a25-2ec3-46ef-e65b-df5771f5cf9e","executionInfo":{"status":"ok","timestamp":1585841046535,"user_tz":-540,"elapsed":1309,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_encoder_layer = EncoderLayer(512, 8, 2048)\n","\n","sample_encoder_layer_output = sample_encoder_layer(\n","    tf.random.uniform((64, 43, 512)), False, None)\n","\n","sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"],"execution_count":276,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 43, 512])"]},"metadata":{"tags":[]},"execution_count":276}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"6LO_48Owmx_o"},"source":["### Decoder layer\n","\n","Each decoder layer consists of sublayers:\n","\n","1.   Masked multi-head attention (with look ahead mask and padding mask)\n","2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n","3.   Point wise feed forward networks\n","\n","Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n","\n","There are N decoder layers in the transformer.\n","\n","As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9SoX0-vd1hue","colab":{}},"source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(d_model, num_heads)\n","    self.mha2 = MultiHeadAttention(d_model, num_heads)\n","\n","    self.ffn = point_wise_feed_forward_network(d_model, dff)\n"," \n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    self.dropout3 = tf.keras.layers.Dropout(rate)\n","    \n","    \n","  def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","    # enc_output.shape == (batch_size, input_seq_len, d_model)\n","\n","    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n","    attn1 = self.dropout1(attn1, training=training)\n","    out1 = self.layernorm1(attn1 + x)\n","    \n","    attn2, attn_weights_block2 = self.mha2(\n","        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n","    attn2 = self.dropout2(attn2, training=training)\n","    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n","    \n","    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n","    ffn_output = self.dropout3(ffn_output, training=training)\n","    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n","    \n","    return out3, attn_weights_block1, attn_weights_block2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ne2Bqx8k71l0","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"746d3b06-f5cd-45c8-a724-7f804e9fd1d8","executionInfo":{"status":"ok","timestamp":1585841046809,"user_tz":-540,"elapsed":1579,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_decoder_layer = DecoderLayer(512, 8, 2048)\n","\n","sample_decoder_layer_output, _, _ = sample_decoder_layer(\n","    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n","    False, None, None)\n","\n","sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"],"execution_count":278,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 50, 512])"]},"metadata":{"tags":[]},"execution_count":278}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SE1H51Ajm0q1"},"source":["### Encoder\n","\n","The `Encoder` consists of:\n","1.   Input Embedding\n","2.   Positional Encoding\n","3.   N encoder layers\n","\n","The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jpEox7gJ8FCI","colab":{}},"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    \n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, \n","                                            self.d_model)\n","    \n","    \n","    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n","                       for _ in range(num_layers)]\n","  \n","    self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","  def call(self, x, training, mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    \n","    # adding embedding and position encoding.\n","    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x, training=training)\n","    \n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, training, mask)\n","    \n","    return x  # (batch_size, input_seq_len, d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8QG9nueFQKXx","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a8dc7150-629c-4471-88f6-92fe56cf535c","executionInfo":{"status":"ok","timestamp":1585841047142,"user_tz":-540,"elapsed":1908,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n","                         dff=2048, input_vocab_size=8500,\n","                         maximum_position_encoding=10000)\n","temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n","\n","sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n","\n","print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"],"execution_count":280,"outputs":[{"output_type":"stream","text":["(64, 62, 512)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p-uO6ls8m2O5"},"source":["### Decoder"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZtT7PKzrXkNr"},"source":[" The `Decoder` consists of:\n","1.   Output Embedding\n","2.   Positional Encoding\n","3.   N decoder layers\n","\n","The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"d5_d5-PLQXwY","colab":{}},"source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    \n","    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n","    \n","    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n","                       for _ in range(num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(rate)\n","    \n","  def call(self, x, enc_output, training, \n","           look_ahead_mask, padding_mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    attention_weights = {}\n","    \n","    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","    \n","    x = self.dropout(x, training=training)\n","\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n","                                             look_ahead_mask, padding_mask)\n","      \n","      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","    \n","    # x.shape == (batch_size, target_seq_len, d_model)\n","    return x, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"a1jXoAMRZyvu","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0b119531-708a-4d55-d1ea-9d334922afff","executionInfo":{"status":"ok","timestamp":1585841047531,"user_tz":-540,"elapsed":2294,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n","                         dff=2048, target_vocab_size=8000,\n","                         maximum_position_encoding=5000)\n","temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n","\n","output, attn = sample_decoder(temp_input, \n","                              enc_output=sample_encoder_output, \n","                              training=False,\n","                              look_ahead_mask=None, \n","                              padding_mask=None)\n","\n","output.shape, attn['decoder_layer2_block2'].shape"],"execution_count":282,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"]},"metadata":{"tags":[]},"execution_count":282}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y54xnJnuYgJ7"},"source":["## Create the Transformer"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uERO1y54cOKq"},"source":["Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PED3bIpOYkBu","colab":{}},"source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n","               target_vocab_size, pe_input, pe_target, rate=0.1):\n","    super(Transformer, self).__init__()\n","\n","    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n","                           input_vocab_size, pe_input, rate)\n","\n","    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n","                           target_vocab_size, pe_target, rate)\n","\n","    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n","    \n","  def call(self, inp, tar, training, enc_padding_mask, \n","           look_ahead_mask, dec_padding_mask):\n","\n","    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","    \n","    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n","    dec_output, attention_weights = self.decoder(\n","        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n","    \n","    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n","    \n","    return final_output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tJ4fbQcIkHW1","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"24278f36-7a69-489b-b96c-dbda7d8cfcf1","executionInfo":{"status":"ok","timestamp":1585841048380,"user_tz":-540,"elapsed":3140,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["sample_transformer = Transformer(\n","    num_layers=2, d_model=512, num_heads=8, dff=2048, \n","    input_vocab_size=8500, target_vocab_size=8000, \n","    pe_input=10000, pe_target=6000)\n","\n","temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n","temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n","\n","fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n","                               enc_padding_mask=None, \n","                               look_ahead_mask=None,\n","                               dec_padding_mask=None)\n","\n","fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"],"execution_count":284,"outputs":[{"output_type":"execute_result","data":{"text/plain":["TensorShape([64, 36, 8000])"]},"metadata":{"tags":[]},"execution_count":284}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wsINyf1VEQLC"},"source":["## Set hyperparameters"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zVjWCxFNcgbt"},"source":["To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n","\n","The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n","\n","Note: By changing the values below, you can get the model that achieved state of the art on many tasks."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lnJn5SLA2ahP","colab":{}},"source":["num_layers = 4\n","d_model = 128\n","dff = 512\n","num_heads = 8\n","\n","input_vocab_size = tokenizer_pt.vocab_size + 2\n","target_vocab_size = tokenizer_en.vocab_size + 2\n","dropout_rate = 0.1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xYEGhEOtzn5W"},"source":["## Optimizer"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GOmWW--yP3zx"},"source":["Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n","\n","$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iYQdOO1axwEI","colab":{}},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    \n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","    \n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","    \n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7r4scdulztRx","colab":{}},"source":["learning_rate = CustomSchedule(d_model)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"f33ZCgvHpPdG","colab":{"base_uri":"https://localhost:8080/","height":296},"outputId":"e295fe1e-08f2-402b-d856-0555a015f247","executionInfo":{"status":"ok","timestamp":1585841048382,"user_tz":-540,"elapsed":3133,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["temp_learning_rate_schedule = CustomSchedule(d_model)\n","\n","plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n","plt.ylabel(\"Learning Rate\")\n","plt.xlabel(\"Train Step\")"],"execution_count":288,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 0, 'Train Step')"]},"metadata":{"tags":[]},"execution_count":288},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAZUAAAEGCAYAAACtqQjWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"YgkDE7hzo8r5"},"source":["## Loss and metrics"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oxGJtoDuYIHL"},"source":["Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MlhsJMm0TW_B","colab":{}},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"67oqVHiT0Eiu","colab":{}},"source":["def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","  \n","  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"phlyxMnm-Tpx","colab":{}},"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n","    name='train_accuracy')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"aeHumfr7zmMa"},"source":["## Training and checkpointing"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UiysUa--4tOU","colab":{}},"source":["transformer = Transformer(num_layers, d_model, num_heads, dff,\n","                          input_vocab_size, target_vocab_size, \n","                          pe_input=input_vocab_size, \n","                          pe_target=target_vocab_size,\n","                          rate=dropout_rate)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZOJUSB1T8GjM","colab":{}},"source":["def create_masks(inp, tar):\n","  # Encoder padding mask\n","  enc_padding_mask = create_padding_mask(inp)\n","  \n","  # Used in the 2nd attention block in the decoder.\n","  # This padding mask is used to mask the encoder outputs.\n","  dec_padding_mask = create_padding_mask(inp)\n","  \n","  # Used in the 1st attention block in the decoder.\n","  # It is used to pad and mask future tokens in the input received by \n","  # the decoder.\n","  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","  dec_target_padding_mask = create_padding_mask(tar)\n","  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","  \n","  return enc_padding_mask, combined_mask, dec_padding_mask"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Fzuf06YZp66w"},"source":["Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"hNhuYfllndLZ","colab":{}},"source":["checkpoint_path = \"./checkpoints/train\"\n","\n","ckpt = tf.train.Checkpoint(transformer=transformer,\n","                           optimizer=optimizer)\n","\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n","\n","# if a checkpoint exists, restore the latest checkpoint.\n","if ckpt_manager.latest_checkpoint:\n","  ckpt.restore(ckpt_manager.latest_checkpoint)\n","  print ('Latest checkpoint restored!!')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0Di_Yaa1gf9r"},"source":["The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n","\n","For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n","\n","`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n","\n","`tar_real` = \"A lion in the jungle is sleeping EOS\"\n","\n","The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n","\n","During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n","\n","As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n","\n","To prevent the model from peaking at the expected output the model uses a look-ahead mask."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LKpoA6q1sJFj","colab":{}},"source":["EPOCHS = 20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iJwmp9OE29oj","colab":{}},"source":["# The @tf.function trace-compiles train_step into a TF graph for faster\n","# execution. The function specializes to the precise shape of the argument\n","# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n","# batch sizes (the last batch is smaller), use input_signature to specify\n","# more generic shapes.\n","\n","train_step_signature = [\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n","]\n","\n","@tf.function(input_signature=train_step_signature)\n","def train_step(inp, tar):\n","  tar_inp = tar[:, :-1]\n","  tar_real = tar[:, 1:]\n","  \n","  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n","  \n","  with tf.GradientTape() as tape:\n","    predictions, _ = transformer(inp, tar_inp, \n","                                 True, \n","                                 enc_padding_mask, \n","                                 combined_mask, \n","                                 dec_padding_mask)\n","    loss = loss_function(tar_real, predictions)\n","\n","  gradients = tape.gradient(loss, transformer.trainable_variables)    \n","  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n","  \n","  train_loss(loss)\n","  train_accuracy(tar_real, predictions)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qM2PDWGDJ_8V"},"source":["Portuguese is used as the input language and English is the target language."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bbvmaKNiznHZ","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"c8112888-1083-4b33-e529-d20228dd3bdd","executionInfo":{"status":"ok","timestamp":1585843032283,"user_tz":-540,"elapsed":154661,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["for epoch in range(EPOCHS):\n","  start = time.time()\n","  \n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  \n","  # inp -> portuguese, tar -> english\n","  for (batch, (inp, tar)) in enumerate(train_dataset):\n","    train_step(inp, tar)\n","    \n","    if batch % 50 == 0:\n","      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n","          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n","      \n","  if (epoch + 1) % 5 == 0:\n","    ckpt_save_path = ckpt_manager.save()\n","    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n","                                                         ckpt_save_path))\n","    \n","  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n","                                                train_loss.result(), \n","                                                train_accuracy.result()))\n","\n","  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"],"execution_count":297,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 8.9619 Accuracy 0.0000\n","Epoch 1 Batch 50 Loss 8.9061 Accuracy 0.0049\n","Epoch 1 Batch 100 Loss 8.8226 Accuracy 0.0157\n","Epoch 1 Batch 150 Loss 8.7227 Accuracy 0.0194\n","Epoch 1 Batch 200 Loss 8.5961 Accuracy 0.0213\n","Epoch 1 Batch 250 Loss 8.4425 Accuracy 0.0225\n","Epoch 1 Batch 300 Loss 8.2656 Accuracy 0.0235\n","Epoch 1 Batch 350 Loss 8.0787 Accuracy 0.0272\n","Epoch 1 Batch 400 Loss 7.9007 Accuracy 0.0312\n","Epoch 1 Batch 450 Loss 7.7387 Accuracy 0.0345\n","Epoch 1 Batch 500 Loss 7.5960 Accuracy 0.0375\n","Epoch 1 Batch 550 Loss 7.4621 Accuracy 0.0411\n","Epoch 1 Batch 600 Loss 7.3359 Accuracy 0.0449\n","Epoch 1 Batch 650 Loss 7.2169 Accuracy 0.0485\n","Epoch 1 Batch 700 Loss 7.1018 Accuracy 0.0520\n","Epoch 1 Loss 7.0974 Accuracy 0.0521\n","Time taken for 1 epoch: 150.8231599330902 secs\n","\n","Epoch 2 Batch 0 Loss 5.6398 Accuracy 0.1018\n","Epoch 2 Batch 50 Loss 5.4665 Accuracy 0.1032\n","Epoch 2 Batch 100 Loss 5.4098 Accuracy 0.1058\n","Epoch 2 Batch 150 Loss 5.3570 Accuracy 0.1080\n","Epoch 2 Batch 200 Loss 5.3064 Accuracy 0.1103\n","Epoch 2 Batch 250 Loss 5.2673 Accuracy 0.1121\n","Epoch 2 Batch 300 Loss 5.2256 Accuracy 0.1140\n","Epoch 2 Batch 350 Loss 5.1872 Accuracy 0.1155\n","Epoch 2 Batch 400 Loss 5.1546 Accuracy 0.1171\n","Epoch 2 Batch 450 Loss 5.1265 Accuracy 0.1186\n","Epoch 2 Batch 500 Loss 5.0950 Accuracy 0.1202\n","Epoch 2 Batch 550 Loss 5.0695 Accuracy 0.1214\n","Epoch 2 Batch 600 Loss 5.0450 Accuracy 0.1227\n","Epoch 2 Batch 650 Loss 5.0229 Accuracy 0.1239\n","Epoch 2 Batch 700 Loss 4.9967 Accuracy 0.1250\n","Epoch 2 Loss 4.9957 Accuracy 0.1251\n","Time taken for 1 epoch: 97.70468378067017 secs\n","\n","Epoch 3 Batch 0 Loss 4.6193 Accuracy 0.1499\n","Epoch 3 Batch 50 Loss 4.5977 Accuracy 0.1457\n","Epoch 3 Batch 100 Loss 4.5777 Accuracy 0.1453\n","Epoch 3 Batch 150 Loss 4.5748 Accuracy 0.1456\n","Epoch 3 Batch 200 Loss 4.5674 Accuracy 0.1462\n","Epoch 3 Batch 250 Loss 4.5578 Accuracy 0.1465\n","Epoch 3 Batch 300 Loss 4.5453 Accuracy 0.1471\n","Epoch 3 Batch 350 Loss 4.5330 Accuracy 0.1472\n","Epoch 3 Batch 400 Loss 4.5193 Accuracy 0.1482\n","Epoch 3 Batch 450 Loss 4.5029 Accuracy 0.1489\n","Epoch 3 Batch 500 Loss 4.4853 Accuracy 0.1500\n","Epoch 3 Batch 550 Loss 4.4702 Accuracy 0.1507\n","Epoch 3 Batch 600 Loss 4.4564 Accuracy 0.1516\n","Epoch 3 Batch 650 Loss 4.4427 Accuracy 0.1525\n","Epoch 3 Batch 700 Loss 4.4262 Accuracy 0.1535\n","Epoch 3 Loss 4.4255 Accuracy 0.1535\n","Time taken for 1 epoch: 96.19743824005127 secs\n","\n","Epoch 4 Batch 0 Loss 3.8531 Accuracy 0.1639\n","Epoch 4 Batch 50 Loss 4.0941 Accuracy 0.1668\n","Epoch 4 Batch 100 Loss 4.0972 Accuracy 0.1683\n","Epoch 4 Batch 150 Loss 4.0870 Accuracy 0.1706\n","Epoch 4 Batch 200 Loss 4.0692 Accuracy 0.1715\n","Epoch 4 Batch 250 Loss 4.0557 Accuracy 0.1724\n","Epoch 4 Batch 300 Loss 4.0407 Accuracy 0.1741\n","Epoch 4 Batch 350 Loss 4.0244 Accuracy 0.1755\n","Epoch 4 Batch 400 Loss 4.0104 Accuracy 0.1765\n","Epoch 4 Batch 450 Loss 3.9912 Accuracy 0.1778\n","Epoch 4 Batch 500 Loss 3.9775 Accuracy 0.1785\n","Epoch 4 Batch 550 Loss 3.9648 Accuracy 0.1793\n","Epoch 4 Batch 600 Loss 3.9491 Accuracy 0.1801\n","Epoch 4 Batch 650 Loss 3.9331 Accuracy 0.1810\n","Epoch 4 Batch 700 Loss 3.9185 Accuracy 0.1819\n","Epoch 4 Loss 3.9178 Accuracy 0.1820\n","Time taken for 1 epoch: 95.44626140594482 secs\n","\n","Epoch 5 Batch 0 Loss 3.5451 Accuracy 0.1669\n","Epoch 5 Batch 50 Loss 3.5580 Accuracy 0.2002\n","Epoch 5 Batch 100 Loss 3.5594 Accuracy 0.2013\n","Epoch 5 Batch 150 Loss 3.5581 Accuracy 0.2011\n","Epoch 5 Batch 200 Loss 3.5500 Accuracy 0.2016\n","Epoch 5 Batch 250 Loss 3.5444 Accuracy 0.2021\n","Epoch 5 Batch 300 Loss 3.5333 Accuracy 0.2029\n","Epoch 5 Batch 350 Loss 3.5199 Accuracy 0.2038\n","Epoch 5 Batch 400 Loss 3.5169 Accuracy 0.2044\n","Epoch 5 Batch 450 Loss 3.5061 Accuracy 0.2051\n","Epoch 5 Batch 500 Loss 3.4975 Accuracy 0.2056\n","Epoch 5 Batch 550 Loss 3.4903 Accuracy 0.2060\n","Epoch 5 Batch 600 Loss 3.4830 Accuracy 0.2063\n","Epoch 5 Batch 650 Loss 3.4723 Accuracy 0.2068\n","Epoch 5 Batch 700 Loss 3.4650 Accuracy 0.2075\n","Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n","Epoch 5 Loss 3.4643 Accuracy 0.2075\n","Time taken for 1 epoch: 96.68395185470581 secs\n","\n","Epoch 6 Batch 0 Loss 3.0381 Accuracy 0.2416\n","Epoch 6 Batch 50 Loss 3.1460 Accuracy 0.2230\n","Epoch 6 Batch 100 Loss 3.1418 Accuracy 0.2235\n","Epoch 6 Batch 150 Loss 3.1462 Accuracy 0.2227\n","Epoch 6 Batch 200 Loss 3.1433 Accuracy 0.2231\n","Epoch 6 Batch 250 Loss 3.1388 Accuracy 0.2236\n","Epoch 6 Batch 300 Loss 3.1297 Accuracy 0.2243\n","Epoch 6 Batch 350 Loss 3.1232 Accuracy 0.2244\n","Epoch 6 Batch 400 Loss 3.1173 Accuracy 0.2249\n","Epoch 6 Batch 450 Loss 3.1110 Accuracy 0.2251\n","Epoch 6 Batch 500 Loss 3.1059 Accuracy 0.2254\n","Epoch 6 Batch 550 Loss 3.1016 Accuracy 0.2256\n","Epoch 6 Batch 600 Loss 3.0956 Accuracy 0.2259\n","Epoch 6 Batch 650 Loss 3.0918 Accuracy 0.2262\n","Epoch 6 Batch 700 Loss 3.0855 Accuracy 0.2267\n","Epoch 6 Loss 3.0857 Accuracy 0.2268\n","Time taken for 1 epoch: 96.91434288024902 secs\n","\n","Epoch 7 Batch 0 Loss 2.8305 Accuracy 0.2336\n","Epoch 7 Batch 50 Loss 2.7423 Accuracy 0.2421\n","Epoch 7 Batch 100 Loss 2.7528 Accuracy 0.2424\n","Epoch 7 Batch 150 Loss 2.7484 Accuracy 0.2429\n","Epoch 7 Batch 200 Loss 2.7460 Accuracy 0.2435\n","Epoch 7 Batch 250 Loss 2.7464 Accuracy 0.2433\n","Epoch 7 Batch 300 Loss 2.7396 Accuracy 0.2436\n","Epoch 7 Batch 350 Loss 2.7352 Accuracy 0.2443\n","Epoch 7 Batch 400 Loss 2.7282 Accuracy 0.2455\n","Epoch 7 Batch 450 Loss 2.7245 Accuracy 0.2463\n","Epoch 7 Batch 500 Loss 2.7215 Accuracy 0.2464\n","Epoch 7 Batch 550 Loss 2.7155 Accuracy 0.2467\n","Epoch 7 Batch 600 Loss 2.7086 Accuracy 0.2472\n","Epoch 7 Batch 650 Loss 2.7053 Accuracy 0.2473\n","Epoch 7 Batch 700 Loss 2.7026 Accuracy 0.2475\n","Epoch 7 Loss 2.7024 Accuracy 0.2475\n","Time taken for 1 epoch: 96.66961574554443 secs\n","\n","Epoch 8 Batch 0 Loss 2.3620 Accuracy 0.2673\n","Epoch 8 Batch 50 Loss 2.3654 Accuracy 0.2674\n","Epoch 8 Batch 100 Loss 2.3738 Accuracy 0.2650\n","Epoch 8 Batch 150 Loss 2.3871 Accuracy 0.2652\n","Epoch 8 Batch 200 Loss 2.3889 Accuracy 0.2648\n","Epoch 8 Batch 250 Loss 2.3960 Accuracy 0.2642\n","Epoch 8 Batch 300 Loss 2.3964 Accuracy 0.2642\n","Epoch 8 Batch 350 Loss 2.3989 Accuracy 0.2639\n","Epoch 8 Batch 400 Loss 2.3933 Accuracy 0.2644\n","Epoch 8 Batch 450 Loss 2.3916 Accuracy 0.2643\n","Epoch 8 Batch 500 Loss 2.3912 Accuracy 0.2646\n","Epoch 8 Batch 550 Loss 2.3888 Accuracy 0.2647\n","Epoch 8 Batch 600 Loss 2.3870 Accuracy 0.2648\n","Epoch 8 Batch 650 Loss 2.3846 Accuracy 0.2651\n","Epoch 8 Batch 700 Loss 2.3827 Accuracy 0.2657\n","Epoch 8 Loss 2.3823 Accuracy 0.2656\n","Time taken for 1 epoch: 96.03750157356262 secs\n","\n","Epoch 9 Batch 0 Loss 2.0451 Accuracy 0.3417\n","Epoch 9 Batch 50 Loss 2.1048 Accuracy 0.2843\n","Epoch 9 Batch 100 Loss 2.1186 Accuracy 0.2798\n","Epoch 9 Batch 150 Loss 2.1224 Accuracy 0.2803\n","Epoch 9 Batch 200 Loss 2.1278 Accuracy 0.2781\n","Epoch 9 Batch 250 Loss 2.1376 Accuracy 0.2777\n","Epoch 9 Batch 300 Loss 2.1375 Accuracy 0.2786\n","Epoch 9 Batch 350 Loss 2.1421 Accuracy 0.2796\n","Epoch 9 Batch 400 Loss 2.1431 Accuracy 0.2791\n","Epoch 9 Batch 450 Loss 2.1417 Accuracy 0.2792\n","Epoch 9 Batch 500 Loss 2.1445 Accuracy 0.2793\n","Epoch 9 Batch 550 Loss 2.1485 Accuracy 0.2790\n","Epoch 9 Batch 600 Loss 2.1483 Accuracy 0.2790\n","Epoch 9 Batch 650 Loss 2.1485 Accuracy 0.2790\n","Epoch 9 Batch 700 Loss 2.1506 Accuracy 0.2793\n","Epoch 9 Loss 2.1510 Accuracy 0.2793\n","Time taken for 1 epoch: 96.62414240837097 secs\n","\n","Epoch 10 Batch 0 Loss 2.0173 Accuracy 0.2812\n","Epoch 10 Batch 50 Loss 1.9287 Accuracy 0.2892\n","Epoch 10 Batch 100 Loss 1.9258 Accuracy 0.2922\n","Epoch 10 Batch 150 Loss 1.9376 Accuracy 0.2911\n","Epoch 10 Batch 200 Loss 1.9415 Accuracy 0.2916\n","Epoch 10 Batch 250 Loss 1.9493 Accuracy 0.2911\n","Epoch 10 Batch 300 Loss 1.9539 Accuracy 0.2910\n","Epoch 10 Batch 350 Loss 1.9593 Accuracy 0.2906\n","Epoch 10 Batch 400 Loss 1.9586 Accuracy 0.2902\n","Epoch 10 Batch 450 Loss 1.9599 Accuracy 0.2904\n","Epoch 10 Batch 500 Loss 1.9633 Accuracy 0.2904\n","Epoch 10 Batch 550 Loss 1.9645 Accuracy 0.2903\n","Epoch 10 Batch 600 Loss 1.9686 Accuracy 0.2898\n","Epoch 10 Batch 650 Loss 1.9713 Accuracy 0.2897\n","Epoch 10 Batch 700 Loss 1.9726 Accuracy 0.2898\n","Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n","Epoch 10 Loss 1.9727 Accuracy 0.2898\n","Time taken for 1 epoch: 96.67814898490906 secs\n","\n","Epoch 11 Batch 0 Loss 1.8200 Accuracy 0.2901\n","Epoch 11 Batch 50 Loss 1.7600 Accuracy 0.3064\n","Epoch 11 Batch 100 Loss 1.7716 Accuracy 0.3039\n","Epoch 11 Batch 150 Loss 1.7723 Accuracy 0.3034\n","Epoch 11 Batch 200 Loss 1.7780 Accuracy 0.3025\n","Epoch 11 Batch 250 Loss 1.7888 Accuracy 0.3012\n","Epoch 11 Batch 300 Loss 1.7931 Accuracy 0.3016\n","Epoch 11 Batch 350 Loss 1.7980 Accuracy 0.3016\n","Epoch 11 Batch 400 Loss 1.8010 Accuracy 0.3010\n","Epoch 11 Batch 450 Loss 1.8033 Accuracy 0.3006\n","Epoch 11 Batch 500 Loss 1.8084 Accuracy 0.3004\n","Epoch 11 Batch 550 Loss 1.8146 Accuracy 0.2999\n","Epoch 11 Batch 600 Loss 1.8176 Accuracy 0.2996\n","Epoch 11 Batch 650 Loss 1.8240 Accuracy 0.2994\n","Epoch 11 Batch 700 Loss 1.8283 Accuracy 0.2991\n","Epoch 11 Loss 1.8286 Accuracy 0.2991\n","Time taken for 1 epoch: 96.68161964416504 secs\n","\n","Epoch 12 Batch 0 Loss 1.8386 Accuracy 0.2932\n","Epoch 12 Batch 50 Loss 1.6594 Accuracy 0.3060\n","Epoch 12 Batch 100 Loss 1.6526 Accuracy 0.3069\n","Epoch 12 Batch 150 Loss 1.6567 Accuracy 0.3078\n","Epoch 12 Batch 200 Loss 1.6618 Accuracy 0.3077\n","Epoch 12 Batch 250 Loss 1.6701 Accuracy 0.3072\n","Epoch 12 Batch 300 Loss 1.6751 Accuracy 0.3067\n","Epoch 12 Batch 350 Loss 1.6806 Accuracy 0.3070\n","Epoch 12 Batch 400 Loss 1.6831 Accuracy 0.3069\n","Epoch 12 Batch 450 Loss 1.6868 Accuracy 0.3059\n","Epoch 12 Batch 500 Loss 1.6940 Accuracy 0.3056\n","Epoch 12 Batch 550 Loss 1.6973 Accuracy 0.3055\n","Epoch 12 Batch 600 Loss 1.7037 Accuracy 0.3056\n","Epoch 12 Batch 650 Loss 1.7097 Accuracy 0.3058\n","Epoch 12 Batch 700 Loss 1.7152 Accuracy 0.3058\n","Epoch 12 Loss 1.7155 Accuracy 0.3058\n","Time taken for 1 epoch: 96.52027797698975 secs\n","\n","Epoch 13 Batch 0 Loss 1.5603 Accuracy 0.3552\n","Epoch 13 Batch 50 Loss 1.5314 Accuracy 0.3117\n","Epoch 13 Batch 100 Loss 1.5425 Accuracy 0.3153\n","Epoch 13 Batch 150 Loss 1.5531 Accuracy 0.3143\n","Epoch 13 Batch 200 Loss 1.5615 Accuracy 0.3150\n","Epoch 13 Batch 250 Loss 1.5654 Accuracy 0.3148\n","Epoch 13 Batch 300 Loss 1.5704 Accuracy 0.3155\n","Epoch 13 Batch 350 Loss 1.5785 Accuracy 0.3153\n","Epoch 13 Batch 400 Loss 1.5849 Accuracy 0.3154\n","Epoch 13 Batch 450 Loss 1.5887 Accuracy 0.3152\n","Epoch 13 Batch 500 Loss 1.5948 Accuracy 0.3144\n","Epoch 13 Batch 550 Loss 1.5988 Accuracy 0.3143\n","Epoch 13 Batch 600 Loss 1.6025 Accuracy 0.3140\n","Epoch 13 Batch 650 Loss 1.6079 Accuracy 0.3135\n","Epoch 13 Batch 700 Loss 1.6130 Accuracy 0.3131\n","Epoch 13 Loss 1.6132 Accuracy 0.3131\n","Time taken for 1 epoch: 96.57977604866028 secs\n","\n","Epoch 14 Batch 0 Loss 1.3888 Accuracy 0.3455\n","Epoch 14 Batch 50 Loss 1.4426 Accuracy 0.3258\n","Epoch 14 Batch 100 Loss 1.4511 Accuracy 0.3235\n","Epoch 14 Batch 150 Loss 1.4554 Accuracy 0.3230\n","Epoch 14 Batch 200 Loss 1.4624 Accuracy 0.3232\n","Epoch 14 Batch 250 Loss 1.4706 Accuracy 0.3226\n","Epoch 14 Batch 300 Loss 1.4812 Accuracy 0.3202\n","Epoch 14 Batch 350 Loss 1.4895 Accuracy 0.3193\n","Epoch 14 Batch 400 Loss 1.4940 Accuracy 0.3194\n","Epoch 14 Batch 450 Loss 1.5008 Accuracy 0.3193\n","Epoch 14 Batch 500 Loss 1.5043 Accuracy 0.3190\n","Epoch 14 Batch 550 Loss 1.5102 Accuracy 0.3189\n","Epoch 14 Batch 600 Loss 1.5165 Accuracy 0.3188\n","Epoch 14 Batch 650 Loss 1.5215 Accuracy 0.3187\n","Epoch 14 Batch 700 Loss 1.5281 Accuracy 0.3184\n","Epoch 14 Loss 1.5285 Accuracy 0.3184\n","Time taken for 1 epoch: 96.50227046012878 secs\n","\n","Epoch 15 Batch 0 Loss 1.3376 Accuracy 0.2941\n","Epoch 15 Batch 50 Loss 1.3585 Accuracy 0.3301\n","Epoch 15 Batch 100 Loss 1.3711 Accuracy 0.3284\n","Epoch 15 Batch 150 Loss 1.3834 Accuracy 0.3274\n","Epoch 15 Batch 200 Loss 1.3923 Accuracy 0.3277\n","Epoch 15 Batch 250 Loss 1.4016 Accuracy 0.3263\n","Epoch 15 Batch 300 Loss 1.4061 Accuracy 0.3256\n","Epoch 15 Batch 350 Loss 1.4141 Accuracy 0.3253\n","Epoch 15 Batch 400 Loss 1.4193 Accuracy 0.3258\n","Epoch 15 Batch 450 Loss 1.4230 Accuracy 0.3251\n","Epoch 15 Batch 500 Loss 1.4283 Accuracy 0.3242\n","Epoch 15 Batch 550 Loss 1.4335 Accuracy 0.3239\n","Epoch 15 Batch 600 Loss 1.4397 Accuracy 0.3239\n","Epoch 15 Batch 650 Loss 1.4450 Accuracy 0.3235\n","Epoch 15 Batch 700 Loss 1.4500 Accuracy 0.3233\n","Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n","Epoch 15 Loss 1.4502 Accuracy 0.3233\n","Time taken for 1 epoch: 96.75636005401611 secs\n","\n","Epoch 16 Batch 0 Loss 1.2736 Accuracy 0.3368\n","Epoch 16 Batch 50 Loss 1.3111 Accuracy 0.3320\n","Epoch 16 Batch 100 Loss 1.3078 Accuracy 0.3303\n","Epoch 16 Batch 150 Loss 1.3174 Accuracy 0.3300\n","Epoch 16 Batch 200 Loss 1.3263 Accuracy 0.3298\n","Epoch 16 Batch 250 Loss 1.3371 Accuracy 0.3299\n","Epoch 16 Batch 300 Loss 1.3438 Accuracy 0.3292\n","Epoch 16 Batch 350 Loss 1.3444 Accuracy 0.3294\n","Epoch 16 Batch 400 Loss 1.3492 Accuracy 0.3300\n","Epoch 16 Batch 450 Loss 1.3537 Accuracy 0.3299\n","Epoch 16 Batch 500 Loss 1.3609 Accuracy 0.3293\n","Epoch 16 Batch 550 Loss 1.3660 Accuracy 0.3296\n","Epoch 16 Batch 600 Loss 1.3719 Accuracy 0.3294\n","Epoch 16 Batch 650 Loss 1.3776 Accuracy 0.3290\n","Epoch 16 Batch 700 Loss 1.3839 Accuracy 0.3285\n","Epoch 16 Loss 1.3842 Accuracy 0.3284\n","Time taken for 1 epoch: 96.78457307815552 secs\n","\n","Epoch 17 Batch 0 Loss 1.1830 Accuracy 0.3750\n","Epoch 17 Batch 50 Loss 1.2314 Accuracy 0.3403\n","Epoch 17 Batch 100 Loss 1.2468 Accuracy 0.3377\n","Epoch 17 Batch 150 Loss 1.2533 Accuracy 0.3376\n","Epoch 17 Batch 200 Loss 1.2672 Accuracy 0.3372\n","Epoch 17 Batch 250 Loss 1.2770 Accuracy 0.3357\n","Epoch 17 Batch 300 Loss 1.2821 Accuracy 0.3352\n","Epoch 17 Batch 350 Loss 1.2847 Accuracy 0.3346\n","Epoch 17 Batch 400 Loss 1.2906 Accuracy 0.3340\n","Epoch 17 Batch 450 Loss 1.2981 Accuracy 0.3335\n","Epoch 17 Batch 500 Loss 1.3023 Accuracy 0.3333\n","Epoch 17 Batch 550 Loss 1.3085 Accuracy 0.3333\n","Epoch 17 Batch 600 Loss 1.3144 Accuracy 0.3331\n","Epoch 17 Batch 650 Loss 1.3195 Accuracy 0.3324\n","Epoch 17 Batch 700 Loss 1.3239 Accuracy 0.3323\n","Epoch 17 Loss 1.3244 Accuracy 0.3323\n","Time taken for 1 epoch: 96.21180820465088 secs\n","\n","Epoch 18 Batch 0 Loss 1.1923 Accuracy 0.3417\n","Epoch 18 Batch 50 Loss 1.1895 Accuracy 0.3449\n","Epoch 18 Batch 100 Loss 1.1925 Accuracy 0.3440\n","Epoch 18 Batch 150 Loss 1.2021 Accuracy 0.3428\n","Epoch 18 Batch 200 Loss 1.2114 Accuracy 0.3416\n","Epoch 18 Batch 250 Loss 1.2145 Accuracy 0.3408\n","Epoch 18 Batch 300 Loss 1.2245 Accuracy 0.3406\n","Epoch 18 Batch 350 Loss 1.2293 Accuracy 0.3404\n","Epoch 18 Batch 400 Loss 1.2326 Accuracy 0.3398\n","Epoch 18 Batch 450 Loss 1.2384 Accuracy 0.3404\n","Epoch 18 Batch 500 Loss 1.2449 Accuracy 0.3400\n","Epoch 18 Batch 550 Loss 1.2520 Accuracy 0.3393\n","Epoch 18 Batch 600 Loss 1.2571 Accuracy 0.3386\n","Epoch 18 Batch 650 Loss 1.2635 Accuracy 0.3382\n","Epoch 18 Batch 700 Loss 1.2695 Accuracy 0.3377\n","Epoch 18 Loss 1.2700 Accuracy 0.3377\n","Time taken for 1 epoch: 96.07073020935059 secs\n","\n","Epoch 19 Batch 0 Loss 1.2558 Accuracy 0.2965\n","Epoch 19 Batch 50 Loss 1.1347 Accuracy 0.3490\n","Epoch 19 Batch 100 Loss 1.1421 Accuracy 0.3490\n","Epoch 19 Batch 150 Loss 1.1510 Accuracy 0.3471\n","Epoch 19 Batch 200 Loss 1.1604 Accuracy 0.3475\n","Epoch 19 Batch 250 Loss 1.1695 Accuracy 0.3457\n","Epoch 19 Batch 300 Loss 1.1768 Accuracy 0.3435\n","Epoch 19 Batch 350 Loss 1.1820 Accuracy 0.3439\n","Epoch 19 Batch 400 Loss 1.1880 Accuracy 0.3435\n","Epoch 19 Batch 450 Loss 1.1933 Accuracy 0.3431\n","Epoch 19 Batch 500 Loss 1.1970 Accuracy 0.3426\n","Epoch 19 Batch 550 Loss 1.2013 Accuracy 0.3423\n","Epoch 19 Batch 600 Loss 1.2069 Accuracy 0.3419\n","Epoch 19 Batch 650 Loss 1.2128 Accuracy 0.3414\n","Epoch 19 Batch 700 Loss 1.2192 Accuracy 0.3407\n","Epoch 19 Loss 1.2192 Accuracy 0.3407\n","Time taken for 1 epoch: 95.74314761161804 secs\n","\n","Epoch 20 Batch 0 Loss 1.1448 Accuracy 0.3606\n","Epoch 20 Batch 50 Loss 1.0874 Accuracy 0.3505\n","Epoch 20 Batch 100 Loss 1.1000 Accuracy 0.3492\n","Epoch 20 Batch 150 Loss 1.1079 Accuracy 0.3499\n","Epoch 20 Batch 200 Loss 1.1175 Accuracy 0.3481\n","Epoch 20 Batch 250 Loss 1.1278 Accuracy 0.3475\n","Epoch 20 Batch 300 Loss 1.1336 Accuracy 0.3466\n","Epoch 20 Batch 350 Loss 1.1374 Accuracy 0.3463\n","Epoch 20 Batch 400 Loss 1.1430 Accuracy 0.3458\n","Epoch 20 Batch 450 Loss 1.1487 Accuracy 0.3450\n","Epoch 20 Batch 500 Loss 1.1559 Accuracy 0.3450\n","Epoch 20 Batch 550 Loss 1.1619 Accuracy 0.3450\n","Epoch 20 Batch 600 Loss 1.1674 Accuracy 0.3443\n","Epoch 20 Batch 650 Loss 1.1720 Accuracy 0.3440\n","Epoch 20 Batch 700 Loss 1.1773 Accuracy 0.3438\n","Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n","Epoch 20 Loss 1.1777 Accuracy 0.3438\n","Time taken for 1 epoch: 96.13243126869202 secs\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2XuFrJnZmjOl","colab_type":"code","colab":{}},"source":["def evaluate(inp_sentence):\n","  start_token = [tokenizer_pt.vocab_size]\n","  end_token = [tokenizer_pt.vocab_size+1]\n","\n","  inp_sentence = start_token + tokenizer_pt.encode(inp_sentence) + end_token\n","  encoder_input = tf.expand_dims(inp_sentence, 0)\n","\n","  decoder_input = [tokenizer_en.vocab_size] # sos\n","  output = tf.expand_dims(decoder_input, 0)\n","\n","  for i in range(MAX_LENGTH):\n","    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n","\n","    predictions, attention_weights = transfomer(encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n","\n","    predictions = predictions[:, -1:, :]\n","\n","    predictions_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n","    print(predictions_id)\n","\n","    if predictions_id == tokenizer_en.vocab_size + 1:\n","      return tf.squeeze(output, axis=0), attention_weights\n","    \n","    output = tf.concat([output, predictions_id], axis=-1)\n","\n","    return tf.squeeze(output, axis=0), attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S48RdlX3nlPy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"322d00bc-dfc5-433f-e624-39492136c814","executionInfo":{"status":"ok","timestamp":1585843269324,"user_tz":-540,"elapsed":845,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["evaluate('e quando melhoramos a procura')"],"execution_count":304,"outputs":[{"output_type":"stream","text":["tf.Tensor([[6513]], shape=(1, 1), dtype=int32)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([8087, 6513], dtype=int32)>,\n"," {'decoder_layer1_block1': <tf.Tensor: shape=(1, 8, 1, 1), dtype=float32, numpy=\n","  array([[[[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]]]], dtype=float32)>,\n","  'decoder_layer1_block2': <tf.Tensor: shape=(1, 8, 1, 8), dtype=float32, numpy=\n","  array([[[[0.12341358, 0.14171085, 0.13273488, 0.11244687, 0.11873712,\n","            0.11593676, 0.12598245, 0.12903745]],\n","  \n","          [[0.11478388, 0.11327739, 0.12973899, 0.134973  , 0.11143824,\n","            0.12816773, 0.12040751, 0.14721322]],\n","  \n","          [[0.11759933, 0.11360823, 0.10857452, 0.13752243, 0.11668846,\n","            0.14760564, 0.12725931, 0.13114208]],\n","  \n","          [[0.11356229, 0.10729198, 0.10343185, 0.09524784, 0.12601839,\n","            0.19813696, 0.1346061 , 0.12170455]],\n","  \n","          [[0.11346406, 0.14177814, 0.12365998, 0.11197163, 0.14405005,\n","            0.12596327, 0.13637021, 0.10274263]],\n","  \n","          [[0.09498871, 0.10718821, 0.1172436 , 0.12798882, 0.13416077,\n","            0.12378065, 0.15288244, 0.14176677]],\n","  \n","          [[0.16873568, 0.1286492 , 0.11484798, 0.12322112, 0.09662259,\n","            0.10498195, 0.12716323, 0.13577822]],\n","  \n","          [[0.13254872, 0.13686739, 0.11503107, 0.1598912 , 0.12953533,\n","            0.08840234, 0.13162485, 0.10609908]]]], dtype=float32)>,\n","  'decoder_layer2_block1': <tf.Tensor: shape=(1, 8, 1, 1), dtype=float32, numpy=\n","  array([[[[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]]]], dtype=float32)>,\n","  'decoder_layer2_block2': <tf.Tensor: shape=(1, 8, 1, 8), dtype=float32, numpy=\n","  array([[[[0.10967931, 0.10534523, 0.11148319, 0.11418214, 0.1281917 ,\n","            0.16639481, 0.10694287, 0.15778069]],\n","  \n","          [[0.12977184, 0.11006804, 0.12881158, 0.13889265, 0.12015974,\n","            0.10643246, 0.132187  , 0.13367668]],\n","  \n","          [[0.10632061, 0.09402286, 0.14237396, 0.14607303, 0.14462624,\n","            0.13592374, 0.11895312, 0.11170648]],\n","  \n","          [[0.10531484, 0.10381088, 0.13630702, 0.13572635, 0.14600946,\n","            0.1466125 , 0.10127237, 0.12494662]],\n","  \n","          [[0.14030045, 0.1074512 , 0.10273672, 0.14024907, 0.12999353,\n","            0.1257116 , 0.11292393, 0.14063354]],\n","  \n","          [[0.1279062 , 0.15797724, 0.11934503, 0.12987097, 0.11306612,\n","            0.11582214, 0.10630485, 0.12970743]],\n","  \n","          [[0.11601944, 0.14793539, 0.1405065 , 0.11762711, 0.13429941,\n","            0.1218114 , 0.09795024, 0.12385045]],\n","  \n","          [[0.08056138, 0.11703543, 0.09288027, 0.08988881, 0.12326437,\n","            0.1678914 , 0.18434194, 0.14413641]]]], dtype=float32)>,\n","  'decoder_layer3_block1': <tf.Tensor: shape=(1, 8, 1, 1), dtype=float32, numpy=\n","  array([[[[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]]]], dtype=float32)>,\n","  'decoder_layer3_block2': <tf.Tensor: shape=(1, 8, 1, 8), dtype=float32, numpy=\n","  array([[[[0.11583024, 0.12303175, 0.12631232, 0.13874687, 0.13949923,\n","            0.11554649, 0.1332044 , 0.10782866]],\n","  \n","          [[0.10898401, 0.11059438, 0.12662862, 0.13261998, 0.13230023,\n","            0.14387263, 0.11940189, 0.12559824]],\n","  \n","          [[0.13664804, 0.12273315, 0.12965102, 0.10370049, 0.14548631,\n","            0.10983873, 0.13899773, 0.11294447]],\n","  \n","          [[0.12468348, 0.11984475, 0.12776457, 0.133677  , 0.11566713,\n","            0.13563105, 0.12010051, 0.12263148]],\n","  \n","          [[0.16419837, 0.11537416, 0.13369815, 0.11742823, 0.13922384,\n","            0.10199023, 0.12319635, 0.10489066]],\n","  \n","          [[0.11150436, 0.101095  , 0.10827666, 0.12809314, 0.15917397,\n","            0.14988197, 0.13458788, 0.10738704]],\n","  \n","          [[0.13726006, 0.1628916 , 0.155004  , 0.11020603, 0.11921719,\n","            0.10109296, 0.10832915, 0.10599906]],\n","  \n","          [[0.11755237, 0.16676414, 0.10676534, 0.15201321, 0.10888983,\n","            0.11538669, 0.1068597 , 0.1257687 ]]]], dtype=float32)>,\n","  'decoder_layer4_block1': <tf.Tensor: shape=(1, 8, 1, 1), dtype=float32, numpy=\n","  array([[[[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]],\n","  \n","          [[1.]]]], dtype=float32)>,\n","  'decoder_layer4_block2': <tf.Tensor: shape=(1, 8, 1, 8), dtype=float32, numpy=\n","  array([[[[0.10769019, 0.11113049, 0.15433288, 0.11905745, 0.10429535,\n","            0.145274  , 0.14105678, 0.11716291]],\n","  \n","          [[0.16722596, 0.11997791, 0.10815635, 0.11751386, 0.1706712 ,\n","            0.0829433 , 0.10664698, 0.12686439]],\n","  \n","          [[0.14700982, 0.13340887, 0.09717292, 0.09198514, 0.11790517,\n","            0.1071361 , 0.1457967 , 0.1595853 ]],\n","  \n","          [[0.12860443, 0.14896362, 0.14056888, 0.12086201, 0.11906535,\n","            0.10512173, 0.1137738 , 0.1230402 ]],\n","  \n","          [[0.14857148, 0.1338379 , 0.1436738 , 0.11959718, 0.11664402,\n","            0.11103854, 0.10005654, 0.1265805 ]],\n","  \n","          [[0.11203205, 0.1298891 , 0.14081259, 0.11891919, 0.1022562 ,\n","            0.13045916, 0.12368274, 0.14194903]],\n","  \n","          [[0.12788536, 0.10876228, 0.11242805, 0.14878756, 0.12225502,\n","            0.13215281, 0.1252534 , 0.1224755 ]],\n","  \n","          [[0.11734283, 0.12347797, 0.1201413 , 0.11875939, 0.12084743,\n","            0.11824019, 0.14322911, 0.13796183]]]], dtype=float32)>})"]},"metadata":{"tags":[]},"execution_count":304}]},{"cell_type":"code","metadata":{"id":"PmU8djIwnu-k","colab_type":"code","colab":{}},"source":["def plot_attention_weights(attention, sentence, result, layer):\n","  fig = plt.figure(figsize=(16, 8))\n","  \n","  sentence = tokenizer_pt.encode(sentence)\n","  \n","  attention = tf.squeeze(attention[layer], axis=0)\n","  \n","  for head in range(attention.shape[0]):\n","    ax = fig.add_subplot(2, 4, head+1)\n","    \n","    # plot the attention weights\n","    ax.matshow(attention[head][:-1, :], cmap='viridis')\n","\n","    fontdict = {'fontsize': 10}\n","    \n","    ax.set_xticks(range(len(sentence)+2))\n","    ax.set_yticks(range(len(result)))\n","    \n","    # ax.set_ylim(len(result)-1.5, -0.5)\n","        \n","    ax.set_xticklabels(\n","        ['<start>']+[tokenizer_pt.decode([i]) for i in sentence]+['<end>'], \n","        fontdict=fontdict, rotation=90)\n","    \n","    ax.set_yticklabels([tokenizer_en.decode([i]) for i in result \n","                        if i < tokenizer_en.vocab_size], \n","                       fontdict=fontdict)\n","    \n","    ax.set_xlabel('Head {}'.format(head+1))\n","  \n","  plt.tight_layout()\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"82ldCsgSoIzP","colab_type":"code","colab":{}},"source":["def translate(sentence, plot=''):\n","  result, attention_weights = evaluate(sentence)\n","  \n","  predicted_sentence = tokenizer_en.decode([i for i in result \n","                                            if i < tokenizer_en.vocab_size])  \n","\n","  print('Input: {}'.format(sentence))\n","  print('Predicted translation: {}'.format(predicted_sentence))\n","  \n","  if plot:\n","    plot_attention_weights(attention_weights, sentence, result, plot)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bp-gXxbWoYCf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"outputId":"0090361c-790b-4456-b34a-64ab95b8a8b8","executionInfo":{"status":"ok","timestamp":1585843379371,"user_tz":-540,"elapsed":717,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["transformer.summary()"],"execution_count":310,"outputs":[{"output_type":"stream","text":["Model: \"transformer_19\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","encoder_26 (Encoder)         multiple                  1844736   \n","_________________________________________________________________\n","decoder_32 (Decoder)         multiple                  2093696   \n","_________________________________________________________________\n","dense_1443 (Dense)           multiple                  1043481   \n","=================================================================\n","Total params: 4,981,913\n","Trainable params: 4,981,913\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pX7j3XjgoKTP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":459},"outputId":"da845cf8-9c30-4eca-af75-1bb40a22e331","executionInfo":{"status":"ok","timestamp":1585843451952,"user_tz":-540,"elapsed":1654,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}}},"source":["translate(\"este é o primeiro livro que eu fiz.\", plot='decoder_layer4_block2')\n","print (\"Real translation: this is the first book i've ever done.\")"],"execution_count":321,"outputs":[{"output_type":"stream","text":["tf.Tensor([[739]], shape=(1, 1), dtype=int32)\n","Input: este é o primeiro livro que eu fiz.\n","Predicted translation: questions \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: UserWarning: Attempting to set identical bottom == top == -0.5 results in singular transformations; automatically expanding.\n","  if sys.path[0] == '':\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABHgAAAFUCAYAAABfpIupAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0sA54JEkSZIkSRo4BzySJEmSJEkD9/8DBQZqob9q3pMAAAAASUVORK5CYII=\n","text/plain":["<Figure size 1152x576 with 8 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Real translation: this is the first book i've ever done.\n"],"name":"stdout"}]}]}