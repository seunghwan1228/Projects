{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"checking_encoder_decoder.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN+g9ZphezFHA7EZUZDgKha"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tVmC5sK5w4Xa","colab_type":"text"},"source":["## Similar Souce with tensorflow\n","\n","https://machinetalk.org/2019/03/29/neural-machine-translation-with-attention-mechanism/"]},{"cell_type":"code","metadata":{"id":"gTWudaaGuezP","colab_type":"code","outputId":"2c98adf0-837b-4412-e815-8592581e2e3a","executionInfo":{"status":"ok","timestamp":1583719634160,"user_tz":-540,"elapsed":561,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lkpiI5Hdulqi","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import os\n","import re\n","import unicodedata"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3CM96prAhrQ","colab_type":"code","outputId":"241f5f68-b527-4d0e-db35-c511370ae6dc","executionInfo":{"status":"ok","timestamp":1583719643049,"user_tz":-540,"elapsed":9439,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["path_to_zip = tf.keras.utils.get_file(\n","    'spa-eng.zip', origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n","    extract=True)\n","\n","path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n","2646016/2638744 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E0W5LSzQ_iCN","colab_type":"code","colab":{}},"source":["df = pd.read_csv(path_to_file, sep='\\t', header=None)\n","df.columns = ['eng', 'spa']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xGCkV5Ng_pE2","colab_type":"code","outputId":"b3eeba5f-5bc5-48d8-a6f3-fd44cc805c5c","executionInfo":{"status":"ok","timestamp":1583719643051,"user_tz":-540,"elapsed":9429,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["df.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(118964, 2)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"N4o1VMYh_36h","colab_type":"code","outputId":"c0a25dd0-db7a-444f-86da-c890eacb82a7","executionInfo":{"status":"ok","timestamp":1583719643340,"user_tz":-540,"elapsed":9709,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":359}},"source":["df.sample(10)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>eng</th>\n","      <th>spa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>38251</th>\n","      <td>Tom wore a white jacket.</td>\n","      <td>Tomás tenía puesto una campera blanca.</td>\n","    </tr>\n","    <tr>\n","      <th>31233</th>\n","      <td>He was drunk and angry.</td>\n","      <td>Estaba borracho y furioso.</td>\n","    </tr>\n","    <tr>\n","      <th>108661</th>\n","      <td>I can't afford to shop at such an expensive st...</td>\n","      <td>No puedo permitirme comprar en una tienda tan ...</td>\n","    </tr>\n","    <tr>\n","      <th>116800</th>\n","      <td>What I can't bear is the sound of chalk squeak...</td>\n","      <td>Lo que no puedo soportar es el sonido de la ti...</td>\n","    </tr>\n","    <tr>\n","      <th>79411</th>\n","      <td>I can't believe you're really here.</td>\n","      <td>No me puedo creer que estés aquí.</td>\n","    </tr>\n","    <tr>\n","      <th>86483</th>\n","      <td>The theft must've been an inside job.</td>\n","      <td>El robo tiene que haberse hecho por alguien de...</td>\n","    </tr>\n","    <tr>\n","      <th>100781</th>\n","      <td>I want to spend the whole weekend in Boston.</td>\n","      <td>Quiero pasar el fin de semana completo en Boston.</td>\n","    </tr>\n","    <tr>\n","      <th>72336</th>\n","      <td>I always get along well with him.</td>\n","      <td>Yo siempre me llevo bien con él.</td>\n","    </tr>\n","    <tr>\n","      <th>59387</th>\n","      <td>We traveled around Australia.</td>\n","      <td>Viajamos por toda Australia.</td>\n","    </tr>\n","    <tr>\n","      <th>20550</th>\n","      <td>I'm a tennis player.</td>\n","      <td>Soy tenista.</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                      eng                                                spa\n","38251                            Tom wore a white jacket.             Tomás tenía puesto una campera blanca.\n","31233                             He was drunk and angry.                         Estaba borracho y furioso.\n","108661  I can't afford to shop at such an expensive st...  No puedo permitirme comprar en una tienda tan ...\n","116800  What I can't bear is the sound of chalk squeak...  Lo que no puedo soportar es el sonido de la ti...\n","79411                 I can't believe you're really here.                  No me puedo creer que estés aquí.\n","86483               The theft must've been an inside job.  El robo tiene que haberse hecho por alguien de...\n","100781       I want to spend the whole weekend in Boston.  Quiero pasar el fin de semana completo en Boston.\n","72336                   I always get along well with him.                   Yo siempre me llevo bien con él.\n","59387                       We traveled around Australia.                       Viajamos por toda Australia.\n","20550                                I'm a tennis player.                                       Soy tenista."]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"JhNo9gob_pdH","colab_type":"code","colab":{}},"source":["def unicode_to_ascii(sentence):\n","  return ''.join(word for word in unicodedata.normalize('NFD', sentence) if unicodedata.category(word) != 'Mn')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sleOd3btA2IK","colab_type":"code","outputId":"34779d7d-338b-4ba1-b559-8bd9f75c88d0","executionInfo":{"status":"ok","timestamp":1583719643341,"user_tz":-540,"elapsed":9700,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["unicodedata.category('¿')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Po'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"DRFIXCXBA5CZ","colab_type":"code","colab":{}},"source":["def preprocess_text(sentence):\n","  sentence = unicode_to_ascii(sentence)\n","  sentence = re.sub(r'([!.,?¿])', r' \\1 ', sentence) # \\1 means the first charater.. \"<space> <first char> <space>\"\n","  sentence = re.sub(r'[\" \"]+', ' ', sentence)\n","  sentence = re.sub(r'[^a-zA-Z!.,?¿]+', ' ', sentence)\n","  sentence = sentence.rstrip().strip()\n","  sentence = '<start> '+sentence+' <end>'\n","  return sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"17u6e7XVBwgK","colab_type":"code","outputId":"1ebc10af-eba6-4734-e8bc-5156a18f2476","executionInfo":{"status":"ok","timestamp":1583719643342,"user_tz":-540,"elapsed":9690,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["preprocess_text('heoool.')"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<start> heoool . <end>'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"J0-ZJe_SCmOh","colab_type":"code","colab":{}},"source":["df['eng'] = df['eng'].apply(preprocess_text)\n","df['spa'] = df['spa'].apply(preprocess_text)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJuUDt5QCwFD","colab_type":"code","outputId":"6af39d88-0ab3-458e-d047-f092f404d3e0","executionInfo":{"status":"ok","timestamp":1583719646934,"user_tz":-540,"elapsed":13272,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["df.head()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>eng</th>\n","      <th>spa</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>&lt;start&gt; Go . &lt;end&gt;</td>\n","      <td>&lt;start&gt; Ve . &lt;end&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>&lt;start&gt; Go . &lt;end&gt;</td>\n","      <td>&lt;start&gt; Vete . &lt;end&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>&lt;start&gt; Go . &lt;end&gt;</td>\n","      <td>&lt;start&gt; Vaya . &lt;end&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>&lt;start&gt; Go . &lt;end&gt;</td>\n","      <td>&lt;start&gt; Vayase . &lt;end&gt;</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>&lt;start&gt; Hi . &lt;end&gt;</td>\n","      <td>&lt;start&gt; Hola . &lt;end&gt;</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  eng                     spa\n","0  <start> Go . <end>      <start> Ve . <end>\n","1  <start> Go . <end>    <start> Vete . <end>\n","2  <start> Go . <end>    <start> Vaya . <end>\n","3  <start> Go . <end>  <start> Vayase . <end>\n","4  <start> Hi . <end>    <start> Hola . <end>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"dggnvUy4KgFN","colab_type":"code","colab":{}},"source":["eng_data = df['eng'].tolist()\n","spa_data = df['spa'].tolist()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LzYr2IRuK08u","colab_type":"code","colab":{}},"source":["lang_token = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<UNK>')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2eyOVsPGKm_9","colab_type":"code","colab":{}},"source":["def tokenize_lang(language_data):\n","  lang_token = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<UNK>')\n","  lang_token.fit_on_texts(language_data)\n","  lang_sequence = lang_token.texts_to_sequences(language_data)\n","  return lang_sequence, lang_token"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pTpVp0KOLAXc","colab_type":"code","colab":{}},"source":["eng_sequence, eng_token = tokenize_lang(eng_data)\n","spa_sequence, spa_token = tokenize_lang(spa_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qad43IzjLG7q","colab_type":"code","outputId":"1f623d4f-1521-4895-fc81-1566242bac16","executionInfo":{"status":"ok","timestamp":1583719650527,"user_tz":-540,"elapsed":16850,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["def lang_maxlength(lang_sequence):\n","  result = max([len(seq) for seq in lang_sequence])\n","  return result\n","\n","print(lang_maxlength(eng_sequence))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["51\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o6z4lzbSLtY9","colab_type":"code","outputId":"2fe9ec1a-9c7c-4047-b62c-533eb4a1a799","executionInfo":{"status":"ok","timestamp":1583719651446,"user_tz":-540,"elapsed":17759,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["eng_maxlen = lang_maxlength(eng_sequence)\n","spa_maxlen = lang_maxlength(spa_sequence)\n","\n","def pad_sequence(lang_seq, max_len):\n","  return tf.keras.preprocessing.sequence.pad_sequences(lang_seq, maxlen=max_len, padding='post')\n","\n","eng_sequence = pad_sequence(eng_sequence, eng_maxlen)\n","spa_sequence = pad_sequence(spa_sequence, spa_maxlen)\n","\n","print(f'eng : {eng_sequence.shape}, eng_max_length: {eng_maxlen}, Token: eng_token')\n","print(f'eng : {spa_sequence.shape}, eng_max_length: {spa_maxlen}, Token: spa_token')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["eng : (118964, 51), eng_max_length: 51, Token: eng_token\n","eng : (118964, 53), eng_max_length: 53, Token: spa_token\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"27dU9Tl8N3zB","colab_type":"code","outputId":"35244cd8-eabb-4a70-8eb0-c155e2603c9d","executionInfo":{"status":"ok","timestamp":1583719651447,"user_tz":-540,"elapsed":17751,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["spa_target_sequence = np.zeros(spa_sequence.shape, dtype=np.int)\n","spa_target_sequence[:, :-1] = spa_sequence[:, 1:]\n","\n","print(spa_sequence[0])\n","print(f'spa input seq : {spa_sequence.shape}')\n","print('==========================================')\n","print(spa_target_sequence[0])\n","print(f'spa target seq : {spa_target_sequence.shape}')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[  2 365   4   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n","spa input seq : (118964, 53)\n","==========================================\n","[365   4   3   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n","spa target seq : (118964, 53)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k1lz_y4t8cf3","colab_type":"code","colab":{}},"source":["dataset = tf.data.Dataset.from_tensor_slices((eng_sequence, spa_sequence, spa_target_sequence)).batch(5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FGGU3oW8cd8","colab_type":"code","colab":{}},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, lstm_units):\n","    super(Encoder, self).__init__()\n","    self.vocab_size = vocab_size\n","    self.embedding_dim = embedding_dim\n","    self.lstm_units = lstm_units\n","\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True, return_state=True)\n","\n","  def call(self, inputs):\n","    sequence = inputs[0]\n","    state = inputs[1]\n","\n","    embed = self.embedding(sequence)\n","    # print(embed)\n","    out, state_h, state_c = self.lstm(embed, initial_state=state)\n","    # print(out)\n","    # print(state_h)\n","    # print(state_c)\n","    return out, state_h, state_c\n","\n","  def init_state(self, batch_size):\n","    return (tf.zeros([batch_size, self.lstm_units]),\n","            tf.zeros([batch_size, self.lstm_units]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Srm8ualdI4t5","colab_type":"code","colab":{}},"source":["class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, lstm_units):\n","    super(Decoder, self).__init__()\n","    self.vocab_size = vocab_size\n","    self.embedding_dim = embedding_dim\n","    self.lstm_units = lstm_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.lstm = tf.keras.layers.LSTM(lstm_units, return_sequences=True, return_state=True)\n","    self.dense = tf.keras.layers.Dense(vocab_size)\n","  \n","  def call(self, inputs):\n","    sequence = inputs[0]\n","    state = inputs[1]\n","\n","    embed = self.embedding(sequence)\n","    # print(embed)\n","    \n","    lstm_out, state_h, state_c = self.lstm(embed, initial_state=state)\n","    # print(lstm_out)\n","    # print(state_h)\n","    # print(state_c)\n","\n","    logits = self.dense(lstm_out)\n","    # print(logits)\n","\n","    return logits, state_h, state_c"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fye5QFU1Jy1K","colab_type":"code","outputId":"d373e3dc-5eb7-4c06-d84e-41540cc41e14","executionInfo":{"status":"ok","timestamp":1583725370520,"user_tz":-540,"elapsed":676,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["class test_env:\n","  embedding_size = 300\n","  lstm_unit = 256\n","  en_vocab_size = len(eng_token.word_index) + 1\n","  sp_vocab_size = len(spa_token.word_index) + 1\n","\n","test_hp = test_env()\n","\n","print('eng')\n","print(test_hp.en_vocab_size)\n","print('spa')\n","print(test_hp.sp_vocab_size)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["eng\n","12935\n","spa\n","24795\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"c9T5MgUdKHeR","colab_type":"code","colab":{}},"source":["encoder = Encoder(test_hp.en_vocab_size, test_hp.embedding_size, test_hp.lstm_unit)\n","\n","decoder = Decoder(test_hp.sp_vocab_size, test_hp.embedding_size, test_hp.lstm_unit)\n","\n","# testing\n","source_input = tf.constant([[1,3,5,7,2,0,0,0]])\n","init_state = encoder.init_state(1)\n","\n","encoder_output, en_state_h, en_state_c = encoder([source_input, init_state])\n","\n","target_input = tf.constant([[1,4,6,9,2,0,0]])\n","decoder_output, de_state_h, de_state_c = decoder([target_input, (en_state_h, en_state_c)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-3Z9AP7bK86V","colab_type":"code","outputId":"4f6436ad-9814-41dd-ec14-c11b357826cf","executionInfo":{"status":"ok","timestamp":1583725379018,"user_tz":-540,"elapsed":619,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["print('Source sequences', source_input.shape)\n","print('Encoder outputs', encoder_output.shape)\n","print('Encoder state_h', en_state_h.shape)\n","print('Encoder state_c', en_state_c.shape)\n","print()\n","print('Destination sequences', target_input.shape)\n","print('Decoder outputs', decoder_output.shape)\n","print('Decoder state_h', de_state_h.shape)\n","print('Decoder state_c', de_state_c.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Source sequences (1, 8)\n","Encoder outputs (1, 8, 256)\n","Encoder state_h (1, 256)\n","Encoder state_c (1, 256)\n","\n","Destination sequences (1, 7)\n","Decoder outputs (1, 7, 24795)\n","Decoder state_h (1, 256)\n","Decoder state_c (1, 256)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BN-WMfZHKVgM","colab_type":"code","colab":{}},"source":["# 1\n","crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","def loss_function(targets, logits):\n","  mask = tf.math.equal(targets, 0)\n","  mask = tf.math.logical_not(mask)\n","  mask = tf.cast(mask, tf.int64)\n","  loss = crossentropy(targets, logits, sample_weight=mask)\n","  return loss\n","\n","# 2\n","ce = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","def non_mask_loss_function(targets, logits):\n","  loss = ce(targets, logits)\n","  return loss\n","\n","\n","# 3\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n","\n","def original_loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"H6WFtKJsM6y0","colab_type":"code","outputId":"6682998e-4700-4e70-d935-da232711b438","executionInfo":{"status":"ok","timestamp":1583720900245,"user_tz":-540,"elapsed":632,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"source":["test_target = tf.constant([[1,2,3,0,0]])\n","\n","test_pred = tf.constant([[[0.7,0.1,0.1,0.1],    # 0\n","                          [0.1,0.7,0.1,0.1],    # 1\n","                          [0.1,0.1,0.7,0.1],    # 2\n","                          [0.7,0.1,0.1,0.1],    # 0\n","                          [0.7,0.1,0.1,0.1]]])  # 0\n","\n","test_pred_2 = tf.constant([[\n","                          [0.1,0.7,0.1,0.1],    # 1\n","                          [0.1,0.1,0.7,0.1],    # 2\n","                          [0.1,0.1,0.1,0.7],    # 3\n","                          [0.7,0.1,0.1,0.1],    # 0\n","                          [0.7,0.1,0.1,0.1]     # 0\n","                          ]])               \n","\n","print(test_pred)\n","print('----')\n","print(test_target)\n","print('with mask loss')\n","print('0,1,2,0,0')\n","print(loss_function(test_target, test_pred))\n","print('1,2,3,0,0')\n","print(loss_function(test_target, test_pred_2))\n","print()\n","print('with out mask loss')\n","print('0,1,2,0,0')\n","print(non_mask_loss_function(test_target, test_pred))\n","print('1,2,3,0,0')\n","print(non_mask_loss_function(test_target, test_pred_2))\n","print()\n","print('original tf loss')\n","print('0,1,2,0,0')\n","print(original_loss_function(test_target, test_pred))\n","print('1,2,3,0,0')\n","print(original_loss_function(test_target, test_pred_2))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tf.Tensor(\n","[[[0.7 0.1 0.1 0.1]\n","  [0.1 0.7 0.1 0.1]\n","  [0.1 0.1 0.7 0.1]\n","  [0.7 0.1 0.1 0.1]\n","  [0.7 0.1 0.1 0.1]]], shape=(1, 5, 4), dtype=float32)\n","----\n","tf.Tensor([[1 2 3 0 0]], shape=(1, 5), dtype=int32)\n","with mask loss\n","0,1,2,0,0\n","tf.Tensor(0.94392794, shape=(), dtype=float32)\n","1,2,3,0,0\n","tf.Tensor(0.58392805, shape=(), dtype=float32)\n","\n","with out mask loss\n","0,1,2,0,0\n","tf.Tensor(1.3332134, shape=(), dtype=float32)\n","1,2,3,0,0\n","tf.Tensor(0.9732134, shape=(), dtype=float32)\n","\n","original tf loss\n","0,1,2,0,0\n","tf.Tensor(0.94392794, shape=(), dtype=float32)\n","1,2,3,0,0\n","tf.Tensor(0.58392805, shape=(), dtype=float32)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BuSxtE2VKfjN","colab_type":"code","colab":{}},"source":["encoder = Encoder(test_hp.en_vocab_size, test_hp.embedding_size, test_hp.lstm_unit)\n","decoder = Decoder(test_hp.sp_vocab_size, test_hp.embedding_size, test_hp.lstm_unit)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nslGTsS_8ccE","colab_type":"code","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q2zKawQdRyUQ","colab_type":"code","colab":{}},"source":["# @tf.function\n","# def train_step(source_sequence, target_seq_in, target_seq_out, en_initial_state):\n","#   with tf.GradientTape() as tape:\n","#     en_outputs, en_h_state, en_c_state = encoder([source_sequence, en_initial_state])\n","#     en_states = (en_h_state, en_c_state) # state_h & state_c\n","#     de_states = en_states\n","\n","#     de_outputs = decoder([target_seq_in, de_states])\n","#     logits= de_outputs[0]\n","#     loss = loss_function(target_seq_out, logits)\n","  \n","#   training_variables = encoder.trainable_variables + decoder.trainable_variables\n","#   gradients = tape.gradient(loss, training_variables)\n","#   optimizer.apply_gradients(zip(gradients, training_variables))\n","#   return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NjZxT-rwsDBo","colab_type":"code","colab":{}},"source":["def loss_func(targets, logits):\n","  crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n","      from_logits=True)\n","  mask = tf.math.logical_not(tf.math.equal(targets, 0))\n","  mask = tf.cast(mask, dtype=tf.int64)\n","  loss = crossentropy(targets, logits, sample_weight=mask)\n","  return loss\n","\n","    \n","# @tf.function\n","# def train_step(source_seq, target_seq_in, target_seq_out, en_initial_states):\n","\n","#   with tf.GradientTape() as tape:\n","#       en_outputs = encoder([source_seq, en_initial_states])  # input: (b, 51)\n","#                                                            # init_state: (b, units)   \n","#       en_states = en_outputs[1:]                           # state_h, state_c: (b, units), (b, units)\n","#       de_states = en_states                                 \n","\n","#       de_outputs = decoder([target_seq_in, de_states])       # input: (b, 53)\n","#                                                            # de state: state_h, state_c: (b, units), (b, units)\n","#       logits = de_outputs[0]                               # logits: (b, output_units)          \n","#       loss = loss_func(target_seq_out, logits)\n","\n","#   variables = encoder.trainable_variables + decoder.trainable_variables\n","#   gradients = tape.gradient(loss, variables)\n","#   optimizer.apply_gradients(zip(gradients, variables))\n","#   return loss\n","\n","@tf.function\n","def train_step(source_seq, target_seq_in, target_seq_out, en_initial_states):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    en_outputs, en_state_h, en_state_c = encoder([source_seq, en_initial_states])\n","\n","    dec_input = tf.expand_dims([spa_token.word_index['<start>']] * 32, 1)\n","    dec_states = [en_state_h, en_state_c]\n","\n","    for t in range(target_seq_in.shape[1]):\n","      prediction, de_state_h, de_state_c = decoder([dec_input, dec_states])\n","      loss += loss_func(target_seq_out[:, t], prediction)\n","      dec_input = tf.expand_dims(target_seq_out[:, t], 1)\n","    \n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","  gradients = tape.gradient(loss, variables)\n","  optimizer.apply_gradients(zip(gradients, variables))\n","  return loss / target_seq_out.shape[1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SUwMt7JEdBud","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lwc6HqWlSjz_","colab_type":"code","colab":{}},"source":["def inference():\n","  test_source_text = eng_data[np.random.choice(len(eng_data))]\n","  print(test_source_text)\n","  test_source_seq = eng_token.texts_to_sequences([test_source_text])\n","  print(test_source_seq)\n","\n","  idx2spa = {v:k for k, v in spa_token.word_index.items()}\n","\n","\n","  en_initial_states = encoder.init_state(1) # batch 1\n","  en_outputs = encoder([tf.constant(test_source_seq), en_initial_states])\n","\n","  de_input = tf.constant([[spa_token.word_index['<start>']]]) # shape 1, 1\n","  de_state_h, de_state_c = en_outputs[1:]\n","\n","  out_word = []\n","\n","  while True:\n","    de_output, de_state_h, de_state_c = decoder([de_input, (de_state_h, de_state_c)])\n","    de_input = tf.argmax(de_output, axis=-1)\n","    out_word.append(spa_token.index_word[de_input.numpy()[0][0]])\n","    if out_word[-1] == '<end>' or len(out_word) >= 20:\n","      break\n","  \n","  print(' '.join(out_word))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ynE11WhtUWmt","colab_type":"code","outputId":"f5265314-50fa-43dd-af92-b38b109c724e","executionInfo":{"status":"ok","timestamp":1583726807077,"user_tz":-540,"elapsed":787,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["inference()"],"execution_count":164,"outputs":[{"output_type":"stream","text":["<start> I began to sweat . <end>\n","[[2, 5, 489, 7, 2445, 4, 3]]\n",". <end>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6MHj-c1GtWDB","colab_type":"code","colab":{}},"source":["dataset = tf.data.Dataset.from_tensor_slices((eng_sequence, spa_sequence, spa_target_sequence))\n","dataset = dataset.batch(32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pX52IdSSJVQS","colab_type":"code","outputId":"6724db1a-b79b-465a-a3d6-0176bd853a6e","executionInfo":{"status":"ok","timestamp":1583725121856,"user_tz":-540,"elapsed":790,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["for n, (eng, spa, spa_out) in enumerate(dataset):\n","  print(n)\n","  print('=======')\n","  print(eng)\n","  print(eng.shape)\n","  print('=======')\n","  print(spa)\n","  print(spa.shape)\n","  print('=======')\n","  print(spa_out)\n","  print(spa_out.shape)\n","  print('=======')\n","\n","  if n == 0:\n","    break"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0\n","=======\n","tf.Tensor(\n","[[   2   50    4 ...    0    0    0]\n"," [   2   50    4 ...    0    0    0]\n"," [   2   50    4 ...    0    0    0]\n"," ...\n"," [   2 1336    4 ...    0    0    0]\n"," [   2  909    4 ...    0    0    0]\n"," [   2 1279  120 ...    0    0    0]], shape=(32, 51), dtype=int32)\n","(32, 51)\n","=======\n","tf.Tensor(\n","[[    2   365     4 ...     0     0     0]\n"," [    2  1322     4 ...     0     0     0]\n"," [    2   501     4 ...     0     0     0]\n"," ...\n"," [    2 10866    27 ...     0     0     0]\n"," [    2  3856     4 ...     0     0     0]\n"," [    2    37  2188 ...     0     0     0]], shape=(32, 53), dtype=int32)\n","(32, 53)\n","=======\n","tf.Tensor(\n","[[  365     4     3 ...     0     0     0]\n"," [ 1322     4     3 ...     0     0     0]\n"," [  501     4     3 ...     0     0     0]\n"," ...\n"," [10866    27  7555 ...     0     0     0]\n"," [ 3856     4     3 ...     0     0     0]\n"," [   37  2188    85 ...     0     0     0]], shape=(32, 53), dtype=int64)\n","(32, 53)\n","=======\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"n9Ud_PJ38caH","colab_type":"code","outputId":"06353db2-6f95-4c0b-f82e-2c0071fac77c","executionInfo":{"status":"ok","timestamp":1583726946071,"user_tz":-540,"elapsed":71394,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["NUM_EPOCHS = 200\n","BATCH_SIZE = 32\n","\n","for e in range(NUM_EPOCHS):\n","  en_init_state = encoder.init_state(32)\n","  for n, (eng, spa, spa_target) in enumerate(dataset.take(1)):\n","    loss_result = train_step(eng, spa, spa_target, en_init_state)\n","  # print(loss_result.numpy())\n","  print('Epoch {} Loss {:.4f}'.format(e + 1, loss_result.numpy()))\n","  print('------')\n","  print(inference())\n","  print('======')"],"execution_count":169,"outputs":[{"output_type":"stream","text":["Epoch 1 Loss 0.1237\n","------\n","<start> What s going on here ? <end>\n","[[2, 30, 16, 79, 37, 63, 11, 3]]\n",". <end>\n","None\n","======\n","Epoch 2 Loss 0.1129\n","------\n","<start> One after another the animals died . <end>\n","[[2, 75, 183, 321, 6, 760, 308, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 3 Loss 0.1130\n","------\n","<start> I understand the risks . <end>\n","[[2, 5, 209, 6, 2799, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 4 Loss 0.1148\n","------\n","<start> I need some soap . <end>\n","[[2, 5, 92, 106, 2563, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 5 Loss 0.1134\n","------\n","<start> The story is full of holes . <end>\n","[[2, 6, 414, 12, 520, 19, 3897, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 6 Loss 0.1109\n","------\n","<start> Have you made a decision yet ? <end>\n","[[2, 23, 8, 146, 10, 796, 300, 11, 3]]\n","! <end>\n","None\n","======\n","Epoch 7 Loss 0.1096\n","------\n","<start> You can t blame them . <end>\n","[[2, 8, 34, 13, 678, 193, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 8 Loss 0.1096\n","------\n","<start> My mission is to photograph the documents . <end>\n","[[2, 26, 2576, 12, 7, 2517, 6, 2602, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 9 Loss 0.1096\n","------\n","<start> He licked his fingers . <end>\n","[[2, 14, 4489, 35, 1447, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 10 Loss 0.1091\n","------\n","<start> This room is very cold . <end>\n","[[2, 24, 142, 12, 60, 274, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 11 Loss 0.1082\n","------\n","<start> Tom and Mary didn t get along very well . <end>\n","[[2, 9, 43, 33, 66, 13, 68, 492, 60, 158, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 12 Loss 0.1070\n","------\n","<start> Don t you want to know what happened ? <end>\n","[[2, 31, 13, 8, 40, 7, 46, 30, 233, 11, 3]]\n","! <end>\n","None\n","======\n","Epoch 13 Loss 0.1059\n","------\n","<start> There s no sugar in my tea . <end>\n","[[2, 48, 16, 76, 976, 17, 26, 595, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 14 Loss 0.1052\n","------\n","<start> If I were you , I would trust her . <end>\n","[[2, 73, 5, 77, 8, 21, 5, 80, 571, 51, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 15 Loss 0.1047\n","------\n","<start> Tom was accused of doing sloppy work . <end>\n","[[2, 9, 20, 1243, 19, 217, 4498, 117, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 16 Loss 0.1045\n","------\n","<start> Will you light the fire ? <end>\n","[[2, 53, 8, 554, 6, 427, 11, 3]]\n","! <end>\n","None\n","======\n","Epoch 17 Loss 0.1043\n","------\n","<start> Keep your coat on . <end>\n","[[2, 219, 36, 853, 37, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 18 Loss 0.1040\n","------\n","<start> I think Tom can be beaten . <end>\n","[[2, 5, 67, 9, 34, 38, 5035, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 19 Loss 0.1037\n","------\n","<start> Tom hasn t told us his opinion yet . <end>\n","[[2, 9, 559, 13, 126, 107, 35, 855, 300, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 20 Loss 0.1034\n","------\n","<start> You may give this picture to whoever wants it . <end>\n","[[2, 8, 186, 149, 24, 458, 7, 1363, 203, 15, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 21 Loss 0.1031\n","------\n","<start> It s worth it . <end>\n","[[2, 15, 16, 858, 15, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 22 Loss 0.1027\n","------\n","<start> Lock the door ! <end>\n","[[2, 1319, 6, 218, 120, 3]]\n","! <end>\n","None\n","======\n","Epoch 23 Loss 0.1023\n","------\n","<start> I want to make friends with your sister . <end>\n","[[2, 5, 40, 7, 140, 212, 39, 36, 326, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 24 Loss 0.1020\n","------\n","<start> How many days will you stay in Boston ? <end>\n","[[2, 56, 124, 304, 53, 8, 196, 17, 168, 11, 3]]\n","! <end>\n","None\n","======\n","Epoch 25 Loss 0.1017\n","------\n","<start> I just want to help Tom . <end>\n","[[2, 5, 89, 40, 7, 95, 9, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 26 Loss 0.1015\n","------\n","<start> This hotel can accommodate guests . <end>\n","[[2, 24, 505, 34, 6757, 1668, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 27 Loss 0.1013\n","------\n","<start> Tom can t understand why Mary got so angry . <end>\n","[[2, 9, 34, 13, 209, 83, 33, 103, 93, 360, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 28 Loss 0.1011\n","------\n","<start> Tom forgot to do his homework . <end>\n","[[2, 9, 552, 7, 22, 35, 446, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 29 Loss 0.1008\n","------\n","<start> I don t have a ticket . <end>\n","[[2, 5, 31, 13, 23, 10, 859, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 30 Loss 0.1005\n","------\n","<start> The sentence is free from grammatical mistakes . <end>\n","[[2, 6, 1050, 12, 491, 71, 4747, 694, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 31 Loss 0.1002\n","------\n","<start> I ll soon register for a course in German . <end>\n","[[2, 5, 57, 231, 2261, 27, 10, 1210, 17, 993, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 32 Loss 0.0999\n","------\n","<start> The children shared a pizza after school . <end>\n","[[2, 6, 207, 2038, 10, 1437, 183, 141, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 33 Loss 0.0996\n","------\n","<start> When do you want me to start ? <end>\n","[[2, 70, 22, 8, 40, 25, 7, 400, 11, 3]]\n","! <end>\n","None\n","======\n","Epoch 34 Loss 0.0993\n","------\n","<start> He achieved his goal . <end>\n","[[2, 14, 3092, 35, 1840, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 35 Loss 0.0990\n","------\n","<start> You re not doing your share . <end>\n","[[2, 8, 54, 42, 217, 36, 962, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 36 Loss 0.0986\n","------\n","<start> It s fun to play baseball . <end>\n","[[2, 15, 16, 447, 7, 201, 684, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 37 Loss 0.0982\n","------\n","<start> I have a computer . <end>\n","[[2, 5, 23, 10, 566, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 38 Loss 0.0979\n","------\n","<start> She made the same mistake again . <end>\n","[[2, 28, 146, 6, 269, 456, 179, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 39 Loss 0.0974\n","------\n","<start> Who broke the cup ? <end>\n","[[2, 81, 450, 6, 652, 11, 3]]\n","! <end>\n","None\n","======\n","Epoch 40 Loss 0.0970\n","------\n","<start> I was told to get enough sleep . <end>\n","[[2, 5, 20, 126, 7, 68, 239, 311, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 41 Loss 0.0967\n","------\n","<start> How s your cold ? <end>\n","[[2, 56, 16, 36, 274, 11, 3]]\n","! <end>\n","None\n","======\n","Epoch 42 Loss 0.0963\n","------\n","<start> They won t be happy until they ve taken everything we own . <end>\n","[[2, 49, 165, 13, 38, 197, 347, 49, 72, 841, 191, 29, 344, 4, 3]]\n","hola <end>\n","None\n","======\n","Epoch 43 Loss 0.0959\n","------\n","<start> Let me do it . <end>\n","[[2, 88, 25, 22, 15, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 44 Loss 0.0954\n","------\n","<start> We will run short of oil some day . <end>\n","[[2, 29, 53, 464, 521, 19, 1370, 106, 110, 4, 3]]\n","! <end>\n","None\n","======\n","Epoch 45 Loss 0.0949\n","------\n","<start> My mother put thirteen candles on my birthday cake . <end>\n","[[2, 26, 192, 176, 1942, 2916, 37, 26, 585, 481, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 46 Loss 0.0944\n","------\n","<start> Tom can t stand the smell of cigarette smoke . <end>\n","[[2, 9, 34, 13, 562, 6, 1074, 19, 1847, 766, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 47 Loss 0.0939\n","------\n","<start> I received a letter written in English yesterday . <end>\n","[[2, 5, 1011, 10, 296, 861, 17, 190, 180, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 48 Loss 0.0935\n","------\n","<start> I m very happy to be here . <end>\n","[[2, 5, 41, 60, 197, 7, 38, 63, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 49 Loss 0.0930\n","------\n","<start> Who are these men ? <end>\n","[[2, 81, 32, 211, 538, 11, 3]]\n","! <end>\n","None\n","======\n","Epoch 50 Loss 0.0926\n","------\n","<start> They won t tell us anything . <end>\n","[[2, 49, 165, 13, 96, 107, 132, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 51 Loss 0.0921\n","------\n","<start> I m a happy man . <end>\n","[[2, 5, 41, 10, 197, 153, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 52 Loss 0.0916\n","------\n","<start> Tom set the alarm clock to go off a . m . <end>\n","[[2, 9, 801, 6, 1220, 443, 7, 50, 162, 10, 4, 41, 4, 3]]\n","hola <end>\n","None\n","======\n","Epoch 53 Loss 0.0911\n","------\n","<start> The price of vegetables varies from day to day . <end>\n","[[2, 6, 977, 19, 1175, 6906, 71, 110, 7, 110, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 54 Loss 0.0906\n","------\n","<start> The sky was filled with airplanes . <end>\n","[[2, 6, 907, 20, 1334, 39, 7907, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 55 Loss 0.0901\n","------\n","<start> Do you need me to give you some money ? <end>\n","[[2, 22, 8, 92, 25, 7, 149, 8, 106, 114, 11, 3]]\n",". <end>\n","None\n","======\n","Epoch 56 Loss 0.0896\n","------\n","<start> The child missed his mother very much . <end>\n","[[2, 6, 359, 735, 35, 192, 60, 111, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 57 Loss 0.0891\n","------\n","<start> I m going to figure out a way to blame this on Tom . <end>\n","[[2, 5, 41, 79, 7, 889, 69, 10, 171, 7, 678, 24, 37, 9, 4, 3]]\n","hola <end>\n","None\n","======\n","Epoch 58 Loss 0.0886\n","------\n","<start> The workman died from the explosion . <end>\n","[[2, 6, 10637, 308, 71, 6, 2252, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 59 Loss 0.0881\n","------\n","<start> He went to a fruit store first . <end>\n","[[2, 14, 123, 7, 10, 1043, 590, 199, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 60 Loss 0.0877\n","------\n","<start> I hate mowing the lawn . <end>\n","[[2, 5, 408, 6225, 6, 2663, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 61 Loss 0.0872\n","------\n","<start> There were many things that Tom wanted Mary to do that day . <end>\n","[[2, 48, 77, 124, 256, 18, 9, 159, 33, 7, 22, 18, 110, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 62 Loss 0.0867\n","------\n","<start> Tom is old and clumsy . <end>\n","[[2, 9, 12, 154, 43, 3540, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 63 Loss 0.0863\n","------\n","<start> I m feeling nervous . <end>\n","[[2, 5, 41, 752, 985, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 64 Loss 0.0858\n","------\n","<start> She pointed her finger at him . <end>\n","[[2, 28, 2146, 51, 1129, 45, 47, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 65 Loss 0.0853\n","------\n","<start> He has a Japanese car . <end>\n","[[2, 14, 52, 10, 392, 112, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 66 Loss 0.0848\n","------\n","<start> Here you are . <end>\n","[[2, 63, 8, 32, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 67 Loss 0.0843\n","------\n","<start> It was obvious that it would be this way . <end>\n","[[2, 15, 20, 1991, 18, 15, 80, 38, 24, 171, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 68 Loss 0.0837\n","------\n","<start> Tom draws very nice pictures . <end>\n","[[2, 9, 9859, 60, 397, 890, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 69 Loss 0.0832\n","------\n","<start> I don t want to remember . <end>\n","[[2, 5, 31, 13, 40, 7, 236, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 70 Loss 0.0827\n","------\n","<start> She didn t come after all . <end>\n","[[2, 28, 66, 13, 84, 183, 62, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 71 Loss 0.0821\n","------\n","<start> I gave a short talk . <end>\n","[[2, 5, 229, 10, 521, 169, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 72 Loss 0.0816\n","------\n","<start> Tom doesn t believe the rumors about Mary . <end>\n","[[2, 9, 78, 13, 222, 6, 3062, 64, 33, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 73 Loss 0.0810\n","------\n","<start> What are you smirking at ? <end>\n","[[2, 30, 32, 8, 6213, 45, 11, 3]]\n",". <end>\n","None\n","======\n","Epoch 74 Loss 0.0805\n","------\n","<start> You re kidding me , right ? <end>\n","[[2, 8, 54, 1649, 25, 21, 152, 11, 3]]\n",". <end>\n","None\n","======\n","Epoch 75 Loss 0.0799\n","------\n","<start> She looked in her bag for the key of the house , but could not find it . <end>\n","[[2, 28, 329, 17, 51, 706, 27, 6, 623, 19, 6, 127, 21, 97, 113, 42, 200, 15, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 76 Loss 0.0794\n","------\n","<start> We are adjourned until . <end>\n","[[2, 29, 32, 9770, 347, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 77 Loss 0.0789\n","------\n","<start> Tom asked Mary when she was going to buy a new pair of shoes for her youngest daughter . <end>\n","[[2, 9, 167, 33, 70, 28, 20, 79, 7, 194, 10, 145, 1194, 19, 428, 27, 51, 3744, 560, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 78 Loss 0.0784\n","------\n","<start> It s Tom s idea . <end>\n","[[2, 15, 16, 9, 16, 282, 4, 3]]\n",". <end>\n","None\n","======\n","Epoch 79 Loss 0.0779\n","------\n","<start> I thought you were Tom . <end>\n","[[2, 5, 138, 8, 77, 9, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 80 Loss 0.0774\n","------\n","<start> They go shopping . <end>\n","[[2, 49, 50, 838, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 81 Loss 0.0770\n","------\n","<start> English is a universal language and is used all over the world . <end>\n","[[2, 190, 12, 10, 5130, 466, 43, 12, 216, 62, 182, 6, 305, 4, 3]]\n","auxilio ! <end>\n","None\n","======\n","Epoch 82 Loss 0.0765\n","------\n","<start> Three workers on board the truck were killed in the accident . <end>\n","[[2, 155, 1705, 37, 2063, 6, 1235, 77, 527, 17, 6, 393, 4, 3]]\n","auxilio ! <end>\n","None\n","======\n","Epoch 83 Loss 0.0761\n","------\n","<start> The man set himself on fire . <end>\n","[[2, 6, 153, 801, 371, 37, 427, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 84 Loss 0.0757\n","------\n","<start> Tom is deaf in one ear . <end>\n","[[2, 9, 12, 1560, 17, 75, 1613, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 85 Loss 0.0753\n","------\n","<start> Tom joined us . <end>\n","[[2, 9, 2653, 107, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 86 Loss 0.0750\n","------\n","<start> He sits in this chair when he watches television . <end>\n","[[2, 14, 3965, 17, 24, 902, 70, 14, 2042, 731, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 87 Loss 0.0746\n","------\n","<start> You should study harder . <end>\n","[[2, 8, 101, 313, 1137, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 88 Loss 0.0743\n","------\n","<start> How much does it cost to get in ? <end>\n","[[2, 56, 111, 134, 15, 768, 7, 68, 17, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 89 Loss 0.0740\n","------\n","<start> The gardener didn t let us walk on the grass . <end>\n","[[2, 6, 4067, 66, 13, 88, 107, 328, 37, 6, 1536, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 90 Loss 0.0737\n","------\n","<start> He knows almost nothing about that animal . <end>\n","[[2, 14, 259, 306, 174, 64, 18, 1391, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 91 Loss 0.0734\n","------\n","<start> You don t fool me . <end>\n","[[2, 8, 31, 13, 1416, 25, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 92 Loss 0.0731\n","------\n","<start> Now I feel sorry for her . <end>\n","[[2, 98, 5, 223, 381, 27, 51, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 93 Loss 0.0728\n","------\n","<start> It s a conspiracy . <end>\n","[[2, 15, 16, 10, 6056, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 94 Loss 0.0726\n","------\n","<start> Now he is recognized as one of the most promising writers . <end>\n","[[2, 98, 14, 12, 2003, 59, 75, 19, 6, 338, 4166, 8448, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 95 Loss 0.0724\n","------\n","<start> We ll show you the way . <end>\n","[[2, 29, 57, 362, 8, 6, 171, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 96 Loss 0.0722\n","------\n","<start> Tom said he didn t think Mary had a driver s license . <end>\n","[[2, 9, 125, 14, 66, 13, 67, 33, 61, 10, 803, 16, 1258, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 97 Loss 0.0719\n","------\n","<start> He dug a hole in the garden . <end>\n","[[2, 14, 3343, 10, 1502, 17, 6, 712, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 98 Loss 0.0717\n","------\n","<start> We re special . <end>\n","[[2, 29, 54, 1203, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 99 Loss 0.0715\n","------\n","<start> The shop did not want him . <end>\n","[[2, 6, 929, 58, 42, 40, 47, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 100 Loss 0.0714\n","------\n","<start> My father is a proud man . <end>\n","[[2, 26, 148, 12, 10, 680, 153, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 101 Loss 0.0712\n","------\n","<start> We ll talk about it tomorrow . <end>\n","[[2, 29, 57, 169, 64, 15, 164, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 102 Loss 0.0710\n","------\n","<start> Tom won t talk to me anymore . <end>\n","[[2, 9, 165, 13, 169, 7, 25, 441, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 103 Loss 0.0708\n","------\n","<start> I really want to know the truth . <end>\n","[[2, 5, 121, 40, 7, 46, 6, 320, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 104 Loss 0.0707\n","------\n","<start> It took them some time to get used to one another . <end>\n","[[2, 15, 206, 193, 106, 55, 7, 68, 216, 7, 75, 321, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 105 Loss 0.0705\n","------\n","<start> Who resigned ? <end>\n","[[2, 81, 3018, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 106 Loss 0.0704\n","------\n","<start> At that time , she was bathing in the sun . <end>\n","[[2, 45, 18, 55, 21, 28, 20, 4189, 17, 6, 740, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 107 Loss 0.0702\n","------\n","<start> Tom blames you for the death of his wife . <end>\n","[[2, 9, 3299, 8, 27, 6, 522, 19, 35, 354, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 108 Loss 0.0701\n","------\n","<start> I always tried to be strict with them and not to smile . <end>\n","[[2, 5, 147, 339, 7, 38, 2432, 39, 193, 43, 42, 7, 909, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 109 Loss 0.0700\n","------\n","<start> I m not going to make it on time . <end>\n","[[2, 5, 41, 42, 79, 7, 140, 15, 37, 55, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 110 Loss 0.0698\n","------\n","<start> He is very angry . <end>\n","[[2, 14, 12, 60, 360, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 111 Loss 0.0697\n","------\n","<start> Tom is younger than Mary . <end>\n","[[2, 9, 12, 819, 99, 33, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 112 Loss 0.0696\n","------\n","<start> He narrowly escaped being run over . <end>\n","[[2, 14, 5589, 1410, 287, 464, 182, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 113 Loss 0.0695\n","------\n","<start> He closed his eyes . <end>\n","[[2, 14, 793, 35, 457, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 114 Loss 0.0694\n","------\n","<start> This is pathetic . <end>\n","[[2, 24, 12, 3807, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 115 Loss 0.0693\n","------\n","<start> Will you all be here tomorrow ? <end>\n","[[2, 53, 8, 62, 38, 63, 164, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 116 Loss 0.0692\n","------\n","<start> I have just washed all the dishes . <end>\n","[[2, 5, 23, 89, 1402, 62, 6, 1233, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 117 Loss 0.0691\n","------\n","<start> They understand us . <end>\n","[[2, 49, 209, 107, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 118 Loss 0.0690\n","------\n","<start> A full moon is shining bright in the sky . <end>\n","[[2, 10, 520, 1109, 12, 2936, 1809, 17, 6, 907, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 119 Loss 0.0690\n","------\n","<start> Eventually , Tom found out what had happened . <end>\n","[[2, 1382, 21, 9, 263, 69, 30, 61, 233, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 120 Loss 0.0689\n","------\n","<start> She s just being snobbish . <end>\n","[[2, 28, 16, 89, 287, 4942, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 121 Loss 0.0688\n","------\n","<start> I don t have time now . <end>\n","[[2, 5, 31, 13, 23, 55, 98, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 122 Loss 0.0688\n","------\n","<start> I want you to follow Tom . <end>\n","[[2, 5, 40, 8, 7, 807, 9, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 123 Loss 0.0687\n","------\n","<start> He is busy preparing for the examination . <end>\n","[[2, 14, 12, 251, 2162, 27, 6, 1554, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 124 Loss 0.0686\n","------\n","<start> He parked his car in front of the building . <end>\n","[[2, 14, 1913, 35, 112, 17, 713, 19, 6, 618, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 125 Loss 0.0686\n","------\n","<start> That child is only four , but he can already count to . <end>\n","[[2, 18, 359, 12, 137, 609, 21, 97, 14, 34, 237, 961, 7, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 126 Loss 0.0685\n","------\n","<start> We ve been discovered . <end>\n","[[2, 29, 72, 90, 1494, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 127 Loss 0.0685\n","------\n","<start> To have doubts about oneself is the first sign of intelligence . <end>\n","[[2, 7, 23, 2071, 64, 5649, 12, 6, 199, 979, 19, 3728, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 128 Loss 0.0684\n","------\n","<start> Tom liked Mary for years , but at some point , his feelings for her changed to love . <end>\n","[[2, 9, 831, 33, 27, 184, 21, 97, 45, 106, 792, 21, 35, 1040, 27, 51, 612, 7, 157, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 129 Loss 0.0684\n","------\n","<start> I wish I were clever . <end>\n","[[2, 5, 302, 5, 77, 1535, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 130 Loss 0.0683\n","------\n","<start> Go inside . <end>\n","[[2, 50, 910, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 131 Loss 0.0683\n","------\n","<start> Do you want to wait ? <end>\n","[[2, 22, 8, 40, 7, 267, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 132 Loss 0.0683\n","------\n","<start> Sister , don t let this patient out of your sight . <end>\n","[[2, 326, 21, 31, 13, 88, 24, 762, 69, 19, 36, 1048, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 133 Loss 0.0682\n","------\n","<start> A boy was driving a flock of sheep . <end>\n","[[2, 10, 244, 20, 772, 10, 4176, 19, 2387, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 134 Loss 0.0682\n","------\n","<start> My mother has been sick for two days . <end>\n","[[2, 26, 192, 52, 90, 432, 27, 144, 304, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 135 Loss 0.0682\n","------\n","<start> There s nothing more important than friendship . <end>\n","[[2, 48, 16, 174, 94, 418, 99, 1822, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 136 Loss 0.0681\n","------\n","<start> Divide the candles among you . <end>\n","[[2, 3825, 6, 2916, 1226, 8, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 137 Loss 0.0681\n","------\n","<start> He loves coffee . <end>\n","[[2, 14, 506, 284, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 138 Loss 0.0681\n","------\n","<start> I guess you were right . <end>\n","[[2, 5, 785, 8, 77, 152, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 139 Loss 0.0680\n","------\n","<start> All the students began talking at once . <end>\n","[[2, 62, 6, 410, 489, 323, 45, 334, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 140 Loss 0.0680\n","------\n","<start> Did you clean your room ? <end>\n","[[2, 58, 8, 547, 36, 142, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 141 Loss 0.0680\n","------\n","<start> I m looking forward to the party . <end>\n","[[2, 5, 41, 241, 770, 7, 6, 265, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 142 Loss 0.0679\n","------\n","<start> I think I m in trouble . <end>\n","[[2, 5, 67, 5, 41, 17, 413, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 143 Loss 0.0679\n","------\n","<start> When does Tom s bus arrive ? <end>\n","[[2, 70, 134, 9, 16, 310, 789, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 144 Loss 0.0679\n","------\n","<start> Put yourself in my position . <end>\n","[[2, 176, 369, 17, 26, 1144, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 145 Loss 0.0679\n","------\n","<start> I won t lower myself to his level . <end>\n","[[2, 5, 165, 13, 2147, 416, 7, 35, 2615, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 146 Loss 0.0679\n","------\n","<start> I seem to have caught a cold . <end>\n","[[2, 5, 543, 7, 23, 478, 10, 274, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 147 Loss 0.0678\n","------\n","<start> I have a previous engagement at ten . <end>\n","[[2, 5, 23, 10, 4322, 3496, 45, 337, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 148 Loss 0.0678\n","------\n","<start> It would be nice to get married . <end>\n","[[2, 15, 80, 38, 397, 7, 68, 243, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 149 Loss 0.0678\n","------\n","<start> Does your mom know ? <end>\n","[[2, 134, 36, 1154, 46, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 150 Loss 0.0678\n","------\n","<start> Tom kept the secret to himself . <end>\n","[[2, 9, 510, 6, 583, 7, 371, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 151 Loss 0.0678\n","------\n","<start> I hope this won t affect our friendship . <end>\n","[[2, 5, 288, 24, 165, 13, 4569, 122, 1822, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 152 Loss 0.0677\n","------\n","<start> I can walk no farther . <end>\n","[[2, 5, 34, 328, 76, 3876, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 153 Loss 0.0677\n","------\n","<start> You re hopeless . <end>\n","[[2, 8, 54, 3570, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 154 Loss 0.0677\n","------\n","<start> Is there room in your van for three more people ? <end>\n","[[2, 12, 48, 142, 17, 36, 4056, 27, 155, 94, 115, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 155 Loss 0.0677\n","------\n","<start> Tom could have come yesterday , but he didn t . <end>\n","[[2, 9, 113, 23, 84, 180, 21, 97, 14, 66, 13, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 156 Loss 0.0677\n","------\n","<start> I am going to swim a lot this summer . <end>\n","[[2, 5, 102, 79, 7, 401, 10, 131, 24, 479, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 157 Loss 0.0676\n","------\n","<start> Americans eat a lot of meat . <end>\n","[[2, 1428, 139, 10, 131, 19, 765, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 158 Loss 0.0676\n","------\n","<start> There are many active volcanoes in Japan . <end>\n","[[2, 48, 32, 124, 2718, 7587, 17, 295, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 159 Loss 0.0676\n","------\n","<start> Tom fell down the stairs . <end>\n","[[2, 9, 484, 189, 6, 1394, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 160 Loss 0.0676\n","------\n","<start> Have I said something to hurt your feelings ? <end>\n","[[2, 23, 5, 125, 105, 7, 389, 36, 1040, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 161 Loss 0.0676\n","------\n","<start> Go to the doctor to get your prescription ! <end>\n","[[2, 50, 7, 6, 316, 7, 68, 36, 6491, 120, 3]]\n","<end>\n","None\n","======\n","Epoch 162 Loss 0.0676\n","------\n","<start> Let s take your temperature first . <end>\n","[[2, 88, 16, 109, 36, 1468, 199, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 163 Loss 0.0676\n","------\n","<start> I ve never seen anything quite like this . <end>\n","[[2, 5, 72, 100, 270, 132, 468, 44, 24, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 164 Loss 0.0675\n","------\n","<start> Tom lowered his voice . <end>\n","[[2, 9, 4516, 35, 926, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 165 Loss 0.0675\n","------\n","<start> Everybody laughed but Tom . <end>\n","[[2, 378, 940, 97, 9, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 166 Loss 0.0675\n","------\n","<start> They were school children then . <end>\n","[[2, 49, 77, 141, 207, 480, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 167 Loss 0.0675\n","------\n","<start> How high is that mountain ? <end>\n","[[2, 56, 549, 12, 18, 830, 11, 3]]\n","<end>\n","None\n","======\n","Epoch 168 Loss 0.0675\n","------\n","<start> Sign above this line . <end>\n","[[2, 979, 1407, 24, 1076, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 169 Loss 0.0675\n","------\n","<start> I should go to bed . <end>\n","[[2, 5, 101, 50, 7, 261, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 170 Loss 0.0674\n","------\n","<start> Tom liked you . <end>\n","[[2, 9, 831, 8, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 171 Loss 0.0674\n","------\n","<start> I could kill you . <end>\n","[[2, 5, 113, 533, 8, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 172 Loss 0.0984\n","------\n","<start> Please tell Tom that Mary called . <end>\n","[[2, 87, 96, 9, 18, 33, 402, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 173 Loss 0.0674\n","------\n","<start> She asked me whether she could use the telephone . <end>\n","[[2, 28, 167, 25, 588, 28, 113, 286, 6, 689, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 174 Loss 0.0675\n","------\n","<start> I ve got to be free . <end>\n","[[2, 5, 72, 103, 7, 38, 491, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 175 Loss 0.0675\n","------\n","<start> Is it convenient for you if I come at p . m . ? <end>\n","[[2, 12, 15, 2726, 27, 8, 73, 5, 84, 45, 1682, 4, 41, 4, 11, 3]]\n","auxilio ! <end>\n","None\n","======\n","Epoch 176 Loss 0.0675\n","------\n","<start> Someone has taken my shoes by mistake . <end>\n","[[2, 292, 52, 841, 26, 428, 82, 456, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 177 Loss 0.0674\n","------\n","<start> Funeral services will take place in the cathedral . <end>\n","[[2, 1738, 5246, 53, 109, 327, 17, 6, 5870, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 178 Loss 0.0674\n","------\n","<start> I have got to go now . <end>\n","[[2, 5, 23, 103, 7, 50, 98, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 179 Loss 0.0674\n","------\n","<start> You must not lose sight of your goal in life . <end>\n","[[2, 8, 130, 42, 633, 1048, 19, 36, 1840, 17, 204, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 180 Loss 0.0674\n","------\n","<start> Tom went to bed at midnight . <end>\n","[[2, 9, 123, 7, 261, 45, 1256, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 181 Loss 0.0674\n","------\n","<start> Everything happened too quickly . <end>\n","[[2, 191, 233, 104, 662, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 182 Loss 0.0674\n","------\n","<start> When do we leave ? <end>\n","[[2, 70, 22, 29, 175, 11, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 183 Loss 0.0673\n","------\n","<start> It was Mary that bought this skirt yesterday . <end>\n","[[2, 15, 20, 33, 18, 213, 24, 1465, 180, 4, 3]]\n","<end>\n","None\n","======\n","Epoch 184 Loss 0.0673\n","------\n","<start> There are more than seven thousand languages in the world . <end>\n","[[2, 48, 32, 94, 99, 726, 1060, 874, 17, 6, 305, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 185 Loss 0.0673\n","------\n","<start> I won t say anything . <end>\n","[[2, 5, 165, 13, 136, 132, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 186 Loss 0.0673\n","------\n","<start> My mother has good handwriting . <end>\n","[[2, 26, 192, 52, 86, 2710, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 187 Loss 0.0673\n","------\n","<start> What natural foods help curb the appetite ? <end>\n","[[2, 30, 1629, 2379, 95, 3821, 6, 2468, 11, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 188 Loss 0.0674\n","------\n","<start> You aren t really going to go to Boston in Tom s car , are you ? <end>\n","[[2, 8, 374, 13, 121, 79, 7, 50, 7, 168, 17, 9, 16, 112, 21, 32, 8, 11, 3]]\n","hola , no no no ? <end>\n","None\n","======\n","Epoch 189 Loss 0.0674\n","------\n","<start> Sleep is no less necessary to health than food . <end>\n","[[2, 311, 12, 76, 802, 965, 7, 711, 99, 275, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 190 Loss 0.0674\n","------\n","<start> I don t like visiting big cities . <end>\n","[[2, 5, 31, 13, 44, 1846, 278, 2229, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 191 Loss 0.0678\n","------\n","<start> Tom forgot to feed his dog . <end>\n","[[2, 9, 552, 7, 921, 35, 172, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 192 Loss 0.0679\n","------\n","<start> She attempted to kill herself . <end>\n","[[2, 28, 2227, 7, 533, 647, 4, 3]]\n","hola . <end>\n","None\n","======\n","Epoch 193 Loss 0.0690\n","------\n","<start> It was just a bad dream . <end>\n","[[2, 15, 20, 89, 10, 252, 530, 4, 3]]\n","intento . <end>\n","None\n","======\n","Epoch 194 Loss 0.0691\n","------\n","<start> I always drive at a moderate speed . <end>\n","[[2, 5, 147, 411, 45, 10, 6690, 2165, 4, 3]]\n","intento . <end>\n","None\n","======\n","Epoch 195 Loss 0.0686\n","------\n","<start> They say she is the kindest woman on earth . <end>\n","[[2, 49, 136, 28, 12, 6, 11265, 424, 37, 966, 4, 3]]\n","intento . ? <end>\n","None\n","======\n","Epoch 196 Loss 0.0688\n","------\n","<start> The movie is showing in movie theaters next month . <end>\n","[[2, 6, 461, 12, 2567, 17, 461, 6784, 202, 396, 4, 3]]\n","ataque ! <end>\n","None\n","======\n","Epoch 197 Loss 0.0692\n","------\n","<start> I have a boyfriend who loves me . <end>\n","[[2, 5, 23, 10, 913, 81, 506, 25, 4, 3]]\n","intento . <end>\n","None\n","======\n","Epoch 198 Loss 0.0687\n","------\n","<start> You are our new neighbor . <end>\n","[[2, 8, 32, 122, 145, 1421, 4, 3]]\n","ataque ! <end>\n","None\n","======\n","Epoch 199 Loss 0.0692\n","------\n","<start> I think it s dangerous to swim in that river . <end>\n","[[2, 5, 67, 15, 16, 665, 7, 401, 17, 18, 490, 4, 3]]\n","ataque ! <end>\n","None\n","======\n","Epoch 200 Loss 0.0690\n","------\n","<start> It was a joke . <end>\n","[[2, 15, 20, 10, 1000, 4, 3]]\n","hola <end>\n","None\n","======\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jju9UPleg1Yv","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fO8qqqTCg1WW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"iKH5W5nGg1Uw","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRaJjhW5g1SZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PG1bM9sEuqea","colab_type":"code","outputId":"e37680aa-bffe-4129-e3ad-b3dd44382948","executionInfo":{"status":"ok","timestamp":1583667326527,"user_tz":-540,"elapsed":876,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["target = 0.2\n","\n","tf.math.equal(target, 0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=bool, numpy=False>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Xezw1JRIwND9","colab_type":"code","outputId":"0e94acaf-70fb-4467-9a5c-44ab348af7db","executionInfo":{"status":"ok","timestamp":1583667326527,"user_tz":-540,"elapsed":869,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tf.math.logical_not(tf.math.equal(target, 0))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=bool, numpy=True>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"KMbtDaCd9X17","colab_type":"code","outputId":"a4ef16fb-dd09-4815-8bd9-b93f7237c450","executionInfo":{"status":"ok","timestamp":1583667842625,"user_tz":-540,"elapsed":584,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["target = 0\n","\n","tf.math.equal(target, 0)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=bool, numpy=True>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"FxQApGeO9Yu_","colab_type":"code","outputId":"c911392f-ad46-4a38-dc5d-38da2b5e4dac","executionInfo":{"status":"ok","timestamp":1583667845577,"user_tz":-540,"elapsed":647,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tf.math.logical_not(tf.math.equal(target, 0))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=bool, numpy=False>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"JDqkCi1TwCuk","colab_type":"code","outputId":"4f6fffb5-e3e3-4508-a4c1-8309d61c58f0","executionInfo":{"status":"ok","timestamp":1583667326528,"user_tz":-540,"elapsed":863,"user":{"displayName":"Seunghwan Oh","photoUrl":"","userId":"13780782377671471234"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["tf.cast(tf.math.logical_not(tf.math.equal(target, 0)), tf.int64)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(), dtype=int64, numpy=1>"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"cHsGhgL77ahj","colab_type":"code","colab":{}},"source":["tf.keras.losses.SparseCategoricalCrossentropy()"],"execution_count":0,"outputs":[]}]}