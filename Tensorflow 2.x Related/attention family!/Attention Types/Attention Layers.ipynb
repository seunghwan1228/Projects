{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Attention Layers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM2iNqB1AWUGWiN+bJ0sqSs"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"p_J32Kb5xoaJ","colab_type":"code","colab":{}},"source":["# Variety Attention Scores\n","# Dot\n","# Scaled Dot\n","# General\n","# Concat\n","# Location\n","\n","\n","\n","import tensorflow as tf\n","import numpy as np"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7L2JmWI9x3RI","colab_type":"code","colab":{}},"source":["class Attention(tf.keras.layers.Layer):\n","  def __init__(self, alignment_type='global', window_width=None, score_function='general', **kwargs):\n","    super(Attention, self).__init__(**kwargs)\n","    self.alignment_type = alignment_type\n","    self.window_width = window_width\n","    self.score_function = score_function\n","\n","  def build(self, input_shape):\n","    # b, seq_len, H\n","    self.input_sequence_length = input_shape[0][1]\n","    self.hidden_dim = input_shape[0][2]\n","\n","    if 'local-p' in self.alignment_type:\n","      self.W_p = tf.keras.layers.Dense(units = self.hidden_dim, use_bias=False)\n","      self.W_p.build(input_shape=(None, None, sel.hidden_dim))\n","      self._trainable_weights += self.W_p.trainable_weights\n","\n","      self.v_p = tf.keras.layers.Dense(1, use_bias=False)\n","      self.v_p.build(input_shape=(None, None, self.hidden_dim))\n","      self._trainable_weights += self.v_p._trainable_weights\n","\n","    if 'dot' not in self.score_function: # not for dot product  [Not Dot or Scaled Dot]\n","      self.W_a = tf.keras.layers.Dense(units=self.hidden_dim, use_bias=False)\n","      self.W_a.build(input_shape=(None, None, self.hidden_dim))\n","      self._trainable_weights += self.W_a.trainable_weights\n","\n","    if self.score_function == 'concat':\n","      self.U_a = tf.keras.layers.Dense(units=self.hidden_dim, use_bias=False)\n","      self.U_a.build(input_shape=(None, None, self.hidden_dim))\n","      self._trainable_weights += self.U_a.trainable_weights\n","\n","      self.v_a = tf.keras.layers.Dense(units=1, use_bias=False)\n","      self.v_a.build(input_shape=(None, None, self.hidden_dim))\n","      self._trainable_weights += self.v_a.trainable_weights\n","\n","    super(Attention, self).build(input_shape)\n","\n","  def call(self, inputs):\n","    source_hidden_states = inputs[0]\n","    target_hidden_state = inputs[1]\n","    current_timestep = inputs[2]\n","  \n","    if self.alignment_type == 'global':\n","      source_hidden_states = source_hidden_states\n","\n","    elif 'local' in self.alignment_type:\n","      self.window_width = 8 if self.window_width is None else self.window_width\n","\n","      if self.alignment_type == 'local-m': # monotonic\n","        aligned_position = current_timestep\n","\n","        left = int(aligned_position + self.window_width if aligned_position- self.window_width >= 0 else 0)\n","        right = int(aligned_position+ self.window_width if aligned_position + self.window_width <= self.input_sequence_length else self.input_sequence_length)\n","\n","        source_hidden_states = tf.keras.layers.Lambda(lambda x: x[:, left:right, :])(source_hidden_states)\n","\n","      elif self.alignment_type == 'local-p': # predictive\n","        aligned_position = self.W_p(target_hidden_state)\n","        aligned_position = tf.keras.layers.Activation('tanh')(aligned_position)\n","        aligned_position = self.v_p(aligned_position)\n","        aligned_position = tf.keras.layers.Activation('sigmoid')(aligned_position)\n","\n","        aligned_position = aligned_positon * self.input_sequence_length\n","\n","    if 'dot' in self.score_function:\n","      attention_score = tf.keras.layers.Dot(axes=[2,2])([source_hidden_states, target_hidden_state])\n","      if self.score_function == 'scaled_dot':\n","        attention-score *= 1 / tf.math.sqrt(tf.cast(source_hidden_states.shape[2], tf.float64))\n","\n","    elif self.score_function == 'general':\n","      weighted_hidden_states = self.W_a(source_hidden_states)\n","      attention_score = tf.keras.layers.Dot(axes=[2,2])([weighted_hidden_states, target_hidden_state])\n","\n","    elif self.score_function == 'location':\n","      weighted_target_state = self.W_a(target_hidden_state)\n","      attention_score = tf.keras.layers.Activation('softmax')(weighted_target_state)\n","      attention_score = tf.keras.layers.RepeatVector(source_hidden_states.shape[1])(attention_score)\n","      attention_score = tf.reduce_sum(attention_score, axis=-1)\n","      attention_score = tf.expand_dims(attention_score, axis=-1)\n","\n","    elif self.score_function == 'concat':\n","      weighted_hidden_states = self.W_a(source_hidden_states)\n","      weighted_target_state = self.U_a(target_hidden_state)\n","      weighted_sum = weighted_hidden_states + weighted_target_state\n","      weighted_sum = tf.keras.layers.Activation('tanh')(weighted_sum)\n","      attention_score = self.V_a(weighted_sum)\n","\n","    attention_weights = tf.keras.layers.Activation('softmax')(attention_score)\n","\n","    if self.alignment_type == 'local-p':\n","      gaussian_estimation = lambda s: tf.exp(-tf.math.square(s - alignment_position) / (2 * tf.math.square(self.window_width / 2)))\n","      gaussian_factor = gaussian_estimation(0)\n","\n","      for i in range(1, self.input_sequence_length):\n","        gaussian_factor = tf.keras.layers.Concatenate(axis=1)([gaussian_factor, gaussian_estimation(i)])\n","\n","      attention_weights = attention_weights * gaussian_factor\n","\n","    context_vector = source_hidden_states * attention_weights\n","\n","    return context_vector, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EMBoc3yKEVAV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}